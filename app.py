def x7_snippet_library_ui(conn):
    """Fallback stub; real implementation is defined later in the file."""
    return None




def x7_render_template_text(
    conn: "sqlite3.Connection",
    template_id: int,
    rfp_id: int | None = None,
    deal_id: int | None = None,
    manual_values: dict | None = None,
) -> str:
    """
    Render a proposal template by id, expanding any {{TOKENS}} using
    deal data, RFP Analyzer data, org profile, and manual overrides.
    """
    from contextlib import closing as _closing
    import re as _re

    manual_values = manual_values or {}

    with _closing(conn.cursor()) as cur:
        cur.execute(
            "SELECT body FROM proposal_templates WHERE id = ?;",
            (int(template_id),),
        )
        row = cur.fetchone()
        if not row:
            return ""
        body = row[0] or ""

        # Collect token names like {{TOKEN_NAME}}
        token_names = set(_re.findall(r"{{\s*([A-Z0-9_]+)\s*}}", body))
        if not token_names:
            return body

        # Load token metadata
        cur.execute(
            """
            SELECT token_name, source_type, source_expr, default_value
            FROM proposal_template_tokens
            WHERE template_id = ?;
            """,
            (int(template_id),),
        )
        meta_rows = cur.fetchall()
        meta_by_name: dict[str, tuple[str, str | None, str | None]] = {}
        for tname, source_type, source_expr, default_value in meta_rows:
            if not tname:
                continue
            meta_by_name[str(tname).upper()] = (
                str(source_type or "manual"),
                source_expr,
                default_value,
            )

        deal_row = None
        deal_cols: list[str] = []
        rfp_row = None
        rfp_cols: list[str] = []
        org_row = None
        org_cols: list[str] = []

        if deal_id is not None:
            try:
                cur.execute("SELECT * FROM deals WHERE id = ?;", (int(deal_id),))
                deal_row = cur.fetchone()
                if deal_row is not None:
                    deal_cols = [d[0] for d in cur.description]
            except Exception:
                deal_row = None
                deal_cols = []

        if rfp_id is not None and rfp_id != 0:
            # Try rfps first, then rfps_t as a fallback.
            try:
                cur.execute("SELECT * FROM rfps WHERE id = ?;", (int(rfp_id),))
                rfp_row = cur.fetchone()
                if rfp_row is not None:
                    rfp_cols = [d[0] for d in cur.description]
            except Exception:
                rfp_row = None
                rfp_cols = []
            if rfp_row is None:
                try:
                    cur.execute("SELECT * FROM rfps_t WHERE id = ?;", (int(rfp_id),))
                    rfp_row = cur.fetchone()
                    if rfp_row is not None:
                        rfp_cols = [d[0] for d in cur.description]
                except Exception:
                    rfp_row = None
                    rfp_cols = []

        try:
            cur.execute("SELECT * FROM org_profile WHERE id = 1;")
            org_row = cur.fetchone()
            if org_row is not None:
                org_cols = [d[0] for d in cur.description]
        except Exception:
            org_row = None
            org_cols = []

    def _resolve_token(name: str) -> str:
        # Manual values override everything.
        if name in manual_values:
            v = manual_values.get(name)
            return "" if v is None else str(v)

        meta = meta_by_name.get(name)
        if not meta:
            # Unknown token: try manual again (case-insensitive) then blank.
            for k, v in manual_values.items():
                if k.upper() == name:
                    return "" if v is None else str(v)
            return ""

        source_type, source_expr, default_value = meta
        source_type = (source_type or "manual").lower()
        source_expr = (source_expr or "").strip()
        default_value = default_value or ""

        try:
            if source_type == "deal" and deal_row is not None and deal_cols:
                if source_expr and source_expr in deal_cols:
                    idx = deal_cols.index(source_expr)
                    val = deal_row[idx]
                    return "" if val is None else str(val)
            elif source_type == "rfp" and rfp_row is not None and rfp_cols:
                if source_expr and source_expr in rfp_cols:
                    idx = rfp_cols.index(source_expr)
                    val = rfp_row[idx]
                    return "" if val is None else str(val)
            elif source_type == "org" and org_row is not None and org_cols:
                if source_expr and source_expr in org_cols:
                    idx = org_cols.index(source_expr)
                    val = org_row[idx]
                    return "" if val is None else str(val)
            elif source_type in ("manual", "static"):
                # For manual/static tokens, fall back to default.
                return default_value
        except Exception:
            # On any lookup failure, fall back to default_value.
            return default_value

        return default_value

    def _repl(match: "_re.Match") -> str:
        tname = match.group(1) or ""
        tname = tname.strip().upper()
        if not tname:
            return ""
        return _resolve_token(tname)

    return _re.sub(r"{{\s*([A-Z0-9_]+)\s*}}", _repl, body)





# ---- Global owner context guard ----
try:
    import streamlit as _st
    # Backwards/forwards-compatible rerun alias so older/newer
    # Streamlit builds both expose st.rerun and st.experimental_rerun.
    try:
        if hasattr(_st, "rerun") and not hasattr(_st, "experimental_rerun"):
            _st.experimental_rerun = _st.rerun  # type: ignore[attr-defined]
        elif hasattr(_st, "experimental_rerun") and not hasattr(_st, "rerun"):
            _st.rerun = _st.experimental_rerun  # type: ignore[attr-defined]
    except Exception:
        # Best-effort; app should still run even if aliasing fails.
        pass

    if "deal_owner_ctx" not in _st.session_state:
        # Default to the first known user so views are user-isolated by default.
        _st.session_state["deal_owner_ctx"] = "Quincy"
    deal_owner_ctx = _st.session_state.get("deal_owner_ctx", "Quincy")
except Exception:
    deal_owner_ctx = "Quincy"
# ------------------------------------
# --- Global user / tenant model ---------------------------------
# Logical app users. Update this list as your team grows.
KNOWN_USERS = ["Quincy", "Collin", "Charles"]

# Default tenant name for this workspace. Existing data for tenant id 1
# will be labeled with this name when possible.
DEFAULT_TENANT_NAME = "ELA"


def get_current_user_name() -> str:
    """Single source of truth for the current logical user name."""
    try:
        import streamlit as st  # type: ignore[import-not-found]
        name = st.session_state.get("current_user_name")
        if not name:
            # Fallback to the first configured user
            name = KNOWN_USERS[0]
        return str(name).strip()
    except Exception:
        # Safe fallback if Streamlit or session state is not available
        return KNOWN_USERS[0]


def get_current_tenant_name(conn: "sqlite3.Connection | None" = None) -> str:
    """Resolve the current tenant's display name, defaulting to DEFAULT_TENANT_NAME.

    If a live DB connection is provided and the tenants/current_tenant tables
    exist, this will try to read the friendly tenant name from there.
    """
    # Try to read from DB if available
    if conn is not None:
        try:
            from contextlib import closing  # type: ignore[import-not-found]
            with closing(conn.cursor()) as cur:
                cur.execute(
                    "SELECT t.name "
                    "FROM current_tenant ct "
                    "JOIN tenants t ON t.id = ct.ctid "
                    "WHERE ct.id=1;"
                )
                row = cur.fetchone()
            if row and row[0]:
                return str(row[0]).strip()
        except Exception:
            # Fall back to static default if anything goes wrong
            pass
    return DEFAULT_TENANT_NAME


def get_current_tenant_id(conn: "sqlite3.Connection | None" = None) -> int:
    """Numeric tenant id for scoping queries. Defaults to 1 (ELA)."""
    if conn is not None:
        try:
            from contextlib import closing  # type: ignore[import-not-found]
            with closing(conn.cursor()) as cur:
                cur.execute("SELECT ctid FROM current_tenant WHERE id=1;")
                row = cur.fetchone()
            if row and row[0]:
                return int(row[0])
        except Exception:
            pass
    return 1
# ----------------------------------------------------------------

try:
    _pb_psychology_framework  # type: ignore[name-defined]
except NameError:
    def _pb_psychology_framework() -> str:
        return ""


def _ensure_selected_rfp_id(conn):
    """Resolve the active RFP id from session or DB and expose it as selected_rfp_id.

    Prefer Streamlit session state (current_rfp_id), then any existing global selected_rfp_id,
    then fall back to the most recent RFP id in the database.
    """
    try:
        import streamlit as st
    except Exception:
        st = None
    try:
        import pandas as pd
    except Exception:
        pd = None

    rid = None

    # 1. Prefer the Streamlit session's current_rfp_id if available
    if st is not None:
        try:
            _sid = st.session_state.get("current_rfp_id", None)
            if _sid:
                rid = int(_sid)
        except Exception:
            rid = None

    # 2. Fall back to any existing global selected_rfp_id only if session does not have one
    if rid is None:
        try:
            if "selected_rfp_id" in globals() and globals().get("selected_rfp_id"):
                rid = int(globals().get("selected_rfp_id"))
        except Exception:
            rid = None

    # 3. Finally, fall back to the latest RFP id in the database
    if rid is None and pd is not None:
        try:
            df = pd.read_sql_query("SELECT id FROM rfps ORDER BY id DESC LIMIT 1;", conn, params=())
            if df is not None and not df.empty:
                rid = int(df.iloc[0]["id"])
        except Exception:
            rid = None

    # Keep the global helper in sync for any legacy code paths
    try:
        globals()["selected_rfp_id"] = rid
    except Exception:
        pass

    return rid


# ---- UI style guide and helpers ----
"""
Internal UI style guide (keep short and practical):

- Page titles: use a single clear header at the top of each main page.
- Section headings: use expanders or subheaders to group related controls.
- Padding: leave a small visual gap between major blocks so content does not feel cramped.
- Cards and tables: keep titles bold, supporting text in normal weight, and use captions for meta info.

The helpers below are the preferred way to render new UI going forward.
Older sections can be gradually updated to use them when you touch that area.
"""

def render_page_title(text: str, subtitle: str | None = None) -> None:
    """Standard page title pattern for new screens."""
    import streamlit as st
    st.header(text)
    if subtitle:
        st.caption(subtitle)

def render_section(title: str, help_text: str | None = None, expanded: bool = True):
    """Standard section wrapper. Returns a container inside an expander."""
    import streamlit as st
    exp = st.expander(title, expanded=expanded)
    with exp:
        if help_text:
            st.caption(help_text)
        # Container allows the caller to further structure the section
        return st.container()

def render_card(title: str, body: str | None = None, footer: str | None = None):
    """Simple card pattern for summary blocks and key info."""
    import streamlit as st
    with st.container():
        st.markdown(f"**{title}**")
        if body:
            st.write(body)
        if footer:
            st.caption(footer)

# ---- Status messaging and empty states ----

def _ui_join(message: str, details: str | None = None) -> str:
    if details:
        return f"{message} — {details}"
    return message

def ui_success(message: str, details: str | None = None) -> None:
    """Standard success message."""
    import streamlit as st
    st.success(_ui_join(f"Success: {message}", details))

def ui_warning(message: str, details: str | None = None) -> None:
    """Standard warning message."""
    import streamlit as st
    st.warning(_ui_join(f"Heads up: {message}", details))

def ui_error(message: str, details: str | None = None) -> None:
    """Standard error message."""
    import streamlit as st
    st.error(_ui_join(f"Something went wrong: {message}", details))

def ui_info(message: str, details: str | None = None) -> None:
    """Standard info / guidance message."""
    import streamlit as st
    st.info(_ui_join(f"FYI: {message}", details))

def render_empty_state(title: str, description: str, primary_action_label: str | None = None, key: str | None = None):
    """Friendly empty state with an optional primary action button.

    Returns True if the primary action button is clicked.
    """
    import streamlit as st
    with st.container():
        st.markdown(f"### {title}")
        st.caption(description)
        clicked = False
        if primary_action_label:
            clicked = st.button(primary_action_label, key=key)
        return clicked

# ---- End UI helpers ----

# === Proposal Builder normalization helpers ===

def _delete_rfp_everywhere(conn, rfp_id: int) -> None:
    """Delete an RFP and its related analyzer records.

    Best-effort cascade across the main RFP-related tables so UI stays clean.
    Safe if some tables have no rows for this id.
    """
    import sqlite3 as _sqlite
    from contextlib import closing as _closing

    try:
        rid = int(rfp_id)
    except Exception:
        return

    tables = [
        # Core One-Page / analyzer artifacts
        "rfp_files",
        "rfp_files_t",
        "rfp_chunks",
        "rfp_meta",
        "rfp_sections",
        "lm_items",
        "clin_lines",
        "key_dates",
        "pocs",
        "compliance_requirements",
        "compliance_links",
        "rtm_requirements",
        "rfp_chat",
        "rfp_chat_turns",
        "y2_threads",
        "draft_snippets",
        # Downstream artifacts tied back to this RFP
        "quote_totals",
        "proposals",
        "quotes",
        "pricing_scenarios",
        "white_papers",
        "rfq_packs",
        "sam_versions",
        # CRM / deals that are explicitly tied to this RFP
        "deals",
    ]

    with _closing(conn.cursor()) as cur:
        for tbl in tables:
            try:
                cur.execute(f"DELETE FROM {tbl} WHERE rfp_id = ?", (rid,))
            except Exception:
                # Table may not have rfp_id or may not exist in this DB version
                pass
        # Finally delete the RFP shell itself
        try:
            cur.execute("DELETE FROM rfps WHERE id = ?", (rid,))
        except Exception:
            pass
        conn.commit()
def _pb_try_json(text: str):
    try:
        import json as _json
        return _json.loads(text)
    except Exception:
        return None

def _pb_strip_html(text: str) -> str:
    if not isinstance(text, str) or ("<" not in text and ">" not in text):
        return text
    import re as _re
    t = text
    t = _re.sub(r"(?is)<(script|style)[^>]*>.*?</\1>", "", t)
    t = _re.sub(r"(?is)<br\s*/?>", "\n", t)
    t = _re.sub(r"(?is)</p\s*>", "\n\n", t)
    t = _re.sub(r"(?is)</li\s*>", "\n", t)
    t = _re.sub(r"(?is)<li[^>]*>\s*", "- ", t)
    t = _re.sub(r"(?is)</h[1-6]\s*>", "\n\n", t)
    t = _re.sub(r"(?is)<[^>]+>", "", t)
    t = _re.sub(r"\n{3,}", "\n\n", t)
    return t.strip()

def _pb_struct_to_text(obj) -> str:
    import pandas as _pd
    lines = []
    if isinstance(obj, dict):
        paras = []
        for k in ("paragraphs","method","rationale","sequence","sequence_of_work","approach"):
            v = obj.get(k)
            if isinstance(v, str) and v.strip():
                paras.append((k, v.strip()))
            elif isinstance(v, list):
                for p in v:
                    if isinstance(p, str) and p.strip():
                        paras.append((k, p.strip()))
        for _, p in paras:
            lines.append(p); lines.append("")
        bullets = obj.get("bullets") or {k: obj[k] for k in ["features","compliance","evidence","benefits"] if k in obj}
        if isinstance(bullets, dict):
            for hdr, items in bullets.items():
                if not items: continue
                lines.append(f"**{str(hdr).title()}**")
                if isinstance(items, list):
                    for b in items:
                        if isinstance(b, str) and b.strip():
                            lines.append(f"- {b.strip()}")
                elif isinstance(items, str):
                    for b in items.splitlines():
                        b = b.strip()
                        if b:
                            lines.append(f"- {b}")
                lines.append("")
        elif isinstance(bullets, list):
            for b in bullets:
                if isinstance(b, str) and b.strip():
                    lines.append(f"- {b.strip()}")
            lines.append("")
        tables = obj.get("tables")
        if isinstance(tables, list):
            for t in tables:
                try:
                    df = _pd.DataFrame(t)
                    if not df.empty:
                        hdr = "| " + " | ".join(map(str, df.columns)) + " |"
                        sep = "| " + " | ".join("---" for _ in df.columns) + " |"
                        lines.append(hdr); lines.append(sep)
                        for _, row in df.iterrows():
                            lines.append("| " + " | ".join("" if _pd.isna(x) else str(x) for x in row.values) + " |")
                        lines.append("")
                except Exception:
                    continue
    return "\n".join(lines).strip() if lines else ""

def _pb_normalize_text(text: str) -> str:
    if not isinstance(text, str) or not text.strip():
        return text
    obj = _pb_try_json(text)
    if obj is not None:
        try:
            text = _pb_struct_to_text(obj) or ""
        except Exception:
            pass
    text = _pb_strip_html(text)
    text = text.replace("\r\n", "\n").replace("\n", "\n")
    import re as _re
    text = _re.sub(r"[ \t]{3,}", "  ", text)
    text = _one_idea_per_paragraph(text)
    return text.strip()


def _pb__write_md(doc, text: str, font_name: str = "Calibri", font_size_pt: int = 11, line_spacing: float | int | str = 1.15) -> None:
    """
    Very small markdown-to-DOCX bridge for headings, bullet lists, and paragraphs.
    This is intentionally light so it can be reused by White Paper and Proposal exports.
    """
    if not text:
        return
    try:
        from docx.enum.text import WD_PARAGRAPH_ALIGNMENT  # type: ignore
        from docx.shared import Pt  # type: ignore
    except Exception:
        WD_PARAGRAPH_ALIGNMENT = None
        Pt = None

    lines = str(text).split("\n")
    for raw in lines:
        line = raw.rstrip()
        if not line:
            doc.add_paragraph("")
            continue

        style = None
        bullet = False

        # Very light markdown handling: headings and bullets
        if line.startswith("### "):
            style = "Heading 3"
            line = line[4:]
        elif line.startswith("## "):
            style = "Heading 2"
            line = line[3:]
        elif line.startswith("# "):
            style = "Heading 1"
            line = line[2:]
        elif line.lstrip().startswith(("-", "*")):
            bullet = True
            # Strip leading bullet char
            stripped = line.lstrip()
            line = stripped[1:].lstrip()

        if bullet:
            p = doc.add_paragraph(line, style="List Bullet")
        elif style:
            p = doc.add_paragraph(line, style=style)
        else:
            p = doc.add_paragraph(line)

        # Apply font overrides if available
        try:
            if Pt is not None:
                for run in p.runs:
                    if font_name:
                        run.font.name = font_name
                    if font_size_pt:
                        run.font.size = Pt(font_size_pt)
            # Line spacing
            if hasattr(p, "paragraph_format"):
                pf = p.paragraph_format
                if isinstance(line_spacing, (int, float)):
                    pf.line_spacing = line_spacing
        except Exception:
            # Styling issues should not break exports
            pass


# ==== Draft Finalization Guards (top-of-file) ====
try:
    _strip_citations
except NameError:
    def _strip_citations(text: str) -> str:
        import re as _re
        if not text:
            return text
        t = str(text)
        t = _re.sub(r"\s*\[\d+\]", "", t)
        t = _re.sub(r"\s*\(\d+\)", "", t)
        t = _re.sub(r"\n+references?:\n.*$", "", t, flags=_re.I|_re.S)
        t = _re.sub(r"^source:.*$", "", t, flags=_re.I|_re.M)
        t = _re.sub(r"\n{3,}", "\n\n", t)
        return t.strip()

try:
    _enforce_style_guide
except NameError:
    def _enforce_style_guide(text: str, max_words=10, max_sents_per_para=10) -> str:
        return str(text or "").strip()

try:
    _finalize_draft
except NameError:
    def _finalize_draft(text: str) -> str:
        try:
            t = _strip_citations(text)
        except Exception:
            t = str(text or "")
        try:
            t = _enforce_style_guide(t)
        except Exception:
            pass
        try:
            import re as _re_fix
            # Headings and terms: Assumptions -> Dependencies
            t = _re_fix.sub(r'(?im)^\s*assumptions\s*:?', 'Dependencies', t)
            def _swap_dep(mo):
                word = mo.group(0)
                # preserve case
                if word.isupper():
                    return 'DEPENDENCIES' if word.endswith('S') else 'DEPENDENCY'
                if word[0].isupper():
                    return 'Dependencies' if word.endswith('s') else 'Dependency'
                return 'dependencies' if word.endswith('s') else 'dependency'
            t = _re_fix.sub(r'(?i)\bassumptions\b', _swap_dep, t)
            t = _re_fix.sub(r'(?i)\bassumption\b', _swap_dep, t)
        except Exception:
            pass
        return t
# ================================================

# ==== Style Guide Guards (top-of-file) ====
try:
    _style_guide
except NameError:  # define if missing
    def _style_guide() -> str:
        return (
            "Follow these rules strictly:\n"
            "1) Understand client need. Mirror solicitation terms exactly.\n"
            "2) State deliverables explicitly.\n"
            "3) Address evaluation factors: technical, management, past performance, price.\n"
            "4) Answer each L&M requirement directly.\n"
            "5) Obey page and format rules.\n"
            "6) Provide HOW procedures, not claims.\n"
            "7) Short sentences (<=10 words). One idea per paragraph.\n"
            "8) Use bullets. Clean headings.\n"
            "9) Include roles, equipment, timeline, QC checks, metrics.\n"
            "10) Identify subcontractors and responsibilities.\n"
            "11) Organize clearly for easy scoring.\n"
            "12) Include a Risk table with mitigations.\n"
            "13) Add a brief L&M compliance crosswalk.\n"
            "14) Keep tone federal and precise."
            "15) Refer to the contractor as 'ELA Management' instead of 'we' or 'us'. "
            "Avoid first-person plural."
        )

try:
    PROPOSAL_STYLE_GUIDE
except NameError:  # set if missing
    PROPOSAL_STYLE_GUIDE = _style_guide()
# ==========================================

def _s1d_haversine_mi(lat1, lon1, lat2, lon2):
    try:
        from math import radians, sin, cos, asin, sqrt
        if None in (lat1, lon1, lat2, lon2):
            return None
        dlat = radians(lat2 - lat1)
        dlon = radians(lon2 - lon1)
        a = sin(dlat/2)**2 + cos(radians(lat1)) * cos(radians(lat2)) * sin(dlon/2)**2
        return 3958.756 * 2 * asin(sqrt(a))
    except Exception:
        return None

from html import escape as _esc



import logging
from zipfile import ZipFile, ZIP_DEFLATED

# === Architecture layers ======================================================
# Data layer: database connections and raw SQL helpers.
# Service layer: business operations that orchestrate data functions.
# UI layer: Streamlit rendering and user interaction.
# New work should respect this separation to keep behavior stable and extensible.
# ==============================================================================
logger = logging.getLogger("ela_app")
if not logger.handlers:
    logging.basicConfig(level=logging.INFO)
# ==============================================================================

import sqlite3 as _sq  # global alias

# === One-paragraph-per-idea enforcement ===
def _one_idea_per_paragraph(text: str) -> str:
    import re as _re
    if not text:
        return text
    t = str(text)
    t = _re.sub(r"(?m)^(?:-|\*|\d+\.)\s", r"\n\g<0>", t)
    t = _re.sub(r"\n{3,}", "\n\n", t)
    markers = r"(?:^|\s)(Moreover|Furthermore|Additionally|Also|Next|Then|Therefore|Thus|However|In addition|Separately|Risk:|Mitigation:|Method:|Rationale:|Sequence:|Benefit:)"
    def split_para(p):
        sents = _re.split(r"(?<=[.!?])\s+", p.strip())
        out = []
        for s in sents:
            if not s:
                continue
            if _re.search(markers, s):
                out.append("\n" + s.strip())
            else:
                out.append(s.strip())
        joined = " ".join(out).replace("\n ", "\n").strip()
        chunks = _re.split(r"(?<=[.!?])\s+", joined)
        if len(chunks) > 4:
            parts = []
            for i in range(0, len(chunks), 4):
                parts.append(" ".join(chunks[i:i+4]).strip())
            return "\n\n".join([p for p in parts if p])
        return joined
    paras = _re.split(r"\n\s*\n", t.strip())
    processed = [split_para(p) for p in paras if p.strip()]
    out = "\n\n".join(processed)
    out = _re.sub(r"\n{3,}", "\n\n", out).strip()
    return out



# ===== Structure-aware drafting helpers =====
def _normalize_section_name(name: str) -> str:
    n = (name or "").strip().lower()
    aliases = {
        "technical approach": "technical",
        "management approach": "management",
        "staffing and key personnel": "staffing",
        "quality assurance / qc": "qc",
        "quality assurance": "qc",
        "qa": "qc",
        "qc": "qc",
        "risks and mitigations": "risk",
        "risk management": "risk",
        "executive summary": "exec",
        "cover letter": "cover",
        "pricing narrative": "price",
        "pricing narrative (non-cost)": "price",
        "compliance crosswalk": "compliance",
        "past performance": "past",
        "understanding of requirements": "exec",
        "subcontractor plan": "subs",
    }
    return aliases.get(n, n)

def _section_structure_rules(section_title: str) -> dict:
    k = _normalize_section_name(section_title)
    rules = {
        "cover":      {"p": True, "b": True,  "t": False, "tables": []},
        "exec":       {"p": True, "b": True,  "t": False, "tables": []},
        "technical":  {"p": True, "b": True,  "t": True,  "tables": ["requirements", "schedule", "staffing"]},
        "management": {"p": True, "b": True,  "t": True,  "tables": ["raci", "reporting"]},
        "staffing":   {"p": True, "b": True,  "t": True,  "tables": ["staffing"]},
        "qc":         {"p": True, "b": True,  "t": True,  "tables": ["qc_metrics"]},
        "risk":       {"p": True, "b": True,  "t": True,  "tables": ["risk_register"]},
        "price":      {"p": True, "b": True,  "t": True,  "tables": ["clin_map"]},
        "compliance": {"p": False,"b": True,  "t": True,  "tables": ["crosswalk"]},
        "past":       {"p": True, "b": True,  "t": True,  "tables": ["past_perf"]},
        "subs":       {"p": True, "b": True,  "t": True,  "tables": ["subs_scope"]},
    }
    return rules.get(k, {"p": True, "b": True, "t": False, "tables": []})

def _mk_md_table(headers, rows):
    cols = len(headers)
    safe = [[str(c).replace("|","/") for c in headers]]
    for r in rows:
        if len(r) < cols:
            r = list(r) + [""]*(cols-len(r))
        safe.append([str(c).replace("|","/") for c in r[:cols]])
    sep = ["---"]*cols
    lines = ["| " + " | ".join(safe[0]) + " |", "| " + " | ".join(sep) + " |"]
    for r in safe[1:]:
        lines.append("| " + " | ".join(r) + " |")
    return "\n".join(lines)

def _auto_tables_for_section(conn, rfp_id: int, section_title: str) -> str:
    import pandas as _pd
    k = _normalize_section_name(section_title)
    out = []
    def add(name, headers, rows):
        out.append(f"**{name}**\n" + _mk_md_table(headers, rows))
    try:
        df_lm = _pd.read_sql_query("SELECT section, item_text AS item FROM lm_items WHERE rfp_id=? ORDER BY id;", conn, params=(int(rfp_id),))
    except Exception:
        df_lm = None
    try:
        df_dates = _pd.read_sql_query("SELECT label, date_text FROM key_dates WHERE rfp_id=? ORDER BY id;", conn, params=(int(rfp_id),))
    except Exception:
        df_dates = None
    try:
        df_pocs = _pd.read_sql_query("SELECT name, role, email, phone FROM pocs WHERE rfp_id=? ORDER BY id;", conn, params=(int(rfp_id),))
    except Exception:
        df_pocs = None
    try:
        df_clin = _pd.read_sql_query("SELECT clin, description, qty, unit FROM clin_lines WHERE rfp_id=? ORDER BY id;", conn, params=(int(rfp_id),))
    except Exception:
        df_clin = None
    rules = _section_structure_rules(section_title)
    if not rules.get("t"):
        return ""
    for tname in rules.get("tables", []):
        if tname == "requirements" and df_lm is not None and not df_lm.empty:
            rows = [[str(i+1), r["section"], r["item"]] for i, (_, r) in enumerate(df_lm.head(30).iterrows())]
            add("Requirements Crosswalk (L&M)", ["#", "Section", "Requirement"], rows)
        if tname == "schedule" and df_dates is not None and not df_dates.empty:
            rows = [[r["label"], r["date_text"]] for _, r in df_dates.iterrows()]
            add("Key Schedule", ["Milestone", "Date"], rows)
        if tname == "staffing":
            if df_pocs is not None and not df_pocs.empty:
                rows = [[r["name"], r["role"], r["email"] or "", r["phone"] or ""] for _, r in df_pocs.iterrows()]
                add("Staffing / Points of Contact", ["Name","Role","Email","Phone"], rows)
        if tname == "raci":
            if df_pocs is not None and not df_pocs.empty:
                rows = [[r["role"], r["name"], "R/A", "C", "I"] for _, r in df_pocs.iterrows()]
                add("RACI Overview", ["Role","Owner","R/A","C","I"], rows)
        if tname == "reporting" and df_dates is not None and not df_dates.empty:
            rows = [["Monthly Status Report","Monthly","CO/CS"], ["Invoice Package","Monthly","CO"]]
            add("Reporting Cadence", ["Deliverable","Frequency","Audience"], rows)
        if tname == "qc_metrics":
            rows = [["On-time delivery","≥ 98%","Key dates"],
                    ["Defect rate","≤ 1%","Inspection"],
                    ["Response time","≤ 1 business day","Ticket log"]]
            add("QC Metrics", ["Metric","Target","Evidence"], rows)
        if tname == "risk_register":
            rows = [["Late delivery","Vendor delay","Schedule","Medium","Secondary supplier; reorder point","PM","Late ASN"],
                    ["Spec mismatch","Ambiguity","Quality","Low","Pre-bid RFIs; submittal checks","QA","Rejected submittal"]]
            add("Risk Register", ["Risk","Cause","Impact","Likelihood","Mitigation","Owner","Trigger"], rows)
        if tname == "clin_map" and df_clin is not None and not df_clin.empty:
            rows = [[r["clin"], r["description"], r["qty"], r["unit"]] for _, r in df_clin.iterrows()]
            add("CLIN Mapping", ["CLIN","Description","Qty","Unit"], rows)
        if tname == "crosswalk" and df_lm is not None and not df_lm.empty:
            rows = [[r["section"], r["item"], "Addressed"] for _, r in df_lm.head(40).iterrows()]
            add("Compliance Crosswalk", ["L&M Section","Item","Status"], rows)
        if tname == "past_perf":
            rows = [["VA Facility O&M","2023","On-time, 0 defects","Contract complete"],
                    ["USCG Grounds","2024","Met SLAs, 100% QASP","Option exercised"]]
            add("Past Performance Summary", ["Project","Year","Outcome","Notes"], rows)
        if tname == "subs_scope":
            rows = [["ABC HVAC","HVAC service","CLIN 0002","QASP 3.2"],
                    ["XYZ Janitorial","Custodial","CLIN 0003","QASP 4.1"]]
            add("Subcontractor Scope", ["Vendor","Scope","CLIN","QC Ref"], rows)
    return "\n\n".join(out).strip()

def _append_tables_if_applicable(conn, rfp_id: int, section_title: str, body_text: str) -> str:
    try:
        md_tables = _auto_tables_for_section(conn, int(rfp_id), section_title)
    except Exception:
        md_tables = ""
    if md_tables:
        return (str(body_text).rstrip() + "\n\n" + md_tables).strip()
    return str(body_text)

# --- early stub: ensures __p_call_sig_ui exists before any imports call it ---
try:
    __p_call_sig_ui  # noqa
except NameError:
    def __p_call_sig_ui(conn):
        """
        Early Signature Editor stub.
        Dispatches to a later full implementation if present.
        Otherwise provides a minimal, working editor.
        """
        import streamlit as st

        # If a later, full implementation is available, delegate.
        for name in ("__p_o4_signature_ui", "__p_signature_ui", "__p_call_sig_ui_full"):
            fn = globals().get(name)
            if callable(fn):
                return fn(conn)

        # Use Outreach connection if provided by app
        sconn = (get_o4_conn() if "get_o4_conn" in globals() else None) or conn

        # Sender discovery: prefer app helper
        senders = []
        try:
            _gs = globals().get("_get_senders")
            if callable(_gs):
                rows = _gs(sconn) or []
                for r in rows:
                    if isinstance(r, (list, tuple)) and r:
                        email = r[0]
                        name = (r[1] if len(r) > 1 else "") or ""
                    elif isinstance(r, dict):
                        email = r.get("email") or r.get("username") or ""
                        name = r.get("name") or r.get("display_name") or r.get("label") or ""
                    else:
                        continue
                    if email:
                        senders.append((email, name or email))
        except Exception:
            pass
        # Fallback table scans
        try:
            cur = sconn.cursor()
            for q in (
                "SELECT user_email, COALESCE(display_name, name, '') FROM email_accounts",
                "SELECT email, COALESCE(name, display_name, label, '') FROM o4_senders",
                "SELECT email, COALESCE(display_name, '') FROM senders",
                "SELECT username, COALESCE(label, '') FROM smtp_settings",
                "SELECT email, COALESCE(label, display, name, '') FROM outreach_sender_accounts",
            ):
                try:
                    for email, disp in cur.execute(q).fetchall() or []:
                        if email:
                            senders.append((email, disp or email))
                except Exception:
                    continue
        except Exception:
            pass

        # De-dup by email
        dd = {}
        for email, name in senders:
            if email not in dd or (name and name != email):
                dd[email] = name or email
        senders = [(e, dd[e]) for e in sorted(dd.keys())]

        # Minimal UI
        if not senders:
            st.info("No senders found. Add one under Outreach → Sender Accounts, then set a signature.")
            return

        labels = [f"{(name or email)} — {email}" for (email, name) in senders]
        choice = st.selectbox("Sender", labels, key="__p_sig_sender_stub")
        email = senders[labels.index(choice)][0]

        # Ensure schema and load
        cur = sconn.cursor()
        cur.execute("""
            CREATE TABLE IF NOT EXISTS outreach_signatures(
                email TEXT PRIMARY KEY,
                signature_html TEXT DEFAULT '',
                logo_blob BLOB,
                logo_mime TEXT,
                logo_name TEXT
            )
        """)
        row = cur.execute(
            "SELECT signature_html FROM outreach_signatures WHERE lower(email)=lower(?)",
            (email,)
        ).fetchone()
        html_default = (row[0] if row else "") or ""
        sig_html = st.text_area("Signature HTML", html_default, height=180, key=f"__p_sig_html_{email}")
        logo = st.file_uploader("Logo image (optional)", type=["png","jpg","jpeg","gif"], key=f"__p_sig_logo_{email}")

        col1, col2 = st.columns(2)
        with col1:
            if st.button("Save signature", key=f"__p_sig_save_{email}"):
                data = logo.getvalue() if logo is not None else None
                name = getattr(logo, "name", None) if logo is not None else None
                ext = (name or "").lower().rsplit(".", 1)[-1] if name and "." in name else ""
                mime = {"png":"image/png","jpg":"image/jpeg","jpeg":"image/jpeg","gif":"image/gif"}.get(ext, "application/octet-stream")
                try:
                    sconn.execute("""INSERT INTO outreach_signatures(email, signature_html, logo_blob, logo_mime, logo_name)
                                     VALUES (?, ?, ?, ?, ?)""", (email, sig_html, data, mime, name))
                except Exception:
                    sconn.execute("""UPDATE outreach_signatures
                                     SET signature_html=?,
                                         logo_blob=COALESCE(?,logo_blob),
                                         logo_mime=COALESCE(?,logo_mime),
                                         logo_name=COALESCE(?,logo_name)
                                     WHERE lower(email)=lower(?)""", (sig_html, data, mime, name, email))
                sconn.commit()
                st.success("Saved")
        with col2:
            if st.button("Remove logo", key=f"__p_sig_remove_{email}"):
                sconn.execute("UPDATE outreach_signatures SET logo_blob=NULL, logo_mime=NULL, logo_name=NULL WHERE lower(email)=lower(?)", (email,))
                sconn.commit()
                st.success("Logo removed")

        st.caption("Preview")
        st.markdown(sig_html or "", unsafe_allow_html=True)


def __p_render_signature(conn, sender_email, body_html):
    """
    Render the stored Outreach email signature for the given sender and
    append it to the provided HTML body.

    Returns a tuple of (html_with_signature, inline_images).

    inline_images is a list of dicts like
    {"cid": "...", "content": bytes, "mime": "image/png"} that should be
    attached as inline images to the email message.
    """
    if conn is None:
        return body_html, []

    try:
        cur = conn.cursor()
        # Ensure the same schema used by the signature editor
        cur.execute("""
            CREATE TABLE IF NOT EXISTS outreach_signatures(
                email TEXT PRIMARY KEY,
                signature_html TEXT DEFAULT '',
                logo_blob BLOB,
                logo_mime TEXT,
                logo_name TEXT
            )
        """)
        row = cur.execute(
            "SELECT signature_html, logo_blob, logo_mime FROM outreach_signatures WHERE lower(email)=lower(?)",
            ((sender_email or "").strip(),),
        ).fetchone()
    except Exception:
        return body_html, []

    if not row:
        return body_html, []

    sig_html, logo_blob, logo_mime = row
    sig_html = sig_html or ""
    logo_html = ""
    inline_images = []

    # Prefer CID-based inline image instead of large data URIs. This keeps
    # the HTML body small so Gmail is less likely to clip the message while
    # still displaying the logo inline in supported clients.
    if logo_blob:
        cid_base = (sender_email or "logo").replace("@", "_at_").replace(">", "").replace("<", "")
        cid = f"{cid_base}_siglogo"
        mime = logo_mime or "image/png"
        logo_html = (
            f'<div style="margin-bottom:4px;">'
            f'<img src="cid:{cid}" alt="Logo" '
            f'style="width:120px;max-width:120px;height:auto;border:0;" />'
            f"</div>"
        )
        inline_images.append({"cid": cid, "content": logo_blob, "mime": mime})

    final_sig = sig_html
    if logo_html:
        if "{{LOGO}}" in final_sig:
            final_sig = final_sig.replace("{{LOGO}}", logo_html)
        else:
            final_sig = logo_html + ("<br>" + final_sig if final_sig else "")

    if not final_sig:
        return body_html, []

    body_html = body_html or ""
    if body_html.strip():
        combined = body_html.rstrip() + "<br><br>" + final_sig
    else:
        combined = final_sig

    return combined, inline_images

# --- end early stub ---

# === BEGIN READSQL SHIM ===
try:
    import pandas as _pd
    # local wrapper if not defined
    if "__p_read_sql_query" not in globals():
        def __p_read_sql_query(q, conn, params=()):
            try:
                # try to strip '#' comments if helper exists
                q2 = __p_strip_sql_hash_comments(q) if isinstance(q, str) else q
            except Exception:
                q2 = q
            return _pd.read_sql_query(q2, conn, params=params)
    # attach shim to pandas in case any call uses pandas.__p_read_sql_query
    if not hasattr(_pd, "__p_read_sql_query"):
        def __pandas_read_sql_shim(q, conn, params=()):
            return __p_read_sql_query(q, conn, params=params)
        _pd.__p_read_sql_query = __pandas_read_sql_shim
except Exception:
    pass
# === END READSQL SHIM ===

import re
import streamlit as st

# === Perf+QA helpers ===
import traceback as _traceback
from contextlib import closing as _closing
from concurrent.futures import ThreadPoolExecutor as _ThreadPoolExecutor, as_completed as _as_completed

def _ensure_debug_schema(conn):
    try:
        with conn:
            conn.execute("""CREATE TABLE IF NOT EXISTS debug_log(
                id INTEGER PRIMARY KEY, context TEXT, level TEXT, message TEXT, traceback TEXT,
                created_at TEXT DEFAULT CURRENT_TIMESTAMP)""" )
    except Exception:
        pass

def _debug_log(conn, context, exc):
    try:
        tb = _traceback.format_exc()
        with conn:
            conn.execute("INSERT INTO debug_log(context, level, message, traceback) VALUES(?,?,?,?)",
                         (str(context)[:200], "ERROR", str(exc)[:500], tb[:4000]))
    except Exception:
        pass

def ensure_unified_schemas(conn):
    _ensure_debug_schema(conn)
    with _closing(conn.cursor()) as cur:
        # rfps table and view alias
        try:
            cur.execute("""CREATE TABLE IF NOT EXISTS rfps(
                id INTEGER PRIMARY KEY, title TEXT, solnum TEXT, notice_id TEXT, sam_url TEXT, file_path TEXT, created_at TEXT)""" )
            cur.execute("DROP VIEW IF EXISTS rfps_t;");
            cur.execute("CREATE VIEW IF NOT EXISTS rfps_t AS SELECT id,title,solnum,notice_id,sam_url,file_path,created_at,tenant_id FROM rfps WHERE tenant_id=(SELECT ctid FROM current_tenant WHERE id=1) OR tenant_id IS NULL;")
        except Exception as e: _debug_log(conn, "rfps", e)
        try:
            cur.execute("CREATE INDEX IF NOT EXISTS idx_rfps_tenant ON rfps(tenant_id)")
        except Exception as e:
            _debug_log(conn, "rfps_tenant_index", e)
        # vendors table and view alias + indexes
        try:
            cur.execute("""CREATE TABLE IF NOT EXISTS vendors(
                id INTEGER PRIMARY KEY, name TEXT, display_name TEXT, website TEXT, cage TEXT, uei TEXT, naics TEXT,
                city TEXT, state TEXT, phone TEXT, email TEXT, notes TEXT, place_id TEXT, normalized_name TEXT, fit_score REAL, tags TEXT)""" )
            cur.execute("CREATE VIEW IF NOT EXISTS vendors_t AS SELECT id,name,phone,city,state,naics,cage,uei,website,notes FROM vendors;")
            cur.execute("CREATE UNIQUE INDEX IF NOT EXISTS idx_vendors_place_id ON vendors(place_id);")
            cur.execute("CREATE INDEX IF NOT EXISTS idx_vendors_norm_phone ON vendors(normalized_name, phone);")
            cur.execute("CREATE INDEX IF NOT EXISTS idx_vendors_naics_state ON vendors(naics, state);")
            # Ensure capability fields exist on vendors table
            try:
                for _col, _ctype in [
                    ("primary_naics", "TEXT"),
                    ("other_naics", "TEXT"),
                    ("coverage_locations", "TEXT"),
                    ("capability_tags", "TEXT"),
                    ("small_business_flag", "INTEGER"),
                    ("set_aside_flags", "TEXT"),
                ]:
                    try:
                        cur.execute("SELECT " + _col + " FROM vendors LIMIT 1;")
                    except Exception:
                        try:
                            cur.execute("ALTER TABLE vendors ADD COLUMN " + _col + " " + _ctype + ";")
                        except Exception as e2:
                            _debug_log(conn, "vendors_" + _col, e2)
            except Exception as e:
                _debug_log(conn, "vendors_capability_cols", e)

        except Exception as e: _debug_log(conn, "vendors", e)
        # vendor capacity and past performance
        try:
            cur.execute("""
                CREATE TABLE IF NOT EXISTS vendor_capacity(
                    id INTEGER PRIMARY KEY,
                    vendor_id INTEGER NOT NULL,
                    capacity_notes TEXT,
                    max_concurrent_jobs INTEGER,
                    max_annual_value REAL,
                    current_load_notes TEXT,
                    quality REAL,
                    schedule REAL,
                    communication REAL,
                    price REAL,
                    overall_score REAL,
                    last_updated TEXT DEFAULT CURRENT_TIMESTAMP
                )
            """)
            cur.execute("CREATE INDEX IF NOT EXISTS idx_vendor_capacity_vendor ON vendor_capacity(vendor_id);")
            cur.execute("""
                CREATE TABLE IF NOT EXISTS vendor_job(
                    id INTEGER PRIMARY KEY,
                    vendor_id INTEGER NOT NULL,
                    job_name TEXT,
                    agency TEXT,
                    contract_number TEXT,
                    naics TEXT,
                    role TEXT,
                    start_date TEXT,
                    end_date TEXT,
                    contract_value REAL,
                    quality REAL,
                    schedule REAL,
                    communication REAL,
                    price REAL,
                    overall_score REAL,
                    notes TEXT
                )
            """)
            cur.execute("CREATE INDEX IF NOT EXISTS idx_vendor_job_vendor ON vendor_job(vendor_id);")
        except Exception as e: _debug_log(conn, "vendor_capacity_job", e)
        # subfinder cache
        try:
            cur.execute("""CREATE TABLE IF NOT EXISTS subfinder_cache(
                place_id TEXT PRIMARY KEY, normalized_name TEXT, phone TEXT, website TEXT, google_url TEXT,
                updated_at TEXT DEFAULT CURRENT_TIMESTAMP)""" )
            cur.execute("CREATE UNIQUE INDEX IF NOT EXISTS idx_s1_cache_norm_phone ON subfinder_cache(normalized_name, phone);")
        except Exception as e: _debug_log(conn, "subfinder_cache", e)
        # quote totals + indexes
        try:
            cur.execute("CREATE INDEX IF NOT EXISTS idx_quotes_rfp ON quotes(rfp_id);")
            cur.execute("CREATE INDEX IF NOT EXISTS idx_quote_lines ON quote_lines(quote_id, clin);")
            cur.execute("""CREATE TABLE IF NOT EXISTS quote_totals(
                rfp_id INTEGER NOT NULL, vendor TEXT NOT NULL, total REAL NOT NULL,
                updated_at TEXT DEFAULT CURRENT_TIMESTAMP, PRIMARY KEY(rfp_id, vendor))""" )
        except Exception as e: _debug_log(conn, "quotes", e)
    conn.commit()


def ensure_pb_templates_schema(conn):
    """Ensure proposal_templates and proposal_template_tokens tables exist."""
    with _closing(conn.cursor()) as cur:
        try:
            cur.execute(
                """
                CREATE TABLE IF NOT EXISTS proposal_templates (
                    id INTEGER PRIMARY KEY,
                    tenant_id INTEGER DEFAULT 1,
                    name TEXT NOT NULL,
                    description TEXT,
                    template_type TEXT,
                    default_section TEXT,
                    body TEXT,
                    owner TEXT,
                    is_active INTEGER DEFAULT 1,
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP
                );
                """
            )
            cur.execute(
                "CREATE INDEX IF NOT EXISTS idx_proposal_templates_tenant "
                "ON proposal_templates(tenant_id);"
            )
        except Exception as e:
            _debug_log(conn, "schema.proposal_templates", e)
        try:
            cur.execute(
                """
                CREATE TABLE IF NOT EXISTS proposal_template_tokens (
                    id INTEGER PRIMARY KEY,
                    tenant_id INTEGER DEFAULT 1,
                    template_id INTEGER NOT NULL,
                    token_name TEXT NOT NULL,
                    source_type TEXT,
                    source_expr TEXT,
                    default_value TEXT,
                    description TEXT,
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY(template_id) REFERENCES proposal_templates(id)
                );
                """
            )
            cur.execute(
                "CREATE INDEX IF NOT EXISTS idx_template_tokens_template "
                "ON proposal_template_tokens(template_id);"
            )
        except Exception as e:
            _debug_log(conn, "schema.proposal_template_tokens", e)


def safe_read_sql(conn, sql: str, params: tuple = ()):
    """Bypass pandas for simple SELECT to avoid recursion from monkey-patching."""
    try:
        cur = conn.execute(sql, params or ())
        rows = cur.fetchall()
        cols = [d[0] for d in cur.description] if cur.description else []
        import pandas as _pd
        return _pd.DataFrame(rows, columns=cols)
    except Exception as e:
        raise e

def _recompute_quote_totals(conn, rfp_id:int):
    try:
        with _closing(conn.cursor()) as cur:
            cur.execute("DELETE FROM quote_totals WHERE rfp_id=?", (int(rfp_id),))
            cur.execute("""INSERT INTO quote_totals(rfp_id, vendor, total)
                           SELECT q.rfp_id, q.vendor, COALESCE(SUM(l.extended_price),0.0)
                           FROM quotes q LEFT JOIN quote_lines l ON q.id=l.quote_id
                           WHERE q.rfp_id=? GROUP BY q.rfp_id, q.vendor""", (int(rfp_id),))
        conn.commit()
    except Exception as e:
        _debug_log(conn, f"recompute_totals({rfp_id})", e)

def _s1d_get_details_cached(conn, pid: str, key: str):
    try:
        if not pid: return {}
        row = conn.execute("SELECT phone, website, google_url FROM subfinder_cache WHERE place_id=?", (pid,)).fetchone()
        if row: return {"formatted_phone_number": row[0] or "", "website": row[1] or "", "url": row[2] or ""}
        det = _s1d_place_details(pid, key)
        phone = _s1d_norm_phone(det.get("formatted_phone_number","")); website = det.get("website") or ""; gurl = det.get("url") or ""
        with conn: conn.execute("INSERT OR REPLACE INTO subfinder_cache(place_id, normalized_name, phone, website, google_url) VALUES (?, ?, ?, ?, ?,?)",
                                (pid, "", phone, website, gurl))
        return {"formatted_phone_number": phone, "website": website, "url": gurl}
    except Exception as e:
        _debug_log(conn, f"_s1d_get_details_cached({pid})", e); return {}

def _force_safe_pd_read():
    try:
        import pandas as _pd
        def _safe_reader(sql, conn=None, params=None, **kwargs):
            if conn is None:
                raise ValueError("conn required")
            _params = params or ()
            cur = conn.execute(sql, _params)
            rows = cur.fetchall()
            cols = [d[0] for d in cur.description] if cur.description else []
            return _pd.DataFrame(rows, columns=cols)
        _pd.read_sql_query = _safe_reader
    except Exception:
        pass
# === End helpers ===

def __p_ensure_column(conn, table: str, col: str, col_def: str):
    try:
        cur = conn.execute(f"PRAGMA table_info({table})")
        cols = [r[1] for r in cur.fetchall()]
        if col not in cols:
            conn.execute(f"ALTER TABLE {table} ADD COLUMN {col} {col_def}")
    except Exception:
        # best-effort migration only
        pass

# --- SQL cleaner to strip '#' comments not supported by SQLite ---
def __p_strip_sql_hash_comments(sql: str) -> str:
    out_lines = []
    for line in sql.splitlines():
        s = line
        in_single = False
        in_double = False
        esc = False
        cut = None
        for i, ch in enumerate(s):
            if esc:
                esc = False
                continue
            if ch == "\\\\":
                esc = True
                continue
            if ch == "'" and not in_double:
                in_single = not in_single
                continue
            if ch == '"' and not in_single:
                in_double = not in_double
                continue
            if ch == "#" and not in_single and not in_double:
                cut = i
                break
        if cut is not None:
            s = s[:cut]
        out_lines.append(s)
    return "\\n".join(out_lines)

def __p___p_read_sql_query(q, conn, params=()):
    try:
        import pandas as _pd
        q2 = __p_strip_sql_hash_comments(q)
        return _pd.__p_read_sql_query(q2, conn, params=params)
    except Exception as e:
        try:
            import streamlit as _st
            _st.warning(f"SQL read failed: {e}")
        except Exception:
            pass
        raise
# --- end SQL cleaner ---

import zipfile
import re

# ==== Section Focus Helpers ====
def _normalize_section_name(name: str) -> str:
    n = (name or "").lower().strip()
    mapping = {
        "technical approach": "technical",
        "management approach": "management",
        "staffing plan": "staffing",
        "quality control": "qc",
        "quality control plan": "qc",
        "risk": "risk",
        "risk management": "risk",
        "risks and mitigations": "risk",
        "transition plan": "transition",
        "cover letter": "cover",
        "executive summary": "exec",
        "price approach": "price",
        "pricing approach": "price",
        "past performance": "past",
        "subcontractor plan": "subs",
        "subcontracting plan": "subs",
        "compliance crosswalk": "compliance",
        "quality assurance": "qc",
        "quality assurance / qc": "qc",
        "qa": "qc",
        "qc": "qc",
        "compliance crosswalk": "compliance",
        "pricing narrative (non-cost)": "price",
        "management plan": "management",
        "understanding of requirements": "exec"
    }
    return mapping.get(n, n)

def _section_blueprint(section_title: str) -> str:
    k = _normalize_section_name(section_title)
    if k == "technical":
        return ("Write only the Technical Approach. Include: dependencies; step-by-step procedure; "
                "tools and materials; interfaces; schedule with dependencies; acceptance criteria and QC checks. "
                "Do not include management, staffing, risks, or crosswalks.")
    if k == "management":
        return ("Write only the Management Approach. Include: organization; RACI; communications; "
                "risk control loop; reporting cadence; change control. "
                "Do not include technical steps or cover letter.")
    if k == "staffing":
        return ("Write only the Staffing Plan. Include: roles; certifications; coverage; surge/backfill; training. "
                "Do not include technical steps or pricing.")
    if k == "qc":
        return ("Write only the Quality Control Plan. Include: QC objectives; inspections; metrics; "
                "nonconformance handling; corrective actions; audit schedule.")
    if k == "risk":
        return ("Write only the Risks and Mitigations. Provide 3–5 risks with causes, impacts, likelihood, and mitigations.")
    if k == "transition":
        return ("Write only the Transition Plan. Include: kickoff; knowledge transfer; asset handoff; cutover; Day-30 and Day-60 checks.")
    if k == "cover":
        return ("Write only the Cover Letter. One page max. Mirror solicitation title and number. "
                "State understanding, commitment, points of contact.")
    if k == "exec":
        return ("Write only the Executive Summary. State client need, proposed solution, value, and key discriminators.")
    if k == "price":
        return ("Write only the Price Approach note. Describe estimating basis, CLIN mapping, and dependencies. No numbers.")
    if k == "past":
        return ("Write only the Past Performance section. Two concise examples with relevance and outcomes.")
    if k == "subs":
        return ("Write only the Subcontractor Plan. Identify vendors, scope, oversight, and flow-down compliance.")
    if k == "compliance":
        return ("Write only the Compliance Crosswalk. Bullet mapping of L&M items to where addressed in the proposal.")
    return ("Write only the requested section content. Do not include other sections.")

def _clip_to_single_section(section_title: str, text: str) -> str:
    # Keep content up to first unrelated top-level heading
    if not text:
        return text
    k = _normalize_section_name(section_title)
    headings = [
        "Executive Summary","Technical Approach","Management Approach","Staffing","Quality Control",
        "Risk","Risks and Mitigations","Transition Plan","Price Approach","Past Performance",
        "Subcontractor Plan","Compliance Crosswalk","Cover Letter"
    ]
    # If the text clearly starts with the wrong heading, remove it
    import re as _re
    lines = text.splitlines()
    out_lines = []
    for ln in lines:
        m = _re.match(r"^\s*([A-Z][A-Za-z ]{2,40})\s*:?\s*$", ln.strip())
        if m:
            head = m.group(1).strip().lower()
            norm = _normalize_section_name(head)
            if norm and norm != k:
                break  # stop at first unrelated heading
        out_lines.append(ln)
    t = "\n".join(out_lines).strip()
    return t or text

def _finalize_section(section_title: str, text: str) -> str:
    try:
        clipped = _clip_to_single_section(section_title, text)
    except Exception:
        clipped = text
    return _finalize_draft(clipped)
# ==================================


# --- Phase 3 inline Analyzer (fallback if run_rfp_analyzer is missing) ---
def _phase3_analyzer_inline(conn):
    import os, io, zipfile, tempfile, re
    import streamlit as st
    from datetime import datetime

    # Safety: ensure required helpers exist
    try:
        _ensure_phase2_schema(conn)
    except Exception as e:
        st.info(f"Phase 2 schema: {e}")
    if ' _ensure_phase3_schema' not in globals():
        pass  # defined elsewhere in this file in Phase 3 block
    else:
        _ensure_phase3_schema(conn)

    st.markdown("### 🧠 Phase 3 — One‑Page Analyzer")
    notice = st.session_state.get("rfp_selected_notice") or {}
    title = notice.get("Title") or notice.get("Solicitation") or "Untitled RFP"
    sam_url = notice.get("SAM Link") or notice.get("URL") or ""

    # Prepare RFP record & dirs
    try:
        rfp_id = _get_or_create_rfp(conn, notice)
    except Exception:
        # minimal record if helper missing
        rfp_id = 1
    base_dir = "./rfp_store"
    try: os.makedirs(base_dir, exist_ok=True)
    except Exception:
        base_dir = "/tmp/rfp_store"; os.makedirs(base_dir, exist_ok=True)
    rfp_dir = os.path.join(base_dir, f"rfp_{rfp_id}"); os.makedirs(rfp_dir, exist_ok=True)
    extracted_dir = os.path.join(rfp_dir, "extracted"); os.makedirs(extracted_dir, exist_ok=True)

    c1, c2 = st.columns([3,1])
    with c1:
        st.subheader(title)
        if sam_url: st.markdown(f"[Open in SAM]({sam_url})")
    with c2:
        st.caption(f"RFP ID: {rfp_id}")

    st.divider()

    up1, up2 = st.columns([2,1])
    with up1:
        files = st.file_uploader("➕ Add files to this RFP", type=["pdf","docx","txt","zip","xlsx","xls","csv","md","rtf"], accept_multiple_files=True, key=f"an_u_{rfp_id}")
        if files:
            with conn:
                for f in files:
                    data = f.read()
                    path = os.path.join(rfp_dir, f.name)
                    os.makedirs(os.path.dirname(path), exist_ok=True)
                    with open(path, "wb") as fh: fh.write(data)
                    try:
                        conn.execute("INSERT INTO rfp_files(rfp_id, filename, path, kind, bytes) VALUES (?, ?, ?, ?, ?);", (rfp_id, f.name, path, f.type, sqlite3.Binary(data)))
                    except Exception:
                        pass
            st.success(f"Saved {len(files)} file(s).")
    with up2:
        st.caption("SAM.gov")
        api = st.text_input("API Key", type="password", key=f"an_api_{rfp_id}")
        cookie = st.text_input("Cookie (optional)", type="password", key=f"an_cookie_{rfp_id}")
        if st.button("Fetch from SAM.gov ▶", key=f"an_fetch_{rfp_id}"):
            try:
                att_dir = os.path.join(rfp_dir, "attachments"); os.makedirs(att_dir, exist_ok=True)
                pulled, errs = _fetch_sam_attachments(notice, api or st.session_state.get("sam_api_key"), att_dir, cookie or st.session_state.get("sam_cookie"))
                if pulled:
                    with conn:
                        for p in pulled:
                            fn = os.path.basename(p)
                            cur = conn.execute("SELECT 1 FROM rfp_files WHERE rfp_id=? AND filename=?;", (rfp_id, fn))
                            if not cur.fetchone():
                                conn.execute("INSERT INTO rfp_files(rfp_id, filename, path, kind, bytes) VALUES (?, ?, ?, ?, ?);", (rfp_id, fn, p, "downloaded", sqlite3.Binary(open(p, "rb").read())))
                if pulled: st.success(f"Pulled {len(pulled)} attachment(s).")
                else: st.info("No attachments found from API/HTML.")
                if errs:
                    with st.expander("Fetch log"):
                        for e in errs: st.write("•", e)
            except Exception as e:
                st.warning(f"Fetch unavailable: {e}")

    st.divider()

    if st.button("One-Click Analyze ▶", type="primary", key=f"an_analyze_{rfp_id}"):
        try:
            cur = conn.execute("SELECT id, filename, path FROM rfp_files WHERE rfp_id = ? ORDER BY id;", (rfp_id,))
            rows = cur.fetchall()
        except Exception:
            rows = []
        extracted_count = 0; combined = []
        for fid, fname, fpath in rows:
            try:
                with open(fpath, "rb") as fh:
                    b = fh.read()
                text = _extract_text_guess(fname, b) if " _extract_text_guess" in globals() else "[extraction helper missing]"
                out_txt = os.path.join(extracted_dir, f"{os.path.splitext(os.path.basename(fname))[0]}.txt")
                with open(out_txt, "w", encoding="utf-8", errors="ignore") as out:
                    out.write(text)
                try: conn.execute("UPDATE rfp_files SET extracted_path = ? WHERE id = ?;", (out_txt, fid))
                except Exception: pass
                extracted_count += 1; combined.append(text[:100000])
            except Exception as e:
                st.warning(f"Extract failed for {fname}: {e}")
        st.success(f"Extracted {extracted_count} file(s).")
        whole = "\\n\\n".join(combined)
        # Due date
        try:
            raw_due, dt_due = _detect_due_date(whole, notice)
        except Exception:
            raw_due, dt_due = None, None
        if raw_due:
            st.info(f"📅 Proposal Due (detected): {raw_due}")
            try:
                ics = _make_ics(title, dt_due)
                if ics: st.download_button("Download Due Date (.ics)", data=ics, file_name=f"due_{rfp_id}.ics", mime="text/calendar", key=f"an_ics_{rfp_id}")
            except Exception: pass
        else:
            st.caption("No due date detected.")

        # Quick brief
        bullets = []
        try:
            for kw in ["statement of work","sow","requirements","evaluation","period of performance","pop","deliverables"]:
                if re.search(rf"\\b{kw}\\b", whole, re.IGNORECASE):
                    bullets.append(f"Contains section: **{kw.title()}**")
        except Exception:
            pass
        if not bullets: bullets.append("No obvious SOW/evaluation markers detected.")
        st.markdown("### Quick Brief")
        st.markdown("\\n".join([f"- {b}" for b in bullets]))

    # Files list
    st.markdown("### Files")
    try:
        cur = conn.execute("SELECT id, filename, path, extracted_path, bytes FROM rfp_files WHERE rfp_id = ? ORDER BY id;", (rfp_id,))
        rows = cur.fetchall()
    except Exception:
        rows = []
    if not rows:
        st.caption("No files yet. Use the uploader above.")
    else:
        for fid, fname, fpath, xpath, nbytes in rows:
            with st.expander(f"{fname}  •  {nbytes or 0} bytes", expanded=False):
                st.code(fpath)
                if xpath and os.path.exists(xpath):
                    try:
                        with open(xpath, "r", encoding="utf-8", errors="ignore") as r:
                            txt = r.read(2000)
                        st.text_area("Extract preview", value=txt, height=160, key=f"an_xp_{fid}")
                    except Exception as e:
                        st.caption(f"[preview unavailable: {e}]")
                else:
                    st.caption("No extracted text yet. Click One-Click Analyze.")

# === Phase 3: RFP ingest schema & helpers ===
def _ensure_phase3_schema(conn):
    """Schema for RFP records & files (idempotent, tolerant of older versions)."""
    try:
        with conn:
            # Legacy table kept for backwards compatibility; main RFP table is `rfps`.
            conn.execute("""
                CREATE TABLE IF NOT EXISTS rfp_records (
                    id INTEGER PRIMARY KEY,
                    notice_id TEXT,
                    title TEXT,
                    sam_url TEXT,
                    created_at TEXT DEFAULT (datetime('now'))
                );
            """)
            # Base definition for rfp_files; if the table already exists this is a no-op.
            conn.execute("""
                CREATE TABLE IF NOT EXISTS rfp_files (
                    id INTEGER PRIMARY KEY,
                    rfp_id INTEGER,
                    filename TEXT,
                    mime TEXT,
                    sha256 TEXT,
                    pages INTEGER,
                    bytes BLOB,
                    status TEXT,
                    last_error TEXT,
                    src_url TEXT,
                    path TEXT,
                    kind TEXT,
                    extracted_path TEXT,
                    created_at TEXT DEFAULT (datetime('now')),
                    FOREIGN KEY(rfp_id) REFERENCES rfps(id)
                );
            """)
        # If rfp_files already existed with an older schema, make sure all expected columns exist.
        try:
            cur = conn.execute("PRAGMA table_info(rfp_files);")
            cols = {row[1] for row in cur.fetchall()}
            def _add(col_def: str):
                name = col_def.split()[0]
                if name not in cols:
                    conn.execute(f"ALTER TABLE rfp_files ADD COLUMN {col_def};")
            with conn:
                # Legacy columns used by some older utilities
                _add("path TEXT")
                _add("kind TEXT")
                _add("extracted_path TEXT")
                # Modern columns used by save_rfp_file_db and analyzers
                _add("mime TEXT")
                _add("sha256 TEXT")
                _add("pages INTEGER")
                _add("bytes BLOB")
                _add("status TEXT")
                _add("last_error TEXT")
                _add("src_url TEXT")
        except Exception:
            # Best-effort; do not block the app if PRAGMA/ALTER fails.
            pass
    except Exception as e:
        try:
            st.warning(f"Phase 3 schema init failed: {e}")
        except Exception:
            pass

def _ensure_phase3_dirs():
    base = "./rfp_store"
    try:
        os.makedirs(base, exist_ok=True)
    except Exception:
        base = "/tmp/rfp_store"
        os.makedirs(base, exist_ok=True)
    return base

def _get_or_create_rfp(conn, notice: dict):
    """Return rfp_id for the selected notice (create if needed)."""
    _ensure_phase3_schema(conn)
    nid = (notice or {}).get("Notice ID") or (notice or {}).get("Solicitation") or ""
    title = (notice or {}).get("Title") or ""
    sam_url = (notice or {}).get("SAM Link") or (notice or {}).get("URL") or ""
    with conn:
        cur = conn.execute("SELECT id FROM rfp_records WHERE notice_id = ? AND title = ?;", (nid, title))
        row = cur.fetchone()
        if row:
            return row[0]
        cur = conn.execute("INSERT INTO rfp_records(notice_id, title, sam_url) VALUES(?,?,?);", (nid, title, sam_url))
        return cur.lastrowid

# --- Ask RFP Analyzer Modal (expander-based, version-safe) ---
def _ask_rfp_analyzer_modal(notice: dict):
    """Open the Ask RFP Analyzer 'modal' using an expander (works across Streamlit versions)."""
    _title = (notice or {}).get("Title") or (notice or {}).get("Solicitation") or "Selected Notice"
    _qid = (notice or {}).get("Notice ID") or (notice or {}).get("Solicitation") or ""
    _qkey = f"ask_rfp_q_{_qid}"

    # Flag so we can auto-render on rerun if needed
    st.session_state["ask_rfp_open"] = True
    st.session_state["ask_rfp_notice"] = notice or {}

    # Anchor + expander + auto-scroll
    st.markdown('<a id="ask-rfp-anchor"></a>', unsafe_allow_html=True)
    with st.expander(f"Ask RFP Analyzer — {_title}", expanded=True):
        try:
            components.sig_html('<script>var el=document.getElementById("ask-rfp-anchor"); if(el){el.scrollIntoView({behavior:"smooth", block:"start"});}</script>', height=0)
        except Exception:
            pass
        st.write("**Title:**", _title)
        q = st.text_area(
            "Your question",
            key=_qkey,
            placeholder="e.g., Summarize key requirements and due dates…"
        )
        c1, c2 = st.columns(2)
        with c1:
            if st.button("Send", key=f"ask_rfp_send_{_qid}"):
                st.session_state["rfp_selected_notice"] = notice or {}
                st.session_state["rfp_question"] = q or ""
                st.success("Sent to Analyzer context.")
        with c2:
            if st.button("Close", key=f"ask_rfp_close_{_qid}"):
                st.session_state["ask_rfp_open"] = False

# --- Phase 2 schema: saved_searches + alerts (with migration) ---
def _ensure_phase2_schema(conn):
    """Create Phase‑2 tables and migrate missing columns safely (idempotent)."""
    import sqlite3 as _sl
    try:
        with conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS saved_searches (
                    id INTEGER PRIMARY KEY,
                    name TEXT,
                    nl_query TEXT,
                    cadence TEXT DEFAULT 'daily',
                    created_at TEXT DEFAULT (datetime('now'))
                );
            """)
            conn.execute("""
                CREATE TABLE IF NOT EXISTS alerts (
                    id INTEGER PRIMARY KEY,
                    saved_search_id INTEGER UNIQUE,
                    enabled INTEGER DEFAULT 1,
                    last_run_at TEXT,
                    last_result_count INTEGER,
                    last_error TEXT,
                    FOREIGN KEY(saved_search_id) REFERENCES saved_searches(id)
                );
            """)
        # Migration: add missing columns (safe to call repeatedly)
        try:
            cols = [r[1] for r in conn.execute("PRAGMA table_info(saved_searches);").fetchall()]
            if 'nl_query' not in cols:
                with conn: conn.execute("ALTER TABLE saved_searches ADD COLUMN nl_query TEXT;")
            if 'cadence' not in cols:
                with conn: conn.execute("ALTER TABLE saved_searches ADD COLUMN cadence TEXT DEFAULT 'daily';")
            if 'created_at' not in cols:
                with conn: conn.execute("ALTER TABLE saved_searches ADD COLUMN created_at TEXT DEFAULT (datetime('now'));")
        except Exception:
            pass
    except Exception as e:
        try:
            st.warning(f"Phase 2 schema init failed: {e}")
        except Exception:
            pass

# --- Session flag helpers ---
def _get_flag(name: str, default: bool = False) -> bool:
    try:
        return bool(st.session_state.get(name, default))
    except Exception:
        return default
    try:
        with conn:
            conn.execute("CREATE UNIQUE INDEX IF NOT EXISTS idx_alerts_saved_unique ON alerts(saved_search_id);")
    except Exception:
        pass

def _set_flag(name: str, value: bool) -> None:
    try:
        st.session_state[name] = bool(value)
    except Exception:
        pass

# === Phase 2.5 helpers (canonical) ===
def notify(msg: str, level: str = "info"):
    """Consistent toast/notice across Streamlit versions."""
    try:
        st.toast(msg)
    except Exception:
        fn = getattr(st, level, st.info)
        fn(msg)

def _ensure_db_meta(conn):
    """Create a tiny db_meta table (idempotent)."""
    try:
        with conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS db_meta(
                    key TEXT PRIMARY KEY,
                    value TEXT
                );
            """)
    except Exception as e:
        try: st.warning(f"db_meta init failed: {e}")
        except Exception: pass

def _backup_db_sql(conn, dest_dir=None):
    """Write a .sql dump of the current DB (best-effort with fallbacks)."""
    try:
        ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        # Choose a writable destination
        dirs = []
        if dest_dir: dirs.append(dest_dir)
        dirs += ["./backups", os.path.join(os.path.expanduser("~"), ".samwatch", "backups"), "/tmp"]
        out_path = None
        for d in dirs:
            try:
                os.makedirs(d, exist_ok=True)
                out_path = os.path.join(d, f"samwatch_backup_{ts}.sql")
                with open(out_path, "w", encoding="utf-8") as f:
                    for line in conn.iterdump():
                        f.write(f"{line}\n")
                break
            except Exception:
                out_path = None
                continue
        if out_path:
            try: st.info(f"Backup saved: {out_path}")
            except Exception: pass
            return out_path
        else:
            try: st.info("Backup skipped: no writable location found.")
            except Exception: pass
            return None
    except Exception as e:
        try: st.info(f"Backup skipped: {e}")
        except Exception: pass
        return None

def phase2_5_bootstrap(conn):
    """Run once per session: ensure meta table, schema, and create a small backup."""
    if st.session_state.get("_p25_bootstrapped"):
        return
    try:
        _ensure_db_meta(conn)
    except Exception:
        pass
    try:
        _ensure_phase2_schema(conn)
    except Exception as e:
        try: st.warning(f"Schema check failed (Phase 2.5): {e}")
        except Exception: pass
    _backup_db_sql(conn)
    st.session_state["_p25_bootstrapped"] = True

def _inject_phase25_css():
    st.markdown(
        """
        <style>
        .sticky-actions { position: sticky; top: 0; z-index: 999; 
            background: var(--background-color,#ffffff); 
            padding: .5rem .75rem; border-bottom: 1px solid rgba(49,51,63,.2);
            border-radius: .5rem; margin-bottom: .5rem;
        }
        .health-ribbon { background: rgba(49,51,63,.05); padding: .5rem .75rem;
            border-radius: .5rem; display: inline-block; margin: .25rem 0 .75rem 0;
            font-size: 0.9rem;
        }
        .health-ribbon b { margin-right: .25rem; }
        </style>
        """,
        unsafe_allow_html=True,
    )

import streamlit.components.v1 as components

# === Phase 2.5 helpers ===

def run_alerts_center(conn):
    import pandas as pd
    _inject_phase25_css()
    phase2_5_bootstrap(conn)

    st.subheader("Alerts Center")

    try:
        _ensure_phase2_schema(conn)
    except Exception as e:
        st.warning(f"Schema init failed in Alerts Center: {e}")

    try:
        df = pd.read_sql_query(
            """
            SELECT s.id, s.name, s.nl_query, s.cadence,
                   COALESCE(a.enabled, 0) AS enabled,
                   a.last_run_at, a.last_result_count, a.last_error
            FROM saved_searches s
            LEFT JOIN alerts a ON a.saved_search_id = s.id
            ORDER BY s.id DESC;
            """,
            conn,
        )
    except Exception as e:
        st.error(f"Failed to load saved searches: {e}")
        df = pd.DataFrame()

    # Health ribbon always visible
    count_total = int(len(df)) if df is not None else 0
    count_enabled = int(df["enabled"].sum()) if (df is not None and "enabled" in df.columns) else 0
    st.markdown(
        f"<div class='health-ribbon'><b>Saved searches:</b> {count_total} &nbsp; • &nbsp; <b>Enabled alerts:</b> {count_enabled}</div>",
        unsafe_allow_html=True,
    )

    if df is None or df.empty:
        st.info("No saved searches yet. Save one in Smart Search, then return here.")
        return

    st.dataframe(df, use_container_width=True, hide_index=True)

    sel = st.multiselect("Select saved search IDs", options=df["id"].tolist(), key="alerts_sel_ids")

    # Sticky action bar
    st.markdown("<div class='sticky-actions'>", unsafe_allow_html=True)
    c1, c2, c3 = st.columns(3)
    with c1:
        
        if st.button("Run now"):
            ok = 0; err = 0
            for sid in sel:
                try:
                    sid_int = int(sid)
                    with conn:
                        # First try update; if nothing updated, insert
                        cur = conn.execute(
                            "UPDATE alerts SET last_run_at = datetime('now'), last_result_count = COALESCE(last_result_count, 0) WHERE saved_search_id = ?;",
                            (sid_int,),
                        )
                        if cur.rowcount == 0:
                            conn.execute(
                                "INSERT INTO alerts(saved_search_id, enabled, last_run_at, last_result_count) VALUES (?, 1, datetime('now'), 0);",
                                (sid_int,),
                            )
                    ok += 1
                except Exception:
                    err += 1
            notify(f"Ran {ok} search(es), errors={err}", "success")
        
    
    with c2:
        if st.button("Enable"):
            ok = 0; err = 0
            for sid in sel:
                try:
                    sid_int = int(sid)
                    with conn:
                        cur = conn.execute("UPDATE alerts SET enabled=1 WHERE saved_search_id = ?;", (sid_int,))
                        if cur.rowcount == 0:
                            conn.execute("INSERT INTO alerts(saved_search_id, enabled) VALUES (?, 1);", (sid_int,))
                    ok += 1
                except Exception:
                    err += 1
            notify(f"Enabled {ok} item(s), errors={err}", "success")
        
    
    with c3:
        if st.button("Disable"):
            ok = 0; err = 0
            for sid in sel:
                try:
                    sid_int = int(sid)
                    with conn:
                        cur = conn.execute("UPDATE alerts SET enabled=0 WHERE saved_search_id = ?;", (sid_int,))
                        if cur.rowcount == 0:
                            conn.execute("INSERT INTO alerts(saved_search_id, enabled) VALUES (?, 0);", (sid_int,))
                    ok += 1
                except Exception:
                    err += 1
            notify(f"Disabled {ok} item(s), errors={err}", "success")
        
    st.markdown("</div>", unsafe_allow_html=True)

    st.markdown("### Edit selected")
    _name_default = ""
    _cad_default = "daily"
    if sel and len(sel) == 1:
        try:
            _row = df[df["id"] == sel[0]].iloc[0]
            _name_default = str(_row.get("name") or "")
            _cad_default = str((_row.get("cadence") or "daily")).lower()
        except Exception:
            pass
    _name_in = st.text_input("New name", value=_name_default, key="alerts_edit_name")
    _cadence_opts = ["daily", "weekly", "monthly"]
    try:
        _cad_idx = _cadence_opts.index(_cad_default) if _cad_default in _cadence_opts else 0
    except Exception:
        _cad_idx = 0
    _cad_in = st.selectbox("New cadence", _cadence_opts, index=_cad_idx, key="alerts_edit_cadence")

    if st.button("Update selected"):
        if not sel:
            st.warning("Select at least one saved search to update.")
        else:
            try:
                with conn:
                    for sid in sel:
                        if _name_in.strip() and _cad_in:
                            conn.execute("UPDATE saved_searches SET name=?, cadence=? WHERE id=?;", (_name_in.strip(), _cad_in, int(sid)))
                        elif _name_in.strip():
                            conn.execute("UPDATE saved_searches SET name=? WHERE id=?;", (_name_in.strip(), int(sid)))
                        elif _cad_in:
                            conn.execute("UPDATE saved_searches SET cadence=? WHERE id=?;", (_cad_in, int(sid)))
                notify("Updated.", "success")
                try:
                    st.rerun()
                except Exception:
                    pass
            except Exception as _e:
                st.error(f"Update failed: {_e}")

def _set_flag(name: str, value: bool) -> None:
    try:
        st.session_state[name] = bool(value)
    except Exception:
        pass

# === Guard: ensure parse_nl_query exists ===
try:
    parse_nl_query  # type: ignore
except NameError:
    def parse_nl_query(q: str) -> dict:
        import re as _re
        if not q:
            return {}
        s = q.lower()
        out = {"keywords": [], "naics": [], "state": None, "set_aside": None, "due_lte_days": None}
        for m in _re.findall(r"\b(\d{5,6})\b", s):
            out["naics"].append(m)
        for key, aliases in {
            "8a": ["8a", "8(a)"],
            "sdvosb": ["sdvosb", "service disabled", "service-disabled"],
            "wosb": ["wosb", "women owned"],
            "hubzone": ["hubzone"],
            "sdb": ["sdb", "small disadvantaged"],
            "veteran": ["veteran", "vosb"]
        }.items():
            if any(a in s for a in aliases):
                out["set_aside"] = key.upper()
                break
        m = _re.search(r"\bin\s+([a-z]{2})\b", s)
        if m:
            out["state"] = m.group(1).upper()
        m = _re.search(r"due\s*(?:<|<=)\s*(\d+)\s*days", s)
        if m:
            out["due_lte_days"] = int(m.group(1))
        tokens = _re.findall(r"[a-zA-Z][a-zA-Z0-9\-/]+", s)
        banned = set(["in","due","days","sdvosb","wosb","hubzone","8a","8","veteran","sdb"] + out["naics"])
        out["keywords"] = [t for t in tokens if t not in banned]
        return out
# === Phase 2: Saved Searches & Alerts schema (SAM-only) ===

# --- Phase 2 schema: saved_searches + alerts (with migration) ---

def _ensure_phase1_schema(conn: "sqlite3.Connection") -> None:
    try:
        cols = {r[1] for r in conn.execute("PRAGMA table_info(rfp_files);").fetchall()}
    except Exception:
        cols = set()
    def _add(col, ddl):
        if col not in cols:
            conn.execute(f"ALTER TABLE rfp_files ADD COLUMN {ddl};")
    with conn:
        _add("status", "status TEXT")         # queued | downloaded | failed | hash_mismatch
        _add("last_error", "last_error TEXT") # last error text
        _add("src_url", "src_url TEXT")       # original download URL when known

def _p1_set_status(conn, file_id: int, status: str, err: str | None = None):
    try:
        with conn:
            conn.execute("UPDATE rfp_files SET status=?, last_error=? WHERE id=?;", (status, err or "", int(file_id)))
    except Exception:
        pass

def _download_with_retry(url: str, retries: int = 3, timeout: int = 30) -> tuple[bytes | None, str | None]:
    last_err = None
    for i in range(retries):
        try:
            r = requests.get(url, timeout=timeout)
            if r.status_code == 200 and r.content:
                return r.content, None
            last_err = f"http {r.status_code} empty={len(r.content)==0}"
        except Exception as e:
            last_err = str(e)
        time.sleep(2 * (i + 1))
    return None, last_err


def _extract_notice_id_from_obj(notice: object) -> str:
    """Best-effort extraction of a SAM notice identifier from a dict or string."""
    try:
        if isinstance(notice, dict):
            for key in (
                "Notice ID", "NoticeID", "notice_id", "id", "NoticeId",
                "Sol Number", "Solicitation", "NoticeIdentifier"
            ):
                v = notice.get(key)
                if v:
                    s = str(v).strip()
                    if s:
                        return s
        elif isinstance(notice, str):
            s = notice.strip()
            if s:
                return s
    except Exception:
        pass
    return ""

def _sam_upsert_attachment_record(conn, notice_id: str, file_name: str, url: str | None = None,
                                  mime_type: str | None = None, size_bytes: int | None = None,
                                  status: str | None = None, last_error: str | None = None) -> int:
    """
    Ensure there is a sam_attachments row for (notice_id, file_name, url) and
    update basic metadata / status. Returns the attachment id (or 0 on failure).
    """
    if not notice_id or not file_name:
        return 0
    try:
        with conn:
            cur = conn.execute(
                """
                SELECT id FROM sam_attachments
                 WHERE notice_id = ? AND file_name = ? AND IFNULL(url,'') = IFNULL(?, '');
                """,
                (notice_id, file_name, url or "")
            )
            row = cur.fetchone()
            if row:
                att_id = int(row[0])
                conn.execute(
                    """
                    UPDATE sam_attachments
                       SET mime_type   = COALESCE(?, mime_type),
                           size_bytes  = COALESCE(?, size_bytes),
                           status      = COALESCE(?, status),
                           last_error  = COALESCE(?, last_error),
                           updated_at  = datetime('now')
                     WHERE id = ?;
                    """,
                    (mime_type, size_bytes, status, last_error, att_id),
                )
            else:
                conn.execute(
                    """
                    INSERT INTO sam_attachments
                        (notice_id, file_name, url, mime_type, size_bytes, status, last_error, created_at, updated_at)
                    VALUES (?, ?, ?, ?, ?, ?, ?, datetime('now'), datetime('now'));
                    """,
                    (notice_id, file_name, url, mime_type, size_bytes, status or "pending", last_error or ""),
                )
                att_id = int(conn.execute("SELECT last_insert_rowid();").fetchone()[0])
        return att_id
    except Exception:
        return 0

def _rfp_documents_link_record(
    conn,
    rfp_id: int,
    sam_attachment_id: int,
    notice_id: str | None = None,
    file_name: str | None = None,
    url: str | None = None,
    status: str | None = None,
    last_error: str | None = None,
    stored_path: str | None = None,
) -> int:
    """
    Ensure there is an rfp_documents row tying an RFP record to a SAM attachment.
    Returns the rfp_documents.id (or 0 on failure).
    """
    if not rfp_id or not sam_attachment_id:
        return 0
    try:
        with conn:
            cur = conn.execute(
                "SELECT id FROM rfp_documents WHERE rfp_id = ? AND sam_attachment_id = ?;",
                (int(rfp_id), int(sam_attachment_id)),
            )
            row = cur.fetchone()
            if row:
                doc_id = int(row[0])
                conn.execute(
                    """
                    UPDATE rfp_documents
                       SET notice_id   = COALESCE(?, notice_id),
                           file_name   = COALESCE(?, file_name),
                           url         = COALESCE(?, url),
                           status      = COALESCE(?, status),
                           last_error  = COALESCE(?, last_error),
                           stored_path = COALESCE(?, stored_path),
                           updated_at  = datetime('now')
                     WHERE id = ?;
                    """,
                    (notice_id, file_name, url, status, last_error, stored_path, doc_id),
                )
            else:
                conn.execute(
                    """
                    INSERT INTO rfp_documents
                        (rfp_id, sam_attachment_id, notice_id, file_name, url, status, last_error, stored_path, created_at, updated_at)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, datetime('now'), datetime('now'));
                    """,
                    (int(rfp_id), int(sam_attachment_id), notice_id, file_name, url, status or "pending", last_error or "", stored_path),
                )
                doc_id = int(conn.execute("SELECT last_insert_rowid();").fetchone()[0])
        return doc_id
    except Exception:
        return 0

def rfp_documents_fetch_from_sam(conn, rfp_id: int, notice: object) -> int:
    """
    High-level helper for the new AI attachment pipeline:

      - Enumerates attachments for the given notice using sam_list_attachments (when available)
      - Upserts rows into sam_attachments (metadata + pending status)
      - Upserts rows into rfp_documents to tie attachments to an RFP record
      - Delegates actual download + status updates to _phase1_fetch_sam_attachments
      - Reconciles rfp_documents / sam_attachments status and stored_path from rfp_files

    Returns:
        Count of attachments linked to this RFP (best-effort).
    """
    notice_id = _extract_notice_id_from_obj(notice)
    if not notice_id:
        return 0

    # Step 1: enumerate metadata (best-effort)
    items = []
    if "sam_list_attachments" in globals():
        try:
            items = list(sam_list_attachments(notice) or [])
        except Exception:
            items = []

    linked = []
    for it in items:
        try:
            name = (it.get("name") or it.get("filename") or it.get("File Name") or "attachment")
            name = str(name).strip() or "attachment"
            url = (it.get("url") or it.get("URL") or it.get("link") or "") or ""
            url = str(url).strip()
            mime_type = it.get("mime") or it.get("MimeType") or it.get("content_type")
            size_val = it.get("size") or it.get("size_bytes")
            try:
                size_int = int(size_val) if size_val is not None else None
            except Exception:
                size_int = None

            att_id = _sam_upsert_attachment_record(
                conn,
                notice_id=notice_id,
                file_name=name,
                url=url or None,
                mime_type=str(mime_type or "") or None,
                size_bytes=size_int,
                status="pending",
                last_error=None,
            )
            if att_id:
                _rfp_documents_link_record(
                    conn,
                    rfp_id=int(rfp_id),
                    sam_attachment_id=att_id,
                    notice_id=notice_id,
                    file_name=name,
                    url=url or None,
                    status="pending",
                )
                linked.append((att_id, name, url))
        except Exception:
            continue

    # Step 2: run the existing Phase 1 pipeline to actually download into rfp_files
    try:
        _phase1_fetch_sam_attachments(conn, int(rfp_id), notice)
    except Exception:
        # Even if download fails, we still keep metadata rows
        pass

    # Step 3: reconcile status + stored_path from rfp_files back into sam_attachments / rfp_documents
    try:
        base_dir = _ensure_phase3_dirs()
        rfp_dir = os.path.join(base_dir, f"rfp_{int(rfp_id)}")
    except Exception:
        rfp_dir = "./rfp_store"

    for att_id, name, url in linked:
        try:
            cur = conn.execute(
                """
                SELECT id, path, status, last_error
                  FROM rfp_files
                 WHERE rfp_id = ?
                   AND (filename = ? OR (src_url IS NOT NULL AND src_url = ?))
                 ORDER BY id DESC
                 LIMIT 1;
                """,
                (int(rfp_id), name, url or None),
            )
            row = cur.fetchone()
            if not row:
                continue
            file_id, path, status, last_error = row
            stored_path = path or os.path.join(rfp_dir, "attachments", name)

            with conn:
                conn.execute(
                    """
                    UPDATE sam_attachments
                       SET status       = ?,
                           last_error   = COALESCE(?, last_error),
                           downloaded_at = CASE WHEN ? = 'downloaded' THEN datetime('now') ELSE downloaded_at END,
                           updated_at   = datetime('now')
                     WHERE id = ?;
                    """,
                    (status or "downloaded", last_error or "", status or "", int(att_id)),
                )
                conn.execute(
                    """
                    UPDATE rfp_documents
                       SET status      = ?,
                           last_error  = COALESCE(?, last_error),
                           stored_path = COALESCE(?, stored_path),
                           updated_at  = datetime('now')
                     WHERE rfp_id = ? AND sam_attachment_id = ?;
                    """,
                    (status or "downloaded", last_error or "", stored_path, int(rfp_id), int(att_id)),
                )
        except Exception:
            continue

    return len(linked)

def _phase1_fetch_sam_attachments(conn: "sqlite3.Connection", rfp_id: int, notice: str | dict) -> int:
    """
    Preferred path:
      - sam_list_attachments(notice) -> iterable of dicts {'name':..., 'url':...}
      - download each with retry, store to rfp_files with sha256 + bytes, set status
    Fallback path:
      - sam_try_fetch_attachments(notice) -> iterable of (name, blob)
    """
    _ensure_phase1_schema(conn)
    added = 0

    items = []
    if 'sam_list_attachments' in globals():
        try:
            items = list(sam_list_attachments(notice) or [])
        except Exception:
            items = []

    if items:
        for it in items:
            name = (it.get("name") or "").strip() or "attachment"
            url  = (it.get("url")  or "").strip()
            if not url:
                continue
            try:
                with conn:
                    conn.execute("""
                        INSERT INTO rfp_files (rfp_id, filename, mime, sha256, bytes, pages, created_at, status, last_error, src_url)
                        VALUES (?, ?, 'application/octet-stream', '', ?, NULL, datetime('now'), 'queued', '', ?);
                    """, (int(rfp_id), name, sqlite3.Binary(b""), url))
                fid = conn.execute(
                    "SELECT id FROM rfp_files WHERE rfp_id=? AND filename=? ORDER BY id DESC LIMIT 1;",
                    (int(rfp_id), name)
                ).fetchone()[0]
                blob, err = _download_with_retry(url)
                if blob is None:
                    _p1_set_status(conn, fid, "failed", err)
                    continue
                sha = hashlib.sha256(blob).hexdigest()
                dup = conn.execute(
                    "SELECT id FROM rfp_files WHERE rfp_id=? AND sha256=? AND id<>? LIMIT 1;",
                    (int(rfp_id), sha, int(fid))
                ).fetchone()
                if dup:
                    _p1_set_status(conn, fid, "hash_mismatch", "duplicate content")
                    continue
                with conn:
                    conn.execute("""
                        UPDATE rfp_files 
                           SET bytes=?, sha256=?, status='downloaded', last_error=''
                         WHERE id=?;
                    """, (sqlite3.Binary(blob), sha, int(fid)))
                added += 1
            except Exception as e:
                try:
                    _p1_set_status(conn, fid, "failed", str(e))
                except Exception:
                    pass
        return added

    if 'sam_try_fetch_attachments' in globals():
        try:
            for name, data in sam_try_fetch_attachments(notice):
                sha = hashlib.sha256(data or b"").hexdigest()
                with conn:
                    conn.execute("""
                        INSERT OR IGNORE INTO rfp_files 
                          (rfp_id, filename, mime, sha256, bytes, pages, created_at, status, last_error, src_url)
                        VALUES (?, ?, 'application/octet-stream', ?, ?, NULL, datetime('now'), 'downloaded', '', NULL);
                    """, (int(rfp_id), str(name or ""), sha, sqlite3.Binary(data or b"")))
                added += 1
        except Exception:
            pass
    return added

def _phase1_retry_file(conn: "sqlite3.Connection", file_id: int) -> bool:
    _ensure_phase1_schema(conn)
    row = None
    try:
        row = conn.execute("SELECT src_url FROM rfp_files WHERE id=?;", (int(file_id),)).fetchone()
    except Exception:
        row = None
    if not row or not (row[0] or "").strip():
        _p1_set_status(conn, file_id, "failed", "no src_url")
        return False
    blob, err = _download_with_retry(row[0])
    if blob is None:
        _p1_set_status(conn, file_id, "failed", err)
        return False
    sha = hashlib.sha256(blob).hexdigest()
    try:
        with conn:
            conn.execute(
                "UPDATE rfp_files SET bytes=?, sha256=?, status='downloaded', last_error='' WHERE id=?;",
                (sqlite3.Binary(blob), sha, int(file_id))
            )
        return True
    except Exception as e:
        _p1_set_status(conn, file_id, "failed", str(e))
        return False
import pandas as pd

# === Phase 1 Helper: insert-or-skip rfp_file by sha256 ===

def _insert_or_skip_rfp_file(conn, rfp_id: int, filename: str, blob: bytes | None,
                             mime: str | None = None, src_url: str | None = None, status: str = "downloaded"):
    _ensure_phase1_schema(conn)
    sha = hashlib.sha256((blob or b"")).hexdigest()
    try:
        with conn:
            conn.execute(
                """
                INSERT OR IGNORE INTO rfp_files 
                    (rfp_id, filename, mime, sha256, bytes, pages, created_at, status, last_error, src_url)
                VALUES (?, ?, ?, ?, ?, NULL, datetime('now'), ?, '', ?);
                """,
                (int(rfp_id), str(filename or ""), str(mime or "application/octet-stream"),
                 sha, sqlite3.Binary(blob or b""), status, src_url or "")
            )
        return True
    except Exception as e:
        try:
            st.error(f"Attach insert failed for {filename}: {e}")
        except Exception:
            pass
        return False

def _one_click_analyze(conn, rfp_id: int, sam_url: str | None = None):
    import pandas as pd
    try:
        notice = None
        _sam_url = (sam_url or "")
        if not _sam_url:
            try:
                _sam_url = pd.read_sql_query("SELECT sam_url FROM rfps WHERE id=?;", conn, params=(int(rfp_id),)).iloc[0].get("sam_url") or ""
            except Exception:
                _sam_url = ""
        try:
            notice = _parse_sam_notice_id(_sam_url) if _sam_url else None
        except Exception:
            notice = None
        # 1) Fetch SAM attachments if possible
        if notice and 'sam_try_fetch_attachments' in globals():
            try:
                for name, data in sam_try_fetch_attachments(notice):
                    _insert_or_skip_rfp_file(conn, int(rfp_id), name, data, None)
            except Exception as e:
                try: st.warning(f"SAM fetch skipped: {e}")
                except Exception: pass
        # 2) Rebuild search index
        try:
            y1_index_rfp(conn, int(rfp_id), rebuild=False)
        except Exception as e:
            try: st.warning(f"Index build issue: {e}")
            except Exception: pass
        # 3) Extract L/M, CLINs, Dates, POCs, Meta (lightweight)
        try:
            # collect full text from rfp_files (reuse existing util if present)
            txt = ""
            try:
                df = pd.read_sql_query("SELECT COALESCE(text,'') AS t FROM rfp_files_t WHERE rfp_id=?;", conn, params=(int(rfp_id),))
                if not df.empty:
                    txt = "\n\n".join([str(x or "") for x in df["t"].tolist()])
            except Exception:
                txt = ""
            try:
                secs = extract_sections_L_M(txt)
            except Exception:
                secs = {}
            try:
                lm = (derive_lm_items(secs.get('L','')) + derive_lm_items(secs.get('M',''))) if secs else []
            except Exception:
                lm = []
            # Persist LM items idempotently
            try:
                with conn:
                    for it in lm[:300]:
                        is_must = 1 if re.search(r"\b(shall|must|required|no later than|shall not|will)\b", it, re.I) else 0
                        conn.execute(
                            "INSERT OR IGNORE INTO lm_items (rfp_id, item_text, is_must, status) VALUES (?,?,?,?);",
                            (int(rfp_id), it.strip(), is_must, "Open")
                        )
            except Exception:
                pass
            # Meta extraction (fallback heuristics)
            try:
                naics = _extract_naics(txt)
            except Exception: naics = ""
            try:
                set_aside = _extract_set_aside(txt)
            except Exception: set_aside = ""
            try:
                place = _extract_place(txt)
            except Exception: place = ""
            # Persist meta keys (idempotent inserts allowed)
            try:
                with conn:
                    if naics: conn.execute("INSERT INTO rfp_meta(rfp_id, key, value) VALUES(?,?,?);", (int(rfp_id), "naics", str(naics)))
                    if set_aside: conn.execute("INSERT INTO rfp_meta(rfp_id, key, value) VALUES(?,?,?);", (int(rfp_id), "set_aside", str(set_aside)))
                    if place: conn.execute("INSERT INTO rfp_meta(rfp_id, key, value) VALUES(?,?,?);", (int(rfp_id), "place_of_performance", str(place)))
            except Exception:
                pass
            # Prefill subcontractor filters into session_state
            try:
                if place:
                    m = re.search(r"\b([A-Z]{2})\b", place.upper())
                    if m:
                        st.session_state['sub_default_state'] = m.group(1)
                if naics:
                    st.session_state['sub_default_naics'] = str(naics)
            except Exception:
                pass
        except Exception:
            pass
        return True
    except Exception as e:
        try:
            st.error(f"One-Click Analyze failed: {e}")
        except Exception:
            pass
        # Phase 1: auto-fetch SAM attachments if we have a notice context
    try:
        _ctx = st.session_state.get('rfp_selected_notice') or {}
        _rid = int(st.session_state.get('current_rfp_id') or 0)
        _nid = _ctx.get('Notice ID') or _ctx.get('noticeId') or _ctx.get('id') or ''
        if _rid and _nid:
            _ = _phase1_fetch_sam_attachments(conn, _rid, _nid)
    except Exception:
        pass
    return False

# === Compliance Matrix safety guards ===
if "x6_requirements_df" not in globals():
    def x6_requirements_df(conn, rfp_id):
        import pandas as pd
        try:
            return pd.read_sql_query(
                "SELECT id, must_flag, file, page, text FROM compliance_requirements "
                "WHERE rfp_id=? ORDER BY must_flag DESC, id ASC;",
                conn, params=(int(rfp_id),)
            )
        except Exception:
            return pd.DataFrame(columns=["id","must_flag","file","page","text"])

if "_ensure_x6_schema" not in globals():
    def _ensure_x6_schema(conn):
        from contextlib import closing as _closing
        with _closing(conn.cursor()) as cur:
            cur.execute("""
            CREATE TABLE IF NOT EXISTS compliance_requirements(
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                rfp_id INTEGER NOT NULL,
                file TEXT, page INTEGER, text TEXT,
                must_flag INTEGER DEFAULT 0,
                hash TEXT,
                created_at TEXT DEFAULT CURRENT_TIMESTAMP
            );
            """)
            cur.execute("""
            CREATE TABLE IF NOT EXISTS compliance_links(
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                rfp_id INTEGER NOT NULL,
                requirement_id INTEGER NOT NULL,
                section TEXT,
                created_at TEXT DEFAULT CURRENT_TIMESTAMP
            );
            """ )
        conn.commit()
# === end guards ===

## ELA Phase3 hybrid_api
# Optional FastAPI backend (run separately) + client with graceful fallback.
# Set GOVCON_API_BASE or st.secrets['api']['base_url'] to use the API.
import os, threading, uuid, time
try:
    import requests
except Exception:
    requests = None

# ---- job store for background tasks (in-memory) ----
_jobs: dict[str, dict] = {}

def _enqueue(fn, *args, **kwargs) -> str:
    jid = str(uuid.uuid4())
    _jobs[jid] = {"status":"queued"}
    def _run():
        _jobs[jid] = {"status":"running"}
        try:
            res = fn(*args, **kwargs)
            _jobs[jid] = {"status":"done", "result": res}
        except Exception as e:
            _jobs[jid] = {"status":"error", "error": str(e)}
    t = threading.Thread(target=_run, daemon=True)
    t.start()
    return jid

def _job_status(jid: str) -> dict:
    return _jobs.get(jid, {"status":"unknown"})

# ---- API service functions (fallbacks) ----
def _svc_analyze(question: str, context: dict | None = None):
    # Use your in-process analyzer if available
    try:
        return _service_analyze_rfp_question(question, context or {})
    except Exception:
        # Minimal fallback
        title = ""
        if isinstance(context, dict):
            title = context.get("title") or context.get("Title") or ""
        return f"[demo] Analysis for {title or 'the selected notice'}: {question}"

# ---- FastAPI app (optional) ----
try:
    from fastapi import FastAPI, BackgroundTasks
    from pydantic import BaseModel
    FASTAPI_OK = True
except Exception:
    FASTAPI_OK = False

if FASTAPI_OK:
    api = FastAPI(title="ELA GovCon API", version="0.1")

    class AnalyzeReq(BaseModel):
        question: str
        context: dict | None = None

    @api.post("/api/analyze")
    def api_analyze(req: AnalyzeReq, background_tasks: BackgroundTasks):
        jid = _enqueue(_svc_analyze, req.question, req.context or {})
        return {"job_id": jid}

    @api.get("/api/job/{job_id}")
    def api_job(job_id: str):
        return _job_status(job_id)

# ---- Client helpers ----
def _api_base_url():
    try:
        base = os.environ.get("GOVCON_API_BASE")
        if not base and hasattr(st, "secrets"):
            base = (getattr(st, "secrets", {}) or {}).get("api", {}).get("base_url")
        return base
    except Exception:
        return None

def _api_post_v2(path: str, payload: dict):
    base = _api_base_url()
    if not base or requests is None:
        return None
    try:
        token = None
        try:
            token = (getattr(st, 'secrets', {}) or {}).get('api', {}).get('token')
            if not token:
                import os as _os
                token = _os.getenv('GOVCON_API_TOKEN')
        except Exception:
            token = None
        url = base.rstrip('/') + path
        headers = {'X-API-Key': token} if token else None
        r = requests.post(url, json=payload, headers=headers, timeout=20)
        if r.status_code == 200:
            return r.json()
    except Exception:
        return None
    return None

def _api_get_v2(path: str):
    base = _api_base_url()
    if not base or requests is None:
        return None
    try:
        token = None
        try:
            token = (getattr(st, 'secrets', {}) or {}).get('api', {}).get('token')
            if not token:
                import os as _os
                token = _os.getenv('GOVCON_API_TOKEN')
        except Exception:
            token = None
        url = base.rstrip('/') + path
        headers = {'X-API-Key': token} if token else None
        r = requests.get(url, headers=headers, timeout=20)
        if r.status_code == 200:
            return r.json()
    except Exception:
        return None
    return None

def _api_post(path: str, payload: dict):
    base = _api_base_url()
    if not base or requests is None:
        return None
    try:
        url = base.rstrip("/") + path
        r = requests.post(url, json=payload, timeout=20)
        if r.status_code == 200:
            return r.json()
    except Exception:
        return None
    return None

def _api_get(path: str):
    base = _api_base_url()
    if not base or requests is None:
        return None
    try:
        url = base.rstrip("/") + path
        r = requests.get(url, timeout=20)
        if r.status_code == 200:
            return r.json()
    except Exception:
        return None
    return None

# ---- Wire into the RFP Analyze service used by the dialog ----
def _phase3_analyze(question: str, opportunity: dict | None = None):
    # Try API path first
    resp = _api_post_v2("/api/analyze", {"question": question, "context": opportunity or {}})
    if isinstance(resp, dict) and resp.get("job_id"):
        st.session_state["phase3_last_job"] = resp["job_id"]
        return f"Queued analysis job: {resp['job_id']}"
    # Fallback to in-process
    try:
        return _service_analyze_rfp_question(question, opportunity or {})
    except Exception as e:
        return f"Analyze error: {e}"

def _phase3_poll_status():
    jid = st.session_state.get("phase3_last_job")
    if not jid:
        return None
    resp = _api_get_v2(f"/api/job/{jid}")
    if not resp:
        return None
    return resp

# ---- Enhance Ask RFP Analyzer dialog to use Phase 3 when available ----
def _phase3_enhance_rfp_dialog():
    # integrate into existing dialog if present
    if "show_rfp_analyzer" in st.session_state and st.session_state.get("show_rfp_analyzer"):
        # add a status row
        c1, c2 = st.columns([1,1])
        with c1:
            if st.button("Check job status", key=_unique_key("poll_job","o3")):
                st.session_state["phase3_last_status"] = _phase3_poll_status()
        with c2:
            if st.session_state.get("phase3_last_status"):
                st.caption(str(st.session_state["phase3_last_status"]))

# ELA Phase2 performance
import sqlite3, hashlib, time

# Cached DB connector with WAL + PRAGMAs
def _ensure_indices(conn):
    """Create useful indices on hot columns if their tables exist.

    This is idempotent and safe to call multiple times. It keeps the core
    tables fast for common filters in Deals, Notices, Contacts, Outreach,
    and RFP Analyzer views.
    """
    try:
        cur = conn.cursor()

        def table_exists(name: str) -> bool:
            try:
                cur.execute(
                    "SELECT name FROM sqlite_master WHERE type='table' AND name=?",
                    (name,),
                )
                return cur.fetchone() is not None
            except Exception:
                return False

        stmts: list[str] = []

        # Notices: search filters in SAM Watch / RFP Analyzer
        if table_exists("notices"):
            stmts.append(
                "CREATE INDEX IF NOT EXISTS idx_notices_notice_id ON notices(notice_id)"
            )
            stmts.append(
                "CREATE INDEX IF NOT EXISTS idx_notices_naics_agency ON notices(naics, agency)"
            )
            stmts.append(
                "CREATE INDEX IF NOT EXISTS idx_notices_setaside_respdate ON notices(set_aside, response_date)"
            )

        # Vendors / Subfinder
        if table_exists("vendors"):
            stmts.append(
                "CREATE INDEX IF NOT EXISTS idx_vendors_place_id ON vendors(place_id)"
            )

        # Deals / pipeline views
        if table_exists("deals"):
            stmts.append(
                "CREATE INDEX IF NOT EXISTS idx_deals_stage ON deals(stage)"
            )
            stmts.append(
                "CREATE INDEX IF NOT EXISTS idx_deals_owner_stage_deadline "
                "ON deals(owner_user, stage, rfp_deadline)"
            )
            stmts.append(
                "CREATE INDEX IF NOT EXISTS idx_deals_notice_id ON deals(notice_id)"
            )

        # Files attached to notices / rfps (lightweight metadata)
        if table_exists("files"):
            stmts.append(
                "CREATE INDEX IF NOT EXISTS idx_files_notice_id ON files(notice_id)"
            )

        # Contacts / CRM
        if table_exists("contacts"):
            stmts.append(
                "CREATE INDEX IF NOT EXISTS idx_contacts_email ON contacts(email)"
            )
            stmts.append(
                "CREATE INDEX IF NOT EXISTS idx_contacts_org_owner "
                "ON contacts(organization, owner_user)"
            )

        # Outreach log
        if table_exists("outreach_log"):
            stmts.append(
                "CREATE INDEX IF NOT EXISTS idx_outreach_log_contact_deal "
                "ON outreach_log(contact_id, deal_id)"
            )
            stmts.append(
                "CREATE INDEX IF NOT EXISTS idx_outreach_log_sent_at "
                "ON outreach_log(sent_at)"
            )

        # RFPs (lightweight header table)
        if table_exists("rfps"):
            stmts.append(
                "CREATE INDEX IF NOT EXISTS idx_rfps_notice_id ON rfps(notice_id)"
            )
            stmts.append(
                "CREATE INDEX IF NOT EXISTS idx_rfps_created_at ON rfps(created_at)"
            )

        # Messages / activity log
        if table_exists("messages"):
            stmts.append(
                "CREATE INDEX IF NOT EXISTS idx_messages_sent_at ON messages(sent_at)"
            )

        for s in stmts:
            try:
                cur.execute(s)
            except Exception:
                # best-effort only; don't block app on index creation
                pass

        try:
            cur.close()
        except Exception:
            pass
    except Exception:
        # index creation is non-critical
        pass


def _db_connect(db_path: str, **kwargs):
    """Central SQLite connector.

    Puts the database into WAL mode and applies performance-friendly PRAGMAs.
    Also ensures indices are created once per Streamlit session.
    """
    import sqlite3
    _sq = sqlite3  # alias for compatibility

    import streamlit as st

    # Build connect kwargs with safe defaults
    base_kwargs = {
        "check_same_thread": False,
        "detect_types": sqlite3.PARSE_DECLTYPES,
        "timeout": 15,
    }
    try:
        base_kwargs.update(kwargs or {})
    except Exception:
        pass

    conn = sqlite3.connect(db_path, **base_kwargs)
    try:
        # WAL + write performance
        conn.execute("PRAGMA journal_mode=WAL;")
        conn.execute("PRAGMA synchronous=NORMAL;")
        # Keep more work in memory and avoid disk churn
        conn.execute("PRAGMA temp_store=MEMORY;")
        conn.execute("PRAGMA mmap_size=300000000;")
        conn.execute("PRAGMA cache_size=-200000;")
        conn.execute("PRAGMA busy_timeout=5000;")
        # Enforce referential integrity at the connection level
        conn.execute("PRAGMA foreign_keys=ON;")
    except Exception:
        pass

    # One-time per-session index creation
    try:
        if not st.session_state.get("_phase2_indices_done"):
            _ensure_indices(conn)
            st.session_state["_phase2_indices_done"] = True
    except Exception:
        pass

    return conn

# Cached SELECT helper (returns rows + cols); pass db_path explicitly
@st.cache_data(ttl=600, show_spinner=False)
def _cached_select(db_path: str, sql: str, params: tuple = ()):
    conn = _db_connect(db_path)
    cur = conn.execute(sql, params)
    rows = cur.fetchall()
    cols = [d[0] for d in cur.description] if cur.description else []
    return rows, cols

# Cache AI answers by (question + context hash)
def _ai_cache_key(question: str, context_hash: str = ""):
    key = (question or "") + "|" + (context_hash or "")
    return hashlib.sha256(key.encode("utf-8")).hexdigest()

def _cached_ai_answer(question: str, context_hash: str = ""):
    @st.cache_data(ttl=86400, show_spinner=False)
    def _inner(k, q, ch):
        try:
            return _service_analyze_rfp_question(q, {"hash": ch})
        except Exception as e:
            return f"AI error: {e}"
    return _inner(_ai_cache_key(question, context_hash), question, context_hash)

# Expand Phase 0 write guard to clear caches after commits
def _write_guard(conn, fn, *args, **kwargs):
    import streamlit as st

    with conn:
        out = fn(*args, **kwargs)
    try:
        st.cache_data.clear()
    except Exception:
        pass
    return out

# ELA Phase1 bootstrap

# Safe dataframe wrapper and monkey patch to avoid height=None issues
def _styled_dataframe(df, use_container_width=True, height=None, hide_index=True, column_config=None):
    kwargs = {"use_container_width": use_container_width}
    if height is not None:
        try:
            kwargs["height"] = int(height) if height != "stretch" else "stretch"
        except Exception:
            if isinstance(height, str):
                kwargs["height"] = height
    try:
        return st.dataframe(df, hide_index=hide_index, column_config=column_config, **kwargs)
    except TypeError:
        return st.dataframe(df, **kwargs)

# Monkey patch st.dataframe to drop height=None safely
if not hasattr(st, "_orig_dataframe"):
    st._orig_dataframe = st.dataframe
    def _safe_dataframe(df, **kwargs):
        if "height" in kwargs and kwargs["height"] is None:
            kwargs.pop("height", None)
        return st._orig_dataframe(df, **kwargs)
    st.dataframe = _safe_dataframe

# Ensure theme exists
if "apply_theme" not in globals():
    def _apply_theme_old():
        if st.session_state.get("_phase1_theme_applied"):
            return
        st.session_state["_phase1_theme_applied"] = True
        st.markdown('''
        <style>
        .block-container {padding-top: 1.2rem; padding-bottom: 1.2rem; max-width: 1400px;}
        h1, h2, h3 {margin-bottom: .4rem;}
        div[data-testid="stDataFrame"] thead th {position: sticky; top: 0; background: #fff; z-index: 2;}
        div[data-testid="stDataFrame"] tbody tr:hover {background: rgba(64,120,242,0.06);}
        [data-testid="stExpander"] {border: 1px solid rgba(49,51,63,0.16); border-radius: 12px; margin-bottom: 10px;}
        [data-testid="stExpander"] summary {font-weight: 600;}
        .stTextInput>div>div>input, .stNumberInput input, .stTextArea textarea {border-radius: 10px !important;}
        button[kind="primary"] {box-shadow: 0 1px 4px rgba(0,0,0,.08);}
        .ela-banner {position: sticky; top: 0; z-index: 999; background: linear-gradient(90deg, #4068f2, #7a9cff); color: #fff; padding: 6px 12px; border-radius: 8px; margin-bottom: 10px;}
        </style>
        ''', unsafe_allow_html=True)
        st.markdown("<div class='ela-banner'>Phase 1 theme active · polished layout and tables</div>", unsafe_allow_html=True)

# ===== Phase 1 Theme (auto-injected) =====
def _apply_theme_old():
    import streamlit as st

    if st.session_state.get("_phase1_theme_applied"):
        return
    st.session_state["_phase1_theme_applied"] = True
    st.markdown('''
    <style>
    .block-container {padding-top: 1.2rem; padding-bottom: 1.2rem; max-width: 1400px;}
    h1, h2, h3 {margin-bottom: .4rem;}
    .ela-subtitle {color: rgba(49,51,63,0.65); font-size: .95rem; margin-bottom: 1rem;}
    /* Dataframe polish */
    div[data-testid="stDataFrame"] thead th {position: sticky; top: 0; background: #fff; z-index: 2;}
    div[data-testid="stDataFrame"] tbody tr:hover {background: rgba(64,120,242,0.06);}
    /* Cards & expanders */
    [data-testid="stExpander"] {border: 1px solid rgba(49,51,63,0.16); border-radius: 12px; margin-bottom: 10px;}
    [data-testid="stExpander"] summary {font-weight: 600;}
    .ela-card {border: 1px solid rgba(49,51,63,0.16); border-radius: 12px; padding: 12px; margin-bottom: 12px;}
    .ela-chip {display:inline-block; padding: 2px 8px; border-radius: 999px; font-size: 12px; margin-right:6px; background: rgba(49,51,63,.06);}
    .ela-ok {background: rgba(0,200,83,.12);} .ela-warn {background: rgba(251,140,0,.12);} .ela-bad {background: rgba(229,57,53,.12);}
    /* Inputs & buttons */
    .stTextInput>div>div>input, .stNumberInput input, .stTextArea textarea {border-radius: 10px !important;}
    button[kind="primary"] {box-shadow: 0 1px 4px rgba(0,0,0,.08);}
    /* Banner */
    .ela-banner {position: sticky; top: 0; z-index: 999; background: linear-gradient(90deg, #4068f2, #7a9cff); color: #fff; padding: 6px 12px; border-radius: 8px; margin-bottom: 10px;}
    </style>
    ''', unsafe_allow_html=True)
    st.markdown("<div class='ela-banner'>Phase 1 theme active · polished layout & tables</div>", unsafe_allow_html=True)

# ===== injected early helpers (do not remove) =====
def _safe_int(x, default=0):
    try:
        if x is None:
            return int(default)
        if isinstance(x, int):
            return x
        s = str(x).strip()
        if s == "" or s.lower() in ("none", "nan"):
            return int(default)
        # try float first (handles "123.0")
        try:
            return int(float(s))
        except Exception:
            pass
        # fallback: keep only digits
        digits = "".join(ch for ch in s if ch.isdigit())
        return int(digits) if digits else int(default)
    except Exception:
        return int(default)

def _uniq_key(base: str, rfp_id: int) -> str:
    try:
        k = f"__uniq_counter_{base}_{rfp_id}"
        n = int(st.session_state.get(k, 0))
        st.session_state[k] = n + 1
        return f"{base}_{rfp_id}_{n}"
    except Exception:
        import time
        return f"{base}_{rfp_id}_{int(time.time()*1000)%100000}"
# ===== end injected early helpers =====

def y3_get_rfp_files(_conn, rfp_id: int):
    """Return [(id, file_name, bytes)] for files saved in rfp_files for this RFP."""
    try:
        from contextlib import closing as _closing
        with _closing(_conn.cursor()) as cur:
            cur.execute("SELECT id, filename, bytes FROM rfp_files WHERE rfp_id=? ORDER BY id", (rfp_id,))
            return cur.fetchall() or []
    except Exception:
        return []

import requests

# ===== X3 MODAL HELPERS =====
def _x3_open_modal(row_dict: dict):
    st.session_state["x3_modal_notice"] = dict(row_dict or {})
    st.session_state["x3_show_modal"] = True
    try:
        st.rerun()
    except Exception:
        pass

def _ensure_x6_schema(conn: "sqlite3.Connection") -> None:
    from contextlib import closing as _closing
    with _closing(conn.cursor()) as cur:
        cur.execute("""
        CREATE TABLE IF NOT EXISTS compliance_requirements(
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            rfp_id INTEGER NOT NULL,
            file TEXT,
            page INTEGER,
            text TEXT,
            must_flag INTEGER DEFAULT 0,
            hash TEXT,
            created_at TEXT DEFAULT CURRENT_TIMESTAMP
        );
        """)
        cur.execute("""
        CREATE TABLE IF NOT EXISTS compliance_links(
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            rfp_id INTEGER NOT NULL,
            requirement_id INTEGER NOT NULL,
            section TEXT,
            created_at TEXT DEFAULT CURRENT_TIMESTAMP
        );
        """ )
    conn.commit()

def x6_sections_suggestions(rfp_id: int) -> list:
    # prefer user outline from session, else defaults
    outline = st.session_state.get(f"proposal_outline_{int(rfp_id)}", "")
    if outline:
        # split numbered lines
        items = [ln.strip() for ln in outline.splitlines() if ln.strip()]
        # filter headers like '# Proposal Outline'
        return [i for i in items if not i.startswith("#")]
    return [
        "Cover Letter",
        "Executive Summary",
        "Technical Approach",
        "Management Approach",
        "Past Performance",
        "Pricing",
        "Compliance Matrix",
    ]

def x6_extract_requirements(conn: "sqlite3.Connection", rfp_id: int, limit_per_file: int = 400) -> int:
    """Parse rfp_files for 'must' and 'shall' style requirements, upsert into compliance_requirements."""
    import hashlib
    from contextlib import closing as _closing
    added = 0
    with _closing(conn.cursor()) as cur:
        cur.execute("SELECT id, filename, bytes, mime FROM rfp_files WHERE rfp_id=? ORDER BY id ASC;", (int(rfp_id),))
        files = cur.fetchall() or []
    for fid, fname, bts, mime in files:
        try:
            pages = extract_text_pages(bts, mime or "")
        except Exception:
            pages = []
        found = 0
        for pi, page_txt in enumerate(pages or [], start=1):
            if not page_txt:
                continue
            # Simple sentence split
            parts = [s.strip() for s in page_txt.replace("\r", "\n").split("\n") if s.strip()]
            for s in parts:
                low = s.lower()
                musty = any(k in low for k in [" shall ", " must ", " required ", " is required ", " will "])
                if not musty and ("shall" not in low and "must" not in low):
                    continue
                h = hashlib.sha1(f"{rfp_id}|{fname}|{pi}|{s}".encode("utf-8")).hexdigest()
                try:
                    with _closing(conn.cursor()) as cur:
                        # check dup by hash
                        cur.execute("SELECT 1 FROM compliance_requirements WHERE rfp_id=? AND hash=? LIMIT 1;", (int(rfp_id), h))
                        if cur.fetchone():
                            pass
                        else:
                            cur.execute(
                                "INSERT INTO compliance_requirements(rfp_id,file,page,text,must_flag,hash) VALUES (?, ?, ?, ?, ?,?,?)",
                                (int(rfp_id), fname or "", int(pi), s[:2000], 1 if musty else 0, h)
                            )
                            added += 1
                    conn.commit()
                except Exception:
                    pass
                found += 1
                if found >= limit_per_file:
                    break
            if found >= limit_per_file:
                break
    return int(added)

def x5_render_transcript_viewer(conn: "sqlite3.Connection", rfp_id: int) -> None:
    """View chat history with color-coded roles, filters, and export."""
    from contextlib import closing as _closing
    import datetime as _dt

    # Map file_id -> filename for scopes
    files = {}
    try:
        with _closing(conn.cursor()) as cur:
            cur.execute("SELECT id, filename FROM rfp_files WHERE rfp_id=?", (int(rfp_id),))
            for rid, fn in cur.fetchall() or []:
                files[int(rid)] = fn or ""
    except Exception:
        pass

    # Load turns
    turns = []
    try:
        with _closing(conn.cursor()) as cur:
            cur.execute("""SELECT id, scope, role, content, created_at FROM rfp_chat_turns WHERE rfp_id=? ORDER BY id ASC;""", (int(rfp_id),))
            for tid, scope, role, content, ts in cur.fetchall() or []:
                label = "Global"
                if (scope or "").startswith("file:"):
                    try:
                        fid = int((scope or "").split(":",1)[1])
                        label = f"File: {files.get(fid, str(fid))}"
                    except Exception:
                        label = "File"
                turns.append({
                    "id": int(tid),
                    "scope": scope or "global",
                    "role": (role or "assistant").lower(),
                    "content": content or "",
                    "ts": ts or "",
                    "label": label,
                })
    except Exception:
        pass

    # Filters
    colA, colB = st.columns([2,3])
    with colA:
        scopes = ["All", "Global"] + [f"File: {fn}" for fn in files.values()]
        chosen = st.selectbox("Scope", options=scopes, key=_uniq_key("x5_tv_scope", int(rfp_id)))
    with colB:
        query = st.text_input("Search", value="", key=_uniq_key("x5_tv_search", int(rfp_id))).strip().lower()

    def _match(t):
        scope_ok = (chosen == "All") or (chosen == "Global" and t["scope"] == "global") or (chosen.startswith("File:") and t["label"] == chosen)
        if not scope_ok:
            return False
        if not query:
            return True
        blob = f"{t['role']} {t['label']} {t['content']} {t['ts']}".lower()
        return query in blob

    filtered = [t for t in turns if _match(t)]

    # Legend
    st.markdown(":blue[Human]  ·  :green[AI]")

    # Render chat using role-based chat balloons
    for t in filtered:
        role = "user" if t["role"] == "user" else "assistant"
        time_str = t["ts"]
        scope_str = t["label"]
        with st.chat_message(role):
            st.caption(f"{scope_str} · {time_str}")
            st.markdown(t["content"])

    # Export CSV
    import io, csv
    buf = io.StringIO()
    writer = csv.writer(buf)
    writer.writerow(["id","time","scope","role","content"])
    for t in filtered:
        writer.writerow([t["id"], t["ts"], t["label"], t["role"], t["content"]])
    st.download_button(
        "Export CSV",
        data=buf.getvalue().encode("utf-8"),
        file_name=f"rfp_{int(rfp_id)}_transcript.csv",
        mime="text/csv",
        key=_uniq_key("x5_tv_csv", int(rfp_id))
    )

    # Export Markdown
    md_lines = []
    for t in filtered:
        md_lines.append(f"### {t['role'].title()} — {t['label']}  \n*{t['ts']}*")
        md_lines.append("")
        md_lines.append(t["content"])
        md_lines.append("\n---")
    md_blob = "\n".join(md_lines).encode("utf-8")
    st.download_button(
        "Export Markdown",
        data=md_blob,
        file_name=f"rfp_{int(rfp_id)}_transcript.md",
        mime="text/markdown",
        key=_uniq_key("x5_tv_md", int(rfp_id))
    )

def x6_coverage(conn: "sqlite3.Connection", rfp_id: int) -> tuple[int, int]:
    from contextlib import closing as _closing
    total = 0
    covered = 0
    with _closing(conn.cursor()) as cur:
        cur.execute("SELECT COUNT(*) FROM compliance_requirements WHERE rfp_id=?", (int(rfp_id),))
        total = int(cur.fetchone()[0] or 0)
        cur.execute("SELECT COUNT(DISTINCT requirement_id) FROM compliance_links WHERE rfp_id=?", (int(rfp_id),))
        covered = int(cur.fetchone()[0] or 0)
    return covered, total

def _x3_render_modal(conn, notice: dict):
    try:
        rfp_id = _ensure_rfp_for_notice(conn, notice)
    except Exception as e:
        st.error(f"Could not open RFP Analyzer: {e}")
        return
    st.caption(f"RFP #{rfp_id} · {notice.get('Title','')}")

    # X.6 Compliance Matrix v1
    try:
        _ensure_x6_schema(conn)
        with st.expander("Compliance Matrix v1", expanded=False):
            cA, cB, cC = st.columns([1,1,2])
            with cA:
                if st.button("Extract requirements", key=_uniq_key("x6_extract", int(rfp_id))):
                    n = x6_extract_requirements(conn, int(rfp_id))
                    st.success(f"Extracted {n} new requirement(s).")
                    try: st.rerun()
                    except Exception: pass
            with cB:
                cov, tot = x6_coverage(conn, int(rfp_id))
                pct = 0 if tot == 0 else int(round(100 * cov / tot))
                st.metric("Coverage", f"{cov}/{tot}", f"{pct}%")
            with cC:
                st.caption("Link requirements to outline sections to increase coverage.")

            df = x6_requirements_df(conn, int(rfp_id))
            if df is None or df.empty:
                st.info("No requirements yet. Click Extract requirements.")
            else:
                # editable mapping column
                suggestions = x6_sections_suggestions(int(rfp_id))
                import pandas as pd
                df_view = df.copy()
                df_view["Map to section"] = ""
                edited = st.data_editor(
                    df_view,
                    column_config={
                        "must_flag": st.column_config.CheckboxColumn("Must", help="Detected must or shall"),
                        "Map to section": st.column_config.SelectboxColumn(options=suggestions),
                    },
                    hide_index=True,
                    use_container_width=True,
                    key=_uniq_key("x6_editor", int(rfp_id))
                )
                # Save mappings
                to_save = []
                for i, row in edited.iterrows():
                    sec = str(row.get('Map to section') or "").strip()
                    if sec:
                        to_save.append((int(row["id"]), sec))
                if st.button("Save mappings", key=_uniq_key("x6_save", int(rfp_id))) and to_save:
                    saved = x6_save_links(conn, int(rfp_id), to_save)
                    st.success(f"Saved {saved} link(s).")
                    try: st.rerun()
                    except Exception: pass

                # Export CSV
                import io
                import csv
                buf = io.StringIO()
                writer = csv.writer(buf)
                writer.writerow(["id","must","file","page","text","section"])
                for _, r in edited.iterrows():
                    writer.writerow([int(r["id"]), int(r["must_flag"]), r["file"], int(r["page"]) if r.get("page") else "", r["text"], r.get("Map to section") or ""])
                st.download_button("Export Compliance CSV", buf.getvalue().encode("utf-8"), file_name=f"rfp_{int(rfp_id)}_compliance.csv", mime="text/csv", key=_uniq_key("x6_csv", int(rfp_id)))
    except Exception as _x6e:
        st.info(f"Compliance Matrix unavailable: {_x6e}")
    # Phase 4: side panel (SAM facts + AI summary + quick actions)
    try:
        _url = notice.get('SAM Link') or notice.get('sam_url') or ''
    except Exception:
        _url = ''
    try:
        render_amendment_sidebar(conn, int(rfp_id), _url, ttl_hours=72)
    except Exception:
        pass
    try:
        with st.sidebar.expander('RFP Summary', expanded=True):
            st.markdown(_rfp_ai_summary(full_text or '', notice))
    except Exception:
        pass
    try:
        with st.sidebar.expander('Quick Actions', expanded=False):
            if st.button('Fetch attachments now', key=_uniq_key('x3_fetch_sidebar', int(rfp_id))):
                c = _fetch_and_save_now(conn, str(notice.get('Notice ID') or ''), int(rfp_id))
                st.success(f'Fetched {c} attachment(s).')
                try: st.rerun()
                except Exception: pass
            if st.button('Rebuild Search Index', key=_uniq_key('x3_reindex_sidebar', int(rfp_id))):
                try:
                    y1_index_rfp(conn, int(rfp_id), rebuild=True)
                    st.success('Index rebuilt')
                except Exception as _e:
                    st.info(f'Index rebuild failed: {_e}')
    except Exception:
        pass
    try:
        outline_key = f'proposal_outline_{int(rfp_id)}'
        with st.sidebar.expander('Proposal Outline', expanded=False):
            if st.button('Generate Outline', key=_uniq_key('x3_outline_sidebar', int(rfp_id))):
                outline = [
                    '# Proposal Outline',
                    '1. Cover Letter',
                    '2. Executive Summary',
                    '3. Technical Approach',
                    '4. Management Approach',
                    '5. Past Performance',
                    '6. Pricing (separate volume if required)',
                    '7. Compliance Matrix',
                ]
                st.session_state[outline_key] = '\n'.join(outline)
                st.success('Outline drafted and saved to session')
            _ol = st.session_state.get(outline_key, '')
            if _ol:
                st.text_area('Current outline', value=_ol, height=180, key=_uniq_key('x3_outline_preview', int(rfp_id)))
                try:
                    data = _ol.encode('utf-8')
                    st.download_button('Download Outline (.md)', data=data, file_name=f'proposal_outline_{int(rfp_id)}.md', mime='text/markdown', key=_uniq_key('x3_outline_dl', int(rfp_id)))
                except Exception:
                    pass
    except Exception:
        pass
    # Attachments area
    try:
        from contextlib import closing as _closing
        with _closing(conn.cursor()) as cur:
            cur.execute("SELECT COUNT(*) FROM rfp_files WHERE rfp_id=?;", (int(rfp_id),))
            n_files = int(cur.fetchone()[0])
    except Exception:
        n_files = 0
    cA, cB = st.columns([2,1])
    with cA:
        st.write(f"Attachments saved: **{n_files}**")
    with cB:
        if st.button("Fetch attachments now", key=_uniq_key("x3_fetch", int(rfp_id))):
            c = _fetch_and_save_now(conn, str(notice.get('Notice ID') or ""), int(rfp_id))
            st.success(f"Fetched {c} attachment(s).")
            try: st.rerun()
            except Exception: pass

    # Index + summary
    try:
        y1_index_rfp(conn, int(rfp_id), rebuild=False)
    except Exception:
        pass
    full_text, sources = _rfp_build_fulltext_from_db(conn, int(rfp_id))
    if not full_text:
        st.info("No documents yet. You can still ask questions; I'll use the SAM description if available.")
        try:
            descs = sam_try_fetch_attachments(str(notice.get('Notice ID') or "")) or []
            for name, b in descs:
                if name.endswith("_description.sig_html"):
                    try:
                        import bs4
                        soup = bs4.BeautifulSoup(b.decode('utf-8', errors='ignore'), 'html.parser')
                        full_text = soup.get_text(" ", strip=True)
                    except Exception:
                        pass
                    break
        except Exception:
            pass
    with st.expander("AI Summary", expanded=True):
                        _rfp_render_summary(_rfp_ai_summary(full_text or "", notice))

    # Per-document summarize chips
    try:
        from contextlib import closing as _closing
        with _closing(conn.cursor()) as cur:
            cur.execute("SELECT id, filename, mime FROM rfp_files WHERE rfp_id=? ORDER BY id;", (int(rfp_id),))
            files = cur.fetchall() or []
    except Exception:
        files = []
    if files:
        st.write("Documents:")
        for fid, fname, fmime in files[:12]:
            if st.button(f"Summarize: {fname}", key=_uniq_key("sumdoc", int(fid))):
                try:
                    import pandas as pd
                    blob = pd.read_sql_query("SELECT bytes, mime FROM rfp_files WHERE id=?;", conn, params=(int(fid),)).iloc[0]
                    _text = "\n\n".join(extract_text_pages(blob['bytes'], blob.get('mime') or (fmime or '')) or [])
                except Exception:
                    _text = ""
                st.session_state["x3_docsum"] = _rfp_ai_summary(_text, notice)
        if st.session_state.get("x3_docsum"):
            with st.expander("Document Summary", expanded=True):
                _rfp_highlight_css()
                st.markdown(_rfp_highlight_html(st.session_state.get("x3_docsum") or ""), unsafe_allow_html=True)

    # Chat
    st.divider()
    st.subheader("Ask about this RFP")
    _chat_k = _uniq_key("x3_chat", int(rfp_id))
    hist_key = f"x3_chat_hist_{rfp_id}"
    st.session_state.setdefault(hist_key, [])
    for who, msg in st.session_state[hist_key]:
        with st.chat_message(who):
            st.markdown(msg)
    q = st.chat_input("Ask a question about the requirements, due dates, sections, etc.", key=_chat_k)
    if q:
        st.session_state[hist_key].append(("user", q))
        with st.chat_message("assistant"):
            ans = _rfp_chat(conn, int(rfp_id), q)
            st.session_state[hist_key].append(("assistant", ans))
            st.markdown(ans)

    # Proposal hand-off
    st.divider()
    if st.button("Start Proposal Outline", key=_uniq_key("x3_outline", int(rfp_id))):
        outline = [
            "# Proposal Outline",
            "1. Cover Letter",
            "2. Executive Summary",
            "3. Technical Approach",
            "4. Management Approach",
            "5. Past Performance",
            "6. Pricing (separate volume if required)",
            "7. Compliance Matrix",
        ]
        st.session_state[f"proposal_outline_{rfp_id}"] = "\n".join(outline)
        st.success("Outline drafted and saved. Open Proposal Builder to continue.")

    with st.expander("Transcript Viewer", expanded=False):
        x5_render_transcript_viewer(conn, int(rfp_id))
def _uniq_key(base: str, rfp_id: int) -> str:
    """Return a unique (but stable per render) Streamlit key to avoid duplicates."""
    try:
        k = f"__uniq_counter_{base}_{rfp_id}"
        n = int(st.session_state.get(k, 0))
        st.session_state[k] = n + 1
        return f"{base}_{rfp_id}_{n}"
    except Exception:
        # Fallback if session_state isn't available
        import time
        return f"{base}_{rfp_id}_{int(time.time()*1000)%100000}"
import time
# ==== O4 unified DB + sender helpers ====
try:
    DB_PATH
except NameError:
    DB_PATH = "./data/app.db"

_O4_CONN = globals().get("_O4_CONN", None)

def ensure_dirs():
    from pathlib import Path as _Path
    _Path(DB_PATH).parent.mkdir(parents=True, exist_ok=True)

def get_db():
    import sqlite3
    from contextlib import closing as _closing
    ensure_dirs()
    conn = _db_connect(DB_PATH, check_same_thread=False)
    with _closing(conn.cursor()) as cur:
        cur.execute("PRAGMA foreign_keys = ON;")
    return conn

def get_o4_conn():
    import streamlit as st

    global _O4_CONN

    # reuse cached connection if present
    if globals().get("_O4_CONN"):
        try:
            st.session_state["conn"] = _O4_CONN
        except Exception:
            pass
        return _O4_CONN

    # otherwise use one from session or create a new one
    try:
        if "conn" in st.session_state and st.session_state.get("conn"):
            _O4_CONN = st.session_state["conn"]
            return _O4_CONN
    except Exception:
        pass

    conn = get_db()
    _O4_CONN = conn
    try:
        st.session_state["conn"] = conn
    except Exception:
        pass
    return conn
def _render_once(name: str):
    # returns True if allowed to render, False if already rendered
    key = f"__rendered__{name}"
    if st.session_state.get(key):
        return False
    st.session_state[key] = True
    return True

def _unique_key(base: str, namespace: str = "ui"):
    # Uses a stable counter in session state to avoid duplicate form/widget keys
    ss = st.session_state
    counter_key = f"__{namespace}_key_counter__"
    if counter_key not in ss:
        ss[counter_key] = 0
    ss[counter_key] += 1
    return f"{base}-{namespace}-{ss[counter_key]}"
    global _O4_CONN
    if _O4_CONN:
        try:
            st.session_state["conn"] = _O4_CONN
        except Exception:
            pass
        return _O4_CONN
    try:
        if "conn" in st.session_state and st.session_state.get("conn"):
            _O4_CONN = st.session_state["conn"]
            return _O4_CONN
    except Exception:
        pass
    conn = get_db()
    _O4_CONN = conn
    try:
        st.session_state["conn"] = conn
    except Exception:
        pass
    return conn

def _ensure_email_accounts_schema(conn):
    # Unified to O1 schema
    ensure_outreach_o1_schema(conn)

def _get_senders(conn):
    """Return a unified list of configured sender accounts.

    Each row is (email, display_name, app_password). We merge across
    the modern outreach_sender_accounts table and legacy tables.
    """
    from contextlib import closing as _closing

    tables = [
        # Modern multi-sender table used by Outreach → Sender accounts
        ("outreach_sender_accounts", "email", "label", "app_password"),
        # Legacy / fallback tables kept for compatibility
        ("email_accounts", "user_email", "display_name", "app_password"),
        ("o4_senders", "email", "name", "app_password"),
        ("senders", "email", "display_name", "app_password"),
        ("smtp_settings", "username", "label", "password"),
    ]

    senders = {}
    for tbl, c_email, c_name, c_pw in tables:
        try:
            with _closing(conn.cursor()) as c:
                c.execute("SELECT name FROM sqlite_master WHERE type='table' AND name=?", (tbl,))
                if not c.fetchone():
                    continue
                c.execute(f"SELECT {c_email}, {c_name}, {c_pw} FROM {tbl} ORDER BY {c_email}")
                rows = c.fetchall() or []
        except Exception:
            continue
        for r in rows:
            email = (r[0] or "").strip()
            if not email:
                continue
            name = (r[1] or "").strip() if len(r) > 1 else ""
            pw = r[2] if len(r) > 2 else ""
            # Prefer the first non-empty display name we see for a given email
            if email not in senders or (name and not senders[email][1]):
                senders[email] = (email, name, pw or "")
    return list(senders.values())
# ==== end O4 helpers ====
# Helper imports for RTM/Amendment
import re as _rtm_re
import json as _rtm_json
import hashlib as _rtm_hashlib
from contextlib import closing as _rtm_closing
try:
    import sqlite3 as _rtm_sqlite3
except Exception:
    import sqlite3 as _rtm_sqlite3
try:
    import pandas as _rtm_pd
except Exception:
    import pandas as _rtm_pd
try:
    import streamlit as _rtm_st
except Exception:
    class _Dummy: pass
    _rtm_st = _Dummy()
# === PHASE 5: L & M compliance gate ===
def require_LM_minimum(conn, rfp_id):
    f"""
    Returns (ok, missing:list[str]).
    Required: Offers Due date in rfp_meta, Section M present in rfp_sections, >=1 L/M checklist item.
    """
    missing = []
    try:
        df_due = pd.read_sql_query(
            "SELECT value FROM rfp_meta WHERE rfp_id=? AND key IN ('offers_due','due_offer') LIMIT 1;",
            conn, params=(int(rfp_id),)
        )
        if df_due is None or df_due.empty or not str(df_due.iloc[0]["value"]).strip():
            missing.append("Offers Due date (Section L)")
    except Exception:
        missing.append("Offers Due date (Section L)")
    try:
        df_m = pd.read_sql_query(
            "SELECT 1 FROM rfp_sections WHERE rfp_id=? AND (section='M' OR section LIKE 'Section M%') LIMIT 1;",
            conn, params=(int(rfp_id),)
        )
        if df_m is None or df_m.empty:
            missing.append("Section M present")
    except Exception:
        missing.append("Section M present")
    try:
        df_lm = pd.read_sql_query(
            "SELECT COUNT(1) AS c FROM lm_items WHERE rfp_id=?;",
            conn, params=(int(rfp_id),)
        )
        c = int(df_lm.iloc[0]["c"]) if df_lm is not None and not df_lm.empty else 0
        if c <= 0:
            missing.append("L/M checklist items")
    except Exception:
        missing.append("L/M checklist items")
    return (len(missing) == 0, missing)
# === end PHASE 5 ===

# --- Router helper to avoid 'Unknown page' blank rendering ---
def _safe_route_call(fn, *a, **kw):
    try:
        if callable(fn):
            return fn(*a, **kw)
    except Exception as _e:
        import streamlit as _st
        _st.error(f"Page failed: {type(_e).__name__}: {_e}")
    return None

# --- O3 helper: safe cursor context ---
from contextlib import contextmanager
@contextmanager
def _o3c(cursor):
    try:
        yield cursor
    finally:
        try:
            cursor.close()
        except Exception:
            pass
def _migrate_deals_columns(conn):
    """
    Add columns used by Deals and SAM Watch if missing. Idempotent.

# ---- Phase 0: Ask RFP Analyzer modal wiring ----

def _service_analyze_rfp_question(q, opportunity):
    try:
        base = _api_base_url()
    except Exception:
        base = None
    if base:
        return _phase3_analyze(q, opportunity)
# TODO: wire to actual analyzer service; for now return a placeholder
    if not q:
    return f"Please enter a question."
    title = (opp.get('title') if isinstance(opp, dict) else str(opp)) if opp else 'the selected notice'
    return f"[demo] Analysis for {title}: {q}"

def _render_ask_rfp_button(opportunity=None):
    if st.button("Ask RFP Analyzer", key=_unique_key('ask_rfp','o3')):
        st.session_state['show_rfp_analyzer'] = True
    if st.session_state.get('show_rfp_analyzer'):
        _ask_rfp_analyzer_modal(opportunity)

    """
    try:
        import pandas as _pd
        from contextlib import closing
        cur_cols = _pd.__p_read_sql_query("PRAGMA table_info(deals);", conn)
        have = set(cur_cols["name"].astype(str).tolist()) if cur_cols is not None else set()
    except Exception:
        have = set()

    def _add(col, ddl):
        if col not in have:
            try:
                from contextlib import closing
                with closing(conn.cursor()) as _c:
                    _c.execute(f"ALTER TABLE deals ADD COLUMN {ddl};")
                conn.commit()
            except Exception:
                pass

    _add("agency", "agency TEXT")
    _add("value", "value REAL")
    _add("sam_url", "sam_url TEXT")
    _add("notice_id", "notice_id TEXT")
    _add("solnum", "solnum TEXT")
    _add("posted_date", "posted_date TEXT")
    _add("rfp_deadline", "rfp_deadline TEXT")
    _add("naics", "naics TEXT")
    _add("psc", "psc TEXT")
    _add("score", "score REAL DEFAULT 0")
    _add("score_reason", "score_reason TEXT")
    _add("co_contact_id", "co_contact_id INTEGER")
    _add("engagement_score", "engagement_score REAL DEFAULT 0")
    _add("last_engagement_at", "last_engagement_at TEXT")
    _add("first_entered_stage_date", "first_entered_stage_date TEXT")
    _add("last_stage_change_date", "last_stage_change_date TEXT")
    _add("work_type_tags", "work_type_tags TEXT")

# Bridge names
sqlite3 = _rtm_sqlite3
pd = _rtm_pd
st = _rtm_st
re = _rtm_re
json = _rtm_json
hashlib = _rtm_hashlib
closing = _rtm_closing

# === RTM + Amendment helpers ===

def _now_iso():
    import pandas as pd
    try:
        return __import__("datetime").datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ")
    except Exception:
        return ""

def _ensure_rtm_schema(conn: "sqlite3.Connection") -> None:
    try:
        with closing(conn.cursor()) as cur:
            cur.execute("SELECT 1 FROM rtm_requirements LIMIT 1;")
    except Exception:
        with closing(conn.cursor()) as cur:
            cur.execute("CREATE TABLE IF NOT EXISTS rtm_requirements(id INTEGER PRIMARY KEY, rfp_id INTEGER, req_key TEXT, source_type TEXT, source_file TEXT, page INTEGER, text TEXT, status TEXT, created_at TEXT, updated_at TEXT);")
            cur.execute("CREATE TABLE IF NOT EXISTS rtm_links(id INTEGER PRIMARY KEY, rtm_id INTEGER, link_type TEXT, target TEXT, note TEXT, created_at TEXT, updated_at TEXT);")
            conn.commit()

def rtm_build_requirements(conn: "sqlite3.Connection", rfp_id: int, max_rows: int = 800) -> int:
    """
    Seed RTM from L/M checklist and SOW-style shall/must statements in rfp_chunks.
    Returns number of rows inserted (new).
    """
    _ensure_rtm_schema(conn)
    inserted = 0
    now = _now_iso()
    # 1) From L/M
    try:
        df_lm = pd.read_sql_query("SELECT id, item_text, is_must FROM lm_items WHERE rfp_id=?", conn, params=(int(rfp_id),))
    except Exception:
        df_lm = pd.DataFrame(columns=["id","item_text","is_must"])
    for i, row in df_lm.head(max_rows).iterrows():
        txt = (row["item_text"] or "").strip()
        if not txt:
            continue
        key = f"LM-{row['id']}"
        with closing(conn.cursor()) as cur:
            cur.execute("SELECT id FROM rtm_requirements WHERE rfp_id=? AND req_key=?;", (int(rfp_id), key))
            if cur.fetchone():
                continue
            cur.execute("""
                INSERT INTO rtm_requirements(rfp_id, req_key, source_type, source_file, page, text, status, created_at, updated_at)
                VALUES (?,?,?,?,?,?,?,?,?);
            """, (int(rfp_id), key, "L/M", None, None, txt, "Open", now, now))
            inserted += 1
    # 2) From SOW chunks, simple heuristic
    try:
        df_chunks = pd.read_sql_query("""
            SELECT id, file_name, page, text FROM rfp_chunks
            WHERE rfp_id=? ORDER BY file_name, page, id
        """, conn, params=(int(rfp_id),))
    except Exception:
        df_chunks = pd.DataFrame(columns=["file_name","page","text"])
    trig = re.compile(r"\\b(shall|must|will|provide|furnish)\\b", re.I)
    for _, row in df_chunks.iterrows():
        t = (row["text"] or "").strip()
        if not t:
            continue
        # split into sentences with light heuristic
        sentences = re.split(r"(?<=[\\.;:])\\s+", t)
        for s in sentences:
            if len(s) < 40:
                continue
            if trig.search(s):
                key = f"SOW-{hashlib.sha1(s.encode('utf-8')).hexdigest()[:10]}"
                with closing(conn.cursor()) as cur:
                    cur.execute("SELECT id FROM rtm_requirements WHERE rfp_id=? AND req_key=?;", (int(rfp_id), key))
                    if cur.fetchone():
                        continue
                    cur.execute("""
                        INSERT INTO rtm_requirements(rfp_id, req_key, source_type, source_file, page, text, status, created_at, updated_at)
                        VALUES (?,?,?,?,?,?,?,?,?);
                    """, (int(rfp_id), key, "SOW", row.get('file_name'), int(row.get('page') or 0), s.strip(), "Open", now, now))
                    inserted += 1
        if inserted >= max_rows:
            break
    conn.commit()
    return inserted

def rtm_metrics(conn: "sqlite3.Connection", rfp_id: int) -> dict:
    q = pd.read_sql_query("""
        SELECT r.id, r.source_type, r.status, COUNT(l.id) AS links
        FROM rtm_requirements r
        LEFT JOIN rtm_links l ON l.rtm_id = r.id
        WHERE r.rfp_id=?
        GROUP BY r.id
    """, conn, params=(int(rfp_id),))
    if q is None or q.empty:
        return {"total":0,"covered":0,"coverage":0.0,"by_type":{}}
    total = len(q)
    covered_rows = (q["links"] > 0) | (q["status"].fillna("")=="Covered")
    covered = int(covered_rows.sum())
    by_type = {}
    for t, sub in q.groupby("source_type"):
        ct = len(sub)
        cv = int(((sub["links"]>0) | (sub["status"].fillna("")=="Covered")).sum())
        by_type[t] = {"total": ct, "covered": cv, "coverage": (cv/ct if ct else 0.0)}
    return {"total": total, "covered": covered, "coverage": (covered/total if total else 0.0), "by_type": by_type}

def rtm_export_csv(conn: "sqlite3.Connection", rfp_id: int) -> str:
    q = pd.read_sql_query("""
        SELECT r.id, r.req_key, r.source_type, r.source_file, r.page, r.text, r.status,
               COALESCE(GROUP_CONCAT(l.link_type || ':' || l.target, '; '), '') AS evidence
        FROM rtm_requirements r
        LEFT JOIN rtm_links l ON l.rtm_id=r.id
        WHERE r.rfp_id=?
        GROUP BY r.id
        ORDER BY r.source_type, r.id
    """, conn, params=(int(rfp_id),))
    fn = f"/mnt/data/rtm_rfp_{rfp_id}.csv"
    try:
        q.to_csv(fn, index=False)
        return fn
    except Exception:
        return ""

def render_rtm_ui(conn: "sqlite3.Connection", rfp_id: int) -> None:
    # Unique key namespace to avoid duplicate element IDs
    _ns = int(st.session_state.get('rtm_ui_ns', 0))
    st.session_state['rtm_ui_ns'] = _ns + 1
    _k = f"{rfp_id}_{_ns}"

    st.subheader("RTM Coverage")
    cols = st.columns([1, 1, 1, 3])

    # Build/Update
    with cols[0]:
        if st.button("Build/Update RTM", key=f"rtm_build_{_k}", help="Pull from L/M and SOW 'shall' statements."):
            n = rtm_build_requirements(conn, int(rfp_id))
            st.success(f"Added {n} requirement(s).")

    # Export with L/M gate
    with cols[1]:
        ok_gate, missing_gate = require_LM_minimum(conn, int(rfp_id))
        if not ok_gate:
            st.button("Export CSV", key=f"rtm_export_blocked_{_k}", disabled=True, help="Blocked: " + ", ".join(missing_gate))
        else:
            path = rtm_export_csv(conn, int(rfp_id))
            if path:
                from pathlib import Path as _Path
                st.download_button("Export CSV",
                                   data=open(path, "rb").read(),
                                   file_name=_Path(path).name,
                                   mime="text/csv",
                                   key=f"rtm_export_{_k}")

    # Mark covered
    with cols[2]:
        if st.button("Mark all with evidence as Covered", key=f"rtm_mark_{_k}"):
            with closing(conn.cursor()) as cur:
                cur.execute("""
                    UPDATE rtm_requirements
                    SET status='Covered', updated_at=?
                    WHERE rfp_id=? AND id IN (
                        SELECT r.id
                        FROM rtm_requirements r
                        LEFT JOIN rtm_links l ON l.rtm_id=r.id
                        GROUP BY r.id
                        HAVING COUNT(l.id) > 0
                    );
                """, (_now_iso(), int(rfp_id)))
                conn.commit()

    # Metrics
    m = rtm_metrics(conn, int(rfp_id))
    st.caption(f"Coverage: {m['covered']}/{m['total']} = {m['coverage']:.0%}")

    # Editor
    df = pd.read_sql_query("""
        SELECT r.id as rtm_id, r.req_key, r.source_type, r.page, r.text, r.status,
               COALESCE(GROUP_CONCAT(l.link_type || ':' || l.target, '
'), '') AS evidence
        FROM rtm_requirements r
        LEFT JOIN rtm_links l ON l.rtm_id=r.id
        WHERE r.rfp_id=?
        GROUP BY r.id
        ORDER BY r.source_type, r.id
        LIMIT 1000
    """, conn, params=(int(rfp_id),))
    df["add_link_type"] = ""
    df["add_link_target"] = ""

    edited = st.data_editor(
        df,
        key=f"rtm_editor_{_k}",
        use_container_width=True,
        num_rows="dynamic",
        column_config={
            "rtm_id": st.column_config.NumberColumn(disabled=True),
            "text": st.column_config.TextColumn(width="large"),
            "evidence": st.column_config.TextColumn(disabled=True),
        },
    )

    # Persist status changes and new links
    now = _now_iso()
    for _, row in edited.iterrows():
        try:
            rid = int(row["rtm_id"])
        except Exception:
            continue
        # Status change
        with closing(conn.cursor()) as cur:
            cur.execute("UPDATE rtm_requirements SET status=?, updated_at=? WHERE id=?;",
                        (row.get('status') or "Open", now, rid))
        # New link
        lt = (row.get('add_link_type') or "").strip()
        tg = (row.get('add_link_target') or "").strip()
        if lt and tg:
            with closing(conn.cursor()) as cur:
                cur.execute(
                    """
                    INSERT INTO rtm_links (rtm_id, link_type, target, note, created_at, updated_at)
                    VALUES (?, ?, ?, ?, ?, ?);
                    """
                    , (rid, lt, tg, "", now, now),
                )
    conn.commit()

def _parse_sam_text_to_facts(txt: str) -> dict:
    d = {}
    # Dates
    m = re.search(r"(Offers|Quotes?) Due[^\\d]*(\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}|[A-Za-z]{3,9} \\d{1,2}, \\d{4})", txt, re.I)
    if m: d["offers_due"] = m.group(2)
    m = re.search(r"(Questions|Q&A) Due[^\\d]*(\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}|[A-Za-z]{3,9} \\d{1,2}, \\d{4})", txt, re.I)
    if m: d["questions_due"] = m.group(2)
    # Codes
    m = re.search(r"NAICS[^\\d]*(\\d{5,6})", txt, re.I)
    if m: d["naics"] = m.group(1)
    m = re.search(r"Set[- ]Aside[^:]*:\\s*([^\\n]+)", txt, re.I)
    if m: d["set_aside"] = m.group(1).strip()
    # Clauses and forms
    clauses = re.findall(r"(52\\.[\\d-]+\\S*)", txt)
    if clauses: d["clauses"] = sorted(set(clauses))[:50]
    forms = re.findall(r"\\b(SF|OF)-?\\s?(\\d{1,4}[A-Z]?)\\b", txt)
    if forms: d["forms"] = sorted(set([f"{a}-{b}" for a,b in forms]))[:50]
    return d

def sam_snapshot(conn: "sqlite3.Connection", rfp_id: int, url: str, ttl_hours: int = 72) -> dict:
    out = {"url": url, "facts": {}, "sha256": "", "cached": False, "text": ""}
    if not (url or "").strip():
        return out
    r = research_fetch(url, ttl_hours)
    txt = (r.get("text") or "").strip()
    out["cached"] = bool(r.get("cached"))
    out["text"] = txt
    if not txt:
        return out
    sha = hashlib.sha256(txt.encode("utf-8")).hexdigest()
    out["sha256"] = sha
    facts = _parse_sam_text_to_facts(txt)
    out["facts"] = facts
    now = _now_iso()
    with closing(conn.cursor()) as cur:
        cur.execute("INSERT INTO sam_versions(rfp_id, url, sha256, extracted_json, created_at) VALUES (?, ?, ?, ?, ?);",
                    (int(rfp_id), url, sha, json.dumps(facts), now))
        vid = cur.lastrowid
        for k, v in facts.items():
            val = json.dumps(v) if not isinstance(v, str) else v
            cur.execute("INSERT INTO sam_extracts(sam_version_id, key, value) VALUES(?,?,?);", (vid, k, val))
    conn.commit()
    return out

def _facts_diff(old: dict, new: dict) -> dict:
    diffs = {}
    keys = set(old.keys()) | set(new.keys())
    for k in keys:
        ov = old.get(k, "")
        nv = new.get(k, "")
        if json.dumps(ov, sort_keys=True) != json.dumps(nv, sort_keys=True):
            diffs[k] = {"old": ov, "new": nv}
    return diffs

def render_amendment_sidebar(conn: "sqlite3.Connection", rfp_id: int, url: str, ttl_hours: int = 72) -> None:
    if not (url or "").strip():
        return
    with st.sidebar.expander("Amendments · SAM Analyzer", expanded=True):
        st.caption("Tracks changes in Brief, Factors, Clauses, Dates, Forms.")
        if st.button("Fetch SAM snapshot", key=_uniq_key("sam_fetch", int(rfp_id))):
            snap = sam_snapshot(conn, int(rfp_id), url, ttl_hours)
            st.success(f"Snapshot stored. Cached={snap.get('cached')}")
        # Last two snapshots
        try:
            dfv = pd.read_sql_query("SELECT id, created_at, sha256, extracted_json FROM sam_versions WHERE rfp_id=? ORDER BY id DESC LIMIT 2;", conn, params=(int(rfp_id),))
        except Exception:
            dfv = None
        if dfv is not None and not dfv.empty:
            st.markdown("Latest snapshot facts:")
            latest = json.loads(dfv.iloc[0]["extracted_json"] or "{}")
            st.json(latest)
            if len(dfv) >= 2:
                prev = json.loads(dfv.iloc[1]["extracted_json"] or "{}")
                diffs = _facts_diff(prev, latest)
                if diffs:
                    st.markdown("**Changes since previous:**")
                    st.json(diffs)
                    # Impact to-dos
                    todos = []
                    if "offers_due" in diffs:
                        todos.append("Update due date everywhere. Recalculate schedule and reminders.")
                    if "clauses" in diffs:
                        todos.append("Re-run compliance matrix and clause flowdowns.")
                    if "forms" in diffs:
                        todos.append("Update RFQ pack forms and signatures.")
                    if "set_aside" in diffs or "naics" in diffs:
                        todos.append("Validate eligibility and certs.")
                    if todos:
                        st.markdown("**Impact to-dos:**")
                        for t in todos:
                            st.write(f"- {t}")
                else:
                    st.info("No changes between the last two snapshots.")
            else:
                st.info("Only one snapshot stored so far.")
        else:
            st.info("No snapshots yet.")

def feature_flag(name: str, default: bool=False) -> bool:
    """
    Read a feature flag from environment or Streamlit secrets.
    Precedence: os.environ["FEATURE_<NAME>"] then st.secrets["features"][name] then default.
    Does not raise if Streamlit is absent.
    """
    val = None
    try:
        import os as _os
        env_key = f"FEATURE_{name.upper()}"
        if env_key in _os.environ:
            val = _os.environ[env_key]
    except Exception:
        pass
    if val is None:
        try:
            import streamlit as _st  # type: ignore
            sec = _st.secrets.get("features", {})
            if isinstance(sec, dict) and name in sec:
                val = sec.get(name)
        except Exception:
            pass
    if isinstance(val, str):
        return val.lower() in {"1","true","yes","on"}
    if isinstance(val, bool):
        return val
    return bool(val) if val is not None else bool(default)

def _guess_solnum(text: str) -> str:
    if not text:
        return ""
    t = text
    m = re.search(r'(?i)Solicitation\s*(?:Number|No\.?|#)\s*[:#]?\s*([A-Z0-9][A-Z0-9\-\._/]{4,})', t)
    if m:
        return m.group(1)[:60]
    m = re.search(r'\b([A-Z0-9]{2,6}[A-Z0-9\-]{0,4}\d{2}[A-Z]?-?[A-Z]?-?\d{3,6})\b', t)
    if m:
        return m.group(1)[:60]
    m = re.search(r'\b(RFQ|RFP|IFB|RFI)[\s#:]*([A-Z0-9][A-Z0-9\-\._/]{3,})\b', t, re.I)
    if m:
        return (m.group(1).upper() + "-" + m.group(2))[:60]
    return ""

from contextlib import closing
from datetime import datetime, timedelta
from pathlib import Path
from typing import Optional, Any, Dict, List, Tuple
import io
import json
import os
import tempfile
import re
import math
import sqlite3

from email import encoders
from email.mime.base import MIMEBase
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
import pandas as pd
import docx  # for DOCX reading in _read_file
import re
try:
    import mathquests  # optional plugin
except Exception:
    mathquests = None
import smtplib
import mimetypes
import streamlit as st

# --- DB helpers: ensure column exists ---
def _ensure_column(conn, table, col, type_sql):
    cur = conn.cursor()
    cols = [r[1] for r in cur.execute(f"PRAGMA table_info({table})").fetchall()]
    if col not in cols:
        try:
            cur.execute(f"ALTER TABLE {table} ADD COLUMN {col} {type_sql}")
            conn.commit()
        except Exception:
            pass

def _update_rfp_meta(conn, rfp_id, title=None, solnum=None, sam_url=None):
    _ensure_column(conn, "rfps", "sam_url", "TEXT")
    sets, vals = [], []
    if title is not None:
        sets.append("title=?"); vals.append(title)
    if solnum is not None:
        sets.append("solnum=?"); vals.append(solnum)
    if sam_url is not None:
        sets.append("sam_url=?"); vals.append(sam_url)
    if sets:
        vals.append(int(rfp_id))
        conn.execute(f'UPDATE rfps SET {", ".join(sets)} WHERE id=?', vals)
        conn.commit()
        return True
    return False

def _parse_sam_notice_id(s):
    # Basic patterns for SAM URLs: /opp/<uuid>/view  or legacy ?id=12345
    import re
    if not s:
        return None
    m = re.search(r'/opp/([0-9a-fA-F\-]{8,36})/view', s)
    if m:
        return m.group(1)
    m = re.search(r'[?&](id|noticeId|oppId)=(\w+)', s, re.I)
    if m:
        return m.group(2)
    return None

# --- Context helpers ---
_CTX_DEFAULTS = {
    "rfp": None,
    "clins": [],
    "sections": [],
    "imp": {},
    "price_sheet": [],
    "staffing": [],
    "notice_id": None,
    "proposal_id": None,
    "meta": {},
}
def _ctxd(ctx, key):
    try:
        return ctx.get(key, _CTX_DEFAULTS.get(key))
    except Exception:
        return _CTX_DEFAULTS.get(key)

# --- Safe DataFrame helpers ---
def _is_df(obj):
    try:
        import pandas as pd
        return isinstance(obj, pd.DataFrame)
    except Exception:
        return False

def _df_nonempty(df):
    return _is_df(df) and not df.empty

def _first_row_value(df, col, default=None):
    try:
        if _df_nonempty(df) and col in df.columns:
            return df.iloc[0][col]
    except Exception:
        pass
    return default

# --- Capability Statement Page (full implementation) ---

# === Y6 helper ===
def _y6_resolve_openai_client():
    try:
        if "get_ai" in globals():
            return get_ai()
        if "get_openai_client" in globals():
            return get_openai_client()
        if "get_ai_client" in globals():
            return get_ai_client()
    except Exception:
        pass
    from openai import OpenAI  # type: ignore
    import os as _os
    key = (
        st.secrets.get("openai_api_key")
        or st.secrets.get("OPENAI_API_KEY")
        or _os.environ.get("OPENAI_API_KEY")
    )
    if not key:
        raise RuntimeError("OPENAI_API_KEY not configured")
    return OpenAI(api_key=key)

def _y6_resolve_model() -> str:
    return st.secrets.get("openai_model") or st.secrets.get("OPENAI_MODEL") or "gpt-4o-mini"

def _y6_chat(messages):
    client = _y6_resolve_openai_client()
    model = _y6_resolve_model()
    resp = client.chat.completions.create(model=model, messages=messages, temperature=0.2)
    try:
        return resp.choices[0].message.content.strip()
    except Exception:
        return "AI response unavailable."

def _y6_fetch_y1_context(conn, rfp_id, question: str, k_auto_fn=None):
    if not (conn and rfp_id):
        return None
    y1 = globals().get("y1_search")
    if not callable(y1):
        return None
    try:
        k = 6
        if callable(k_auto_fn):
            try:
                k = int(max(3, min(12, k_auto_fn(question))))
            except Exception:
                pass
        hits = y1(conn, int(rfp_id), question or "", k=k) or []
        if not hits:
            return None
        blocks = []
        for i, h in enumerate(hits, start=1):
            cid = h.get("chunk_id", i)
            rid = h.get("rfp_id", rfp_id)
            text = h.get("chunk") or h.get("text") or ""
            tag = f"[RFP-{rid}:{cid}]"
            blocks.append(f"{tag} {text}")
        return "\n\n".join(blocks)
    except Exception:
        return None

def y6_render_co_box(conn, rfp_id=None, *, key_prefix: str, title: str, help_text: str="CO answers. Uses RFP context when available.") -> None:
    c1, c2 = st.columns([3, 1])
    with c1:
        st.subheader(title)
        q = st.text_area("Your question", key=f"{key_prefix}_q", height=120, help=help_text)
    with c2:
        st.caption("Y6 CO helper")
    if not st.button("Ask", key=f"{key_prefix}_ask") or not (q or "").strip():
        return
    with st.spinner("CO is analyzing"):
        ctx = _y6_fetch_y1_context(conn, rfp_id, q, globals().get("y_auto_k"))
        CO_SYS = (
            "You are a senior U.S. federal Contracting Officer (CO). "
            "Answer with short, precise sentences or numbered bullets. "
            "If RFP context is provided, cite it with [RFP-<id>:<chunk#>]. "
            "Avoid raw JSON."
        )
        sys_prompt = CO_SYS + (f"\n\nRFP context follows. Cite it when relevant:\n{ctx}" if ctx else "")
        messages = [
            {"role": "system", "content": sys_prompt},
            {"role": "user", "content": (q or '').strip()},
        ]
        ans = _y6_chat(messages)
    st.markdown(ans)
    saver = globals().get("y5_save_snippet") or globals().get("pb_add_snippet") or globals().get("save_text_to_drafts")
    if saver:
        with st.expander("Add answer to Drafts", expanded=False):
            sec = st.text_input("Section label", value="CO Notes", key=f"{key_prefix}_draft_sec")
            if st.button("Add to drafts", key=f"{key_prefix}_draft_add"):
                try:
                    if "y5_save_snippet" in globals():
                        y5_save_snippet(conn, int(rfp_id) if rfp_id else 0, sec, ans, source="Y6 CO Box")
                    elif "pb_add_snippet" in globals():
                        pb_add_snippet(conn, int(rfp_id) if rfp_id else 0, sec, ans, source="Y6 CO Box")
                    else:
                        saver(conn, rfp_id, ans)
                    st.success("Saved to drafts")
                except Exception:
                    st.info("Drafts saver not available in this build.")
# === end Y6 helper ===

# === Global extractors to avoid NameError in early calls ===
def _extract_naics(text: str) -> str:
    import re as _re
    if not text:
        return ""
    m = _re.search(r'(?i)NAICS(?:\s*Code)?\s*[:#]?\s*([0-9]{5,6})', text)
    if m:
        return m.group(1)[:6]
    m = _re.search(r'(?i)NAICS[^\n]{0,50}?([0-9]{6})', text)
    if m:
        return m.group(1)
    m = _re.search(r'(?i)(?:industry|classification)[^\n]{0,50}?([0-9]{6})', text)
    return m.group(1) if m else ""

def _extract_set_aside(text: str) -> str:
    import re as _re
    if not text:
        return ""
    tags = ["SDVOSB","SDVOSBC","WOSB","EDWOSB","8(a)","8A","HUBZone","SBA","SDB","VOSB","Small Business","Total Small Business"]
    for t in tags:
        if _re.search(rf'(?i)\\b{_re.escape(t)}\\b', text):
            norm = t.upper().replace("(A)", "8A").replace("TOTAL SMALL BUSINESS","SMALL BUSINESS")
            if norm == "8(A)":
                norm = "8A"
            return norm
    m = _re.search(r'(?i)Set[- ]Aside\s*[:#]?\s*([A-Za-z0-9 \-/()]+)', text)
    if m:
        v = m.group(1).strip()
        v = _re.sub(r'\\s+', ' ', v)
        return v[:80]
    return ""
# === End global extractors ===

# === BEGIN: inlined One-Page Analyzer (rfp_onepage.py) ===

import re
import textwrap
from typing import List, Dict, Any

import streamlit as st

# ---- Minimal AI helpers (mirror app helpers; safe fallbacks) ----
def _resolve_openai_client():
    try:
        # Reuse app-level providers if available
        if "get_ai" in globals():
            return globals()["get_ai"]()
        if "get_openai_client" in globals():
            return globals()["get_openai_client"]()
        if "get_ai_client" in globals():
            return globals()["get_ai_client"]()
    except Exception:
        pass
    try:
        from openai import OpenAI  # type: ignore
    except Exception:
        return None
    import os as _os
    key = (
        st.secrets.get("openai_api_key")
        or st.secrets.get("OPENAI_API_KEY")
        or _os.environ.get("OPENAI_API_KEY")
    )
    if not key:
        return None
    try:
        client = OpenAI(api_key=key)
        return client
    except Exception:
        return None

def _resolve_model():
    try:
        import streamlit as st
        return st.secrets.get('openai_model') or st.secrets.get('OPENAI_MODEL') or 'gpt-4o-mini'
    except Exception:
        return 'gpt-4o-mini'


def _rfp_highlight_css():
    """Inject CSS once for highlighted previews with human-friendly ChatGPT-like typography."""
    try:
        import streamlit as _st
        if not _st.session_state.get("_rfp_hl_css", False):
            _st.markdown(
                """
                <style>
                :root {
                  --rfp-font-size: 0.98rem;
                  --rfp-line: 1.7;
                  --rfp-max: 72ch;
                }
                .hl-req   { background: #fff3b0; padding: 0 3px; border-radius: 4px; }
                .hl-due   { background: #ffd6a5; padding: 0 3px; border-radius: 4px; }
                .hl-poc   { background: #caffbf; padding: 0 3px; border-radius: 4px; }
                .hl-price { background: #bde0fe; padding: 0 3px; border-radius: 4px; }
                .hl-task  { background: #e0bbff; padding: 0 3px; border-radius: 4px; }
                .hl-clin  { background: #bbf7d0; padding: 0 3px; border-radius: 4px; }
                .hl-mark  { background: #f1f1f1; padding: 0 3px; border-radius: 4px; }

                .rfp-typo {
                  font-family: ui-sans-serif, system-ui, -apple-system, "Segoe UI", Roboto, Helvetica, Arial, "Apple Color Emoji","Segoe UI Emoji";
                  font-size: var(--rfp-font-size);
                  line-height: var(--rfp-line);
                  letter-spacing: .005em;
                  -webkit-font-smoothing: antialiased;
                  text-rendering: optimizeLegibility;
                  color: inherit;
                  word-break: normal;
                  hyphens: auto;
                  max-width: var(--rfp-max);
                }
                .rfp-typo p { margin: 0 0 1rem; }
                .rfp-typo p + p { margin-top: 0; }
                .rfp-typo ul, .rfp-typo ol { margin: 0 0 1rem 1.25rem; padding: 0; }
                .rfp-typo li { margin: .25rem 0; }
                .rfp-typo a { color: inherit; text-decoration: underline; }
                .rfp-typo code { font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, "Liberation Mono", monospace; font-size: .92em; }
                @media (max-width: 640px) {
                  .rfp-typo { font-size: 1rem; line-height: 1.8; max-width: 100%; }
                }
                </style>
                """,
                unsafe_allow_html=True,
            )
            _st.session_state["_rfp_hl_css"] = True
    except Exception:
        pass



def _rfp_render_summary(txt: str):
    """Render a highlighted summary safely as HTML."""
    import streamlit as _st
    _rfp_highlight_css()
    _st.markdown(_rfp_highlight_html(txt or ""), unsafe_allow_html=True)

def _rfp_highlight_html(txt: str) -> str:
    """Return HTML wrapping ORIGINAL Markdown with selective highlights. No escaping. GPT-like typography."""
    import re
    _rfp_highlight_css()  # ensure styles are present
    if not txt:
        return "<div class='rfp-typo'>(empty)</div>"
    src = txt or ""  # do NOT escape; we want Markdown to render

    # Priority: due > price > poc/email/phone > clin > req > task
    patterns = [
        ("due",   re.compile(r"(?i)\b(proposal due|response due|closing (?:date|time)|due date|submission deadline|closing)\b")),
        ("price", re.compile(r"(?i)\b(price(?:\s+(?:realism|reasonableness))?|pricing|cost|bid|quote|fee|far\s*15\.4|service contract act|sca|davis[- ]bacon|wage determination|wd)\b")),
        ("poc",   re.compile(r"(?i)\b(point of contact|poc|contracting officer|co|contract specialist)\b")),
        ("poc",   re.compile(r"(?i)[A-Z0-9._%+-]+@[A-Z0-9.-]+\.[A-Z]{2,}")),
        ("poc",   re.compile(r"(?i)\+?1?\s*\(?\d{3}\)?[\s.-]?\d{3}[\s.-]?\d{4}")),
        ("clin",  re.compile(r"(?i)\bCLIN\s*[:#-]?\s*[0-9A-Z.-]+\b")),
        ("req",   re.compile(r"(?i)\b(shall|must|required|mandatory|shall not|no later than|will)\b")),
        ("task",  re.compile(r"(?i)\b(scope of work|statement of work|sow|performance work statement|pws|deliverables?|requirements?|period of performance|pop|place of performance|provide|deliver|implement|support|manage|prepare|submit|develop|perform|test|train)\b")),
    ]

    out_lines = []
    for line in src.splitlines(True):  # keep newline chars
        # Leave blank lines untouched
        if not line.strip():
            out_lines.append(line)
            continue

        # Respect leading indentation/bullets so Markdown renders correctly
        stripped = line.lstrip()
        prefix = line[:len(line)-len(stripped)]
        content = stripped

        # One highlight max per visual line
        best = None
        for cls, pat in patterns:
            m = pat.search(content)
            if not m:
                continue
            st = m.start()
            if best is None or st < best[0]:
                best = (st, m.end(), cls)
            if best and best[0] == 0:
                break

        if best is None:
            out_lines.append(line)
        else:
            a, b, cls = best
            highlighted = content[:a] + f"<span class='hl-{cls}'>" + content[a:b] + "</span>" + content[b:]
            out_lines.append(prefix + highlighted)

    result = "".join(out_lines)
    return "<div class='rfp-typo'>\n" + result + "\n</div>"

def _extract_pricing_factors_text(text: str, max_hits: int = 20) -> list[str]:
    if not text:
        return []
    hits = []
    lines = [l.strip() for l in text.splitlines() if l.strip()]
    keys = re.compile(r"(?i)\\b(price realism|price reasonableness|best value|tradeoff|lpta|basis of award|evaluation factors? for award|most advantageous|lowest price)\\b")
    for ln in lines:
        if keys.search(ln):
            hits.append(ln)
            if len(hits) >= max_hits:
                break
    return hits

def _extract_task_lines(text: str, max_hits: int = 30) -> list[str]:
    if not text:
        return []
    hits = []
    lines = [l.strip() for l in text.splitlines()]
    section = None
    for ln in lines:
        low = ln.lower()
        if any(h in low for h in ["scope of work", "statement of work", "performance work statement", "pws", "sow", "tasks", "deliverables"]):
            section = "task"
        if section == "task":
            # Collect bullets or numbered lines as tasks
            if re.match(r"^\\s*(?:[-*•\\u2022]|\\(?[a-z0-9]\\)|\\d+\\.)\\s+", ln, re.IGNORECASE):
                hits.append(ln.strip())
                if len(hits) >= max_hits:
                    break
            # Stop if a new all-caps section header appears
            if re.match(r"^[A-Z][A-Z \\-/]{6,}$", ln.strip()):
                section = None
    # Fallback: catch "shall" sentences
    if not hits:
        for ln in lines:
            if re.search(r"(?i)\\b(shall|must)\\b", ln):
                hits.append(ln.strip())
                if len(hits) >= max_hits:
                    break
    return hits

    return st.secrets.get("openai_model") or st.secrets.get("OPENAI_MODEL") or "gpt-4o-mini"

def _ai_chat(prompt: str) -> str:
    client = _resolve_openai_client()
    if not client:
        return "AI response unavailable (no OpenAI key configured)."
    try:
        resp = client.chat.completions.create(
            model=_resolve_model(),
            messages=[
                {"role": "system", "content": "You are a contracts analyst writing precise, concise outputs tailored for federal RFPs."},
                {"role": "user", "content": prompt},
            ],
            temperature=0.2,
        )
        return (resp.choices[0].message.content or "").strip()
    except Exception as e:
        return f"AI response unavailable: {e}"

# ---- Simple extractors (regex) ----
def _extract_naics(text: str) -> str:
    m = re.search(r"NAICS[^0-9]*([0-9]{6})", text, re.IGNORECASE)
    return m.group(1) if m else ""

def _extract_due_date(text: str) -> str:
    # crude; looks for "due" "closing" etc.
    m = re.search(r"(?:due|closing|proposal (?:due|deadline))[:\s]+([A-Z][a-z]{2,9}\s+\d{1,2},\s+\d{4}|\d{1,2}/\d{1,2}/\d{2,4}|\d{4}-\d{2}-\d{2})", text, re.IGNORECASE)
    return m.group(1) if m else ""

def _extract_pop_state(text: str) -> str:
    m = re.search(r"\b(?:Place of Performance|POP)[:\s]+([A-Za-z ,]+)\b", text, re.IGNORECASE)
    candidate = m.group(1) if m else ""
    m2 = re.search(r"\b(AL|AK|AZ|AR|CA|CO|CT|DE|DC|FL|GA|HI|ID|IL|IN|IA|KS|KY|LA|ME|MD|MA|MI|MN|MS|MO|MT|NE|NV|NH|NJ|NM|NY|NC|ND|OH|OK|OR|PA|RI|SC|SD|TN|TX|UT|VT|VA|WA|WV|WI|WY)\b", candidate.upper())
    return m2.group(1) if m2 else ""

def _extract_evaluation_factors(text: str) -> str:
    # naive pull of "Section M" content lines
    m = re.search(r"SECTION\s*M[:\s\\-]+(.*?)(?:SECTION\s*[NQ]|ATTACHMENT|EVALUATION CRITERIA END)", text, re.IGNORECASE | re.DOTALL)
    if m:
        return re.sub(r"\s{3,}", "  ", m.group(1).strip())[:1200]
    return ""

def _sentences(text: str) -> List[str]:
    # basic split by period/semicolon/newlines
    chunks = re.split(r"(?<=[\.\?\!])\s+|\n+", text)
    return [c.strip() for c in chunks if c.strip()]

def _find_requirements(text: str) -> List[str]:
    reqs = []
    for s in _sentences(text):
        if re.search(r"\b(shall|must)\b", s, re.IGNORECASE):
            reqs.append(s)
    # de-dup
    seen = set()
    out = []
    for r in reqs:
        k = r.lower()
        if k not in seen:
            seen.add(k)
            out.append(r)
    return out[:400]

# ---- Draft outline ----
DEFAULT_SECTIONS = [
    "Executive Summary",
    "Technical Approach",
    "Management & Staffing",
    "Quality Assurance / QC",
    "Past Performance",
    "Risk & Mitigation",
    "Pricing Narrative (non-cost)",
    "Compliance Matrix Response Summary"
]

def _draft_section(section: str, context: str) -> str:
    prompt = f"""
You are a veteran federal proposal writer. Write ONLY the section titled '{section}'.
{_style_guide()}
Rules:
- Stay strictly within the scope of '{section}'. Do not include Risk Mitigation, Quality, Staffing, Management, Past Performance, or Pricing.
- One idea per paragraph. Sentences average 14–20 words.
- Refer to the contractor as 'ELA Management' (no 'we' or 'us'). Use present-tense commitments. Avoid hedges.
- Include one proof point: a metric, artifact, or timeline relevant to the section.
- Close with a promise line tied to risk control.

RFP context (truncated):
{context[:6000]}
""".strip()
    draft = _ai_chat(prompt)
    mixed = _assemble_section_output(section, draft, context)
    return _finalize_section(section, mixed)

def run_rfp_analyzer_onepage(pages: List[Dict[str, Any]]) -> None:
    st.title("RFP Analyzer — One‑Page View")
    if not pages:
        st.info("No parsed pages were provided.")
        return

    # Combine texts
    by_file: Dict[str, List[str]] = {}
    for p in pages:
        by_file.setdefault(p.get("file") or "Unknown", []).append(p.get("text") or "")
    combined = "\n\n".join(["\n".join(v) for v in by_file.values()])

    tab_overview, tab_require, tab_proposal, tab_notes = st.tabs(
        ["Overview", "Requirements & Compliance", "Proposal Draft", "Notes & Search"]
    )

    # Overview: key facts and document summaries
    with tab_overview:
        c1, c2, c3, c4 = st.columns(4)
        with c1:
            st.metric("NAICS", _extract_naics(combined) or "—")
        with c2:
            st.metric("Due Date", _extract_due_date(combined) or "—")
        with c3:
            st.metric("POP (State)", _extract_pop_state(combined) or "—")
        with c4:
            st.metric("# Files", str(len(by_file)))

        st.subheader("Summaries")
        if st.button("Summarize All Documents ▶", type="primary", key="onepage_summarize_all"):
            sums: Dict[str, str] = {}
            for fname, texts in by_file.items():
                t = "\n".join(texts)
                prompt = (
                    f"Summarize the document '{fname}' for a federal government contracting capture/proposal team. "
                    "Highlight scope, key deliverables, key dates, Section L instructions, and Section M evaluation factors.\n\n"
                    f"{t[:12000]}"
                )
                sums[fname] = _ai_chat(prompt)
            st.session_state["onepage_summaries"] = sums
        sums = st.session_state.get("onepage_summaries") or {}
        if sums:
            for fname, ss in sums.items():
                with st.expander(f"Summary — {fname}", expanded=False):
                    _rfp_highlight_css()
                    st.markdown(_rfp_highlight_html(ss or ""), unsafe_allow_html=True)

    # Requirements & compliance view
    with tab_require:
        st.subheader("Compliance Snapshot (auto-extracted L/M obligations)")
        reqs = _find_requirements(combined)
        if not reqs:
            st.info("No clear 'shall/must' obligations detected. (Section L/M not found or documents are scanned.)")
        else:
            draft_map = st.session_state.get("onepage_draft") or {}
            drafted_all = "\n\n".join(draft_map.values()) if draft_map else ""
            covered = 0
            for r in reqs[:100]:
                hit = (len(r) > 20 and r[:20].lower() in drafted_all.lower())
                st.checkbox(("✅ " if hit else "⬜️ ") + r, value=bool(hit), key=f"req_{abs(hash(r))}")
                covered += int(bool(hit))
            st.caption(f"Coverage (light heuristic): {covered} / {min(100, len(reqs))} shown.")

    # Proposal drafting workspace
    with tab_proposal:
        st.subheader("Proposal Draft")
        sel = st.multiselect(
            "Sections to draft",
            DEFAULT_SECTIONS,
            default=DEFAULT_SECTIONS,
            key="onepage_sections_to_draft",
        )


        # Draft all sections via background job record
        if st.button(
            "Draft All Sections ▶",
            type="primary",
            help="Generate a first-pass draft for the selected sections.",
            key="onepage_draft_all",
        ):
            if not sel:
                st.warning("Select at least one section to draft.")
            else:
                # Enqueue job; execution happens in the background worker
                conn_jobs = get_db()
                payload = {
                    "scope": "rfp_onepage_proposal_draft",
                    "sections": list(sel),
                    "notice_id": st.session_state.get("onepage_notice_id"),
                    "context": combined,
                }
                job_id = jobs_enqueue(conn_jobs, job_type="pb_draft_all", payload=payload)
                st.session_state["pb_draft_all_last_job_id"] = job_id
                st.info("Draft job submitted. It will run in the background; see status below.")

        # Pull in completed background drafts from the last job, if any
        try:
            last_job_id = st.session_state.get("pb_draft_all_last_job_id")
            if last_job_id:
                import pandas as _pd
                import json as _json
                conn_jobs = get_db()
                ensure_jobs_schema(conn_jobs)
                _df = _pd.read_sql_query(
                    "SELECT status, result_json FROM jobs WHERE id = ?",
                    conn_jobs,
                    params=(int(last_job_id),),
                )
                if not _df.empty:
                    _status = str(_df.iloc[0].get("status") or "").lower()
                    _result_json = _df.iloc[0].get("result_json") or ""
                    if _status == "done" and _result_json:
                        try:
                            _data = _json.loads(_result_json) if isinstance(_result_json, str) else {}
                        except Exception:
                            _data = {}
                        _job_draft = _data.get("draft") or {}
                        if _job_draft:
                            st.session_state["onepage_draft"] = _job_draft
        except Exception:
            pass

        draft = st.session_state.get("onepage_draft") or {}
        if draft:
            for sec, body in draft.items():
                with st.expander(f"📝 {sec}", expanded=False):
                    st.text_area("Text", value=body, height=240, key=f"ta_{abs(hash(sec))}")

        # Lightweight job status panel for this feature
        with st.expander("Draft-all job status", expanded=False):
            import pandas as _pd

            conn_jobs = get_db()
            ensure_jobs_schema(conn_jobs)

            try:
                _user_name = get_current_user_name()
            except Exception:
                _user_name = ""

            if not _user_name:
                st.caption("Select a user in the sidebar to see your jobs.")
            else:
                _df_jobs = jobs_list_for_user(conn_jobs, user_name=_user_name, limit=25)
                if not _df_jobs.empty:
                    try:
                        _df_jobs = _df_jobs[_df_jobs["type"] == "pb_draft_all"]
                    except Exception:
                        pass
                if _df_jobs.empty:
                    st.caption("No 'Draft all sections' jobs for this user yet.")
                else:
                    cols = [
                        c
                        for c in [
                            "id",
                            "type",
                            "status",
                            "progress",
                            "created_at",
                            "started_at",
                            "finished_at",
                            "error_message",
                        ]
                        if c in _df_jobs.columns
                    ]
                    st.dataframe(
                        _df_jobs[cols].head(10),
                        use_container_width=True,
                        hide_index=True,
                    )


    # Notes and full-text search
    with tab_notes:
        st.subheader("Quick Search (full text)")
        q = st.text_input("Find text", placeholder="evaluation factor, CLIN, deliverables...", key="onepage_search")
        if q:
            hits = [(i, s) for i, s in enumerate(_sentences(combined), start=1) if q.lower() in s.lower()]
            if not hits:
                st.info("No matches.")
            else:
                for i, s in hits[:100]:
                    st.write(f"**{i}.** {s}")


def getenv_int(name: str, default: int) -> int:
    try:
        return int(os.environ.get(name, default))
    except Exception:
        return default


import os
from types import SimpleNamespace as _NS

SETTINGS = _NS(
    APP_NAME=os.environ.get("ELA_APP_NAME", "ELA GovCon Suite"),
    APP_VERSION=os.environ.get("ELA_APP_VERSION", "X-Base"),
    DATA_DIR=os.environ.get("ELA_DATA_DIR", "data"),
    UPLOADS_SUBDIR=os.environ.get("ELA_UPLOADS_SUBDIR", "uploads"),
    DEFAULT_PAGE_SIZE=getenv_int("ELA_PAGE_SIZE", 50),
)

SETTINGS.UPLOADS_DIR = os.path.join(SETTINGS.DATA_DIR, SETTINGS.UPLOADS_SUBDIR)

# Ensure directories exist
try:
    os.makedirs(SETTINGS.DATA_DIR, exist_ok=True)
    os.makedirs(SETTINGS.UPLOADS_DIR, exist_ok=True)
except Exception:
    pass

# Back-compat constants
DATA_DIR = SETTINGS.DATA_DIR
UPLOADS_DIR = SETTINGS.UPLOADS_DIR

# Feature flag alias
def flag(name: str, default: bool=False) -> bool:
    return feature_flag(name, default)

# External

APP_TITLE = "ELA GovCon Suite"
BUILD_LABEL = "Master A–F — SAM • RFP Analyzer • L&M • Proposal • Subs+Outreach • Quotes • Pricing • Win Prob • Chat • Capability"

def apply_theme_phase1():
    import streamlit as st

    if st.session_state.get("_phase1_theme_applied"):
        return
    st.session_state["_phase1_theme_applied"] = True
    st.markdown('''
    <style>
    .block-container {padding-top: 1.2rem; padding-bottom: 1.2rem; max-width: 1400px;}
    h1, h2, h3 {margin-bottom: .4rem;}
    .ela-subtitle {color: rgba(49,51,63,0.65); font-size: .95rem; margin-bottom: 1rem;}
    div[data-testid="stDataFrame"] thead th {position: sticky; top: 0; background: #fff; z-index: 2;}
    div[data-testid="stDataFrame"] tbody tr:hover {background: rgba(64,120,242,0.06);}
    [data-testid="stExpander"] {border: 1px solid rgba(49,51,63,0.16); border-radius: 12px; margin-bottom: 10px;}
    [data-testid="stExpander"] summary {font-weight: 600;}
    .ela-card {border: 1px solid rgba(49,51,63,0.16); border-radius: 12px; padding: 12px; margin-bottom: 12px;}
    .ela-chip {display:inline-block; padding: 2px 8px; border-radius: 999px; font-size: 12px; margin-right:6px; background: rgba(49,51,63,0.06);}
    .ela-ok {background: rgba(0,200,83,0.12);} .ela-warn {background: rgba(251,140,0,0.12);} .ela-bad {background: rgba(229,57,53,0.12);}
    .stTextInput>div>div>input, .stNumberInput input, .stTextArea textarea {border-radius: 10px !important;}
    button[kind="primary"] {box-shadow: 0 1px 4px rgba(0,0,0,0.08);}
    .ela-banner {position: sticky; top: 0; z-index: 999; background: linear-gradient(90deg, #4068f2, #7a9cff); color: #fff; padding: 6px 12px; border-radius: 8px; margin-bottom: 10px;}
    </style>
    ''', unsafe_allow_html=True)
    st.markdown("<div class='ela-banner'>Phase 1 theme active · polished layout & tables</div>", unsafe_allow_html=True)

st.set_page_config(page_title=APP_TITLE, layout="wide", initial_sidebar_state="expanded")
apply_theme_phase1()

# === Y0: GPT-5 Thinking CO assistant (streaming) ===
try:
    from openai import OpenAI as _Y0OpenAI
except Exception:
    _Y0OpenAI = None

SYSTEM_CO = ("Act as a GS-1102 Contracting Officer. Cite exact pages. "
             "Flag non-compliance. Be concise. If evidence is missing, say so.")

# === helper: auto-select number of sources to cite (Y1–Y3) ===

import os
import tempfile

def _resolve_model():
    try:
        import streamlit as st
        return st.secrets.get('openai_model') or st.secrets.get('OPENAI_MODEL') or 'gpt-4o-mini'
    except Exception:
        return 'gpt-4o-mini'


def _rfp_highlight_css():
    """Inject CSS once for highlighted previews with human-friendly ChatGPT-like typography."""
    try:
        import streamlit as _st
        if not _st.session_state.get("_rfp_hl_css", False):
            _st.markdown(
                """
                <style>
                :root {
                  --rfp-font-size: 0.98rem;
                  --rfp-line: 1.7;
                  --rfp-max: 72ch;
                }
                .hl-req   { background: #fff3b0; padding: 0 3px; border-radius: 4px; }
                .hl-due   { background: #ffd6a5; padding: 0 3px; border-radius: 4px; }
                .hl-poc   { background: #caffbf; padding: 0 3px; border-radius: 4px; }
                .hl-price { background: #bde0fe; padding: 0 3px; border-radius: 4px; }
                .hl-task  { background: #e0bbff; padding: 0 3px; border-radius: 4px; }
                .hl-clin  { background: #bbf7d0; padding: 0 3px; border-radius: 4px; }
                .hl-mark  { background: #f1f1f1; padding: 0 3px; border-radius: 4px; }

                .rfp-typo {
                  font-family: ui-sans-serif, system-ui, -apple-system, "Segoe UI", Roboto, Helvetica, Arial, "Apple Color Emoji","Segoe UI Emoji";
                  font-size: var(--rfp-font-size);
                  line-height: var(--rfp-line);
                  letter-spacing: .005em;
                  -webkit-font-smoothing: antialiased;
                  text-rendering: optimizeLegibility;
                  color: inherit;
                  word-break: normal;
                  hyphens: auto;
                  max-width: var(--rfp-max);
                }
                .rfp-typo p { margin: 0 0 1rem; }
                .rfp-typo p + p { margin-top: 0; }
                .rfp-typo ul, .rfp-typo ol { margin: 0 0 1rem 1.25rem; padding: 0; }
                .rfp-typo li { margin: .25rem 0; }
                .rfp-typo a { color: inherit; text-decoration: underline; }
                .rfp-typo code { font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, "Liberation Mono", monospace; font-size: .92em; }
                @media (max-width: 640px) {
                  .rfp-typo { font-size: 1rem; line-height: 1.8; max-width: 100%; }
                }
                </style>
                """,
                unsafe_allow_html=True,
            )
            _st.session_state["_rfp_hl_css"] = True
    except Exception:
        pass


def _rfp_highlight_html(txt: str) -> str:
    """Return HTML wrapping ORIGINAL Markdown with selective highlights. No escaping. GPT-like typography."""
    import re
    _rfp_highlight_css()  # ensure styles are present
    if not txt:
        return "<div class='rfp-typo'>(empty)</div>"
    src = txt or ""  # do NOT escape; we want Markdown to render

    # Priority: due > price > poc/email/phone > clin > req > task
    patterns = [
        ("due",   re.compile(r"(?i)\b(proposal due|response due|closing (?:date|time)|due date|submission deadline|closing)\b")),
        ("price", re.compile(r"(?i)\b(price(?:\s+(?:realism|reasonableness))?|pricing|cost|bid|quote|fee|far\s*15\.4|service contract act|sca|davis[- ]bacon|wage determination|wd)\b")),
        ("poc",   re.compile(r"(?i)\b(point of contact|poc|contracting officer|co|contract specialist)\b")),
        ("poc",   re.compile(r"(?i)[A-Z0-9._%+-]+@[A-Z0-9.-]+\.[A-Z]{2,}")),
        ("poc",   re.compile(r"(?i)\+?1?\s*\(?\d{3}\)?[\s.-]?\d{3}[\s.-]?\d{4}")),
        ("clin",  re.compile(r"(?i)\bCLIN\s*[:#-]?\s*[0-9A-Z.-]+\b")),
        ("req",   re.compile(r"(?i)\b(shall|must|required|mandatory|shall not|no later than|will)\b")),
        ("task",  re.compile(r"(?i)\b(scope of work|statement of work|sow|performance work statement|pws|deliverables?|requirements?|period of performance|pop|place of performance|provide|deliver|implement|support|manage|prepare|submit|develop|perform|test|train)\b")),
    ]

    out_lines = []
    for line in src.splitlines(True):  # keep newline chars
        # Leave blank lines untouched
        if not line.strip():
            out_lines.append(line)
            continue

        # Respect leading indentation/bullets so Markdown renders correctly
        stripped = line.lstrip()
        prefix = line[:len(line)-len(stripped)]
        content = stripped

        # One highlight max per visual line
        best = None
        for cls, pat in patterns:
            m = pat.search(content)
            if not m:
                continue
            st = m.start()
            if best is None or st < best[0]:
                best = (st, m.end(), cls)
            if best and best[0] == 0:
                break

        if best is None:
            out_lines.append(line)
        else:
            a, b, cls = best
            highlighted = content[:a] + f"<span class='hl-{cls}'>" + content[a:b] + "</span>" + content[b:]
            out_lines.append(prefix + highlighted)

    result = "".join(out_lines)
    return "<div class='rfp-typo'>\n" + result + "\n</div>"

def _extract_pricing_factors_text(text: str, max_hits: int = 20) -> list[str]:
    if not text:
        return []
    hits = []
    lines = [l.strip() for l in text.splitlines() if l.strip()]
    keys = re.compile(r"(?i)\\b(price realism|price reasonableness|best value|tradeoff|lpta|basis of award|evaluation factors? for award|most advantageous|lowest price)\\b")
    for ln in lines:
        if keys.search(ln):
            hits.append(ln)
            if len(hits) >= max_hits:
                break
    return hits

def _extract_task_lines(text: str, max_hits: int = 30) -> list[str]:
    if not text:
        return []
    hits = []
    lines = [l.strip() for l in text.splitlines()]
    section = None
    for ln in lines:
        low = ln.lower()
        if any(h in low for h in ["scope of work", "statement of work", "performance work statement", "pws", "sow", "tasks", "deliverables"]):
            section = "task"
        if section == "task":
            # Collect bullets or numbered lines as tasks
            if re.match(r"^\\s*(?:[-*•\\u2022]|\\(?[a-z0-9]\\)|\\d+\\.)\\s+", ln, re.IGNORECASE):
                hits.append(ln.strip())
                if len(hits) >= max_hits:
                    break
            # Stop if a new all-caps section header appears
            if re.match(r"^[A-Z][A-Z \\-/]{6,}$", ln.strip()):
                section = None
    # Fallback: catch "shall" sentences
    if not hits:
        for ln in lines:
            if re.search(r"(?i)\\b(shall|must)\\b", ln):
                hits.append(ln.strip())
                if len(hits) >= max_hits:
                    break
    return hits

    # Priority: Streamlit secrets -> env var -> safe default
    try:
        import streamlit as st

        for key in ("OPENAI_MODEL", "openai_model", "model"):
            try:
                val = st.secrets.get(key)  # type: ignore[attr-defined]
                if isinstance(val, str) and val.strip():
                    return val.strip()
            except Exception:
                pass
    except Exception:
        pass
    return os.getenv("OPENAI_MODEL", "gpt-4o-mini")

_ai_client = None
def get_ai():
    import streamlit as st

    global _ai_client
    if _ai_client is None:
        if _Y0OpenAI is None:
            raise RuntimeError("openai library missing")
        _ai_client = _Y0OpenAI()  # uses OPENAI_API_KEY from Streamlit secrets
    return _ai_client

def ask_ai(messages, tools=None, temperature=0.2):
    client = get_ai()
    model_name = _resolve_model()
    try:
        resp = client.chat.completions.create(
            model=model_name,
            messages=(messages if (messages and isinstance(messages, list) and isinstance(messages[0], dict) and messages[0].get('role')=='system') else [{"role":"system","content": SYSTEM_CO}, *messages]),
            tools=tools or [],
            temperature=float(temperature),
            stream=True
        )
    except Exception as _e:
        if "model_not_found" in str(_e) or "does not exist" in str(_e):
            model_name = "gpt-4o-mini"
            resp = client.chat.completions.create(
                model=model_name,
                messages=[{"role":"system","content": SYSTEM_CO}, *messages],
                tools=tools or [],
                temperature=float(temperature),
                stream=True
            )
        else:
            yield f"AI unavailable: {type(_e).__name__}: {_e}"
            return
    for ch in resp:
        try:
            delta = ch.choices[0].delta
            if hasattr(delta, "content") and delta.content:
                yield delta.content
        except Exception:
            pass

def y0_ai_panel():

    st.header(f"Ask the CO (AI) · {_resolve_model()}")
    q = st.text_area("Your question", key="y0_q", height=120)
    if st.button("Ask", key="y0_go"):
        if not (q or "").strip():
            st.warning("Enter a question")
        else:
            ph = st.empty()
            acc = []
            for tok in ask_ai([{"role":"user","content": q.strip()}]):
                acc.append(tok)
                ph.markdown("".join(acc))
# === end Y0 ===

# --- key namespacing helper (Phase U) ---
DATA_DIR = "data"
DEFAULT_SQLITE_PATH = os.path.join(DATA_DIR, "govcon.db")
UPLOADS_DIR = os.path.join(DATA_DIR, "uploads")
SAM_ENDPOINT = "https://api.sam.gov/opportunities/v2/search"

# Central database URL so we can point the app at SQLite or Postgres.
# If DATABASE_URL is not set, default to a local SQLite file.
DATABASE_URL = os.getenv("DATABASE_URL", f"sqlite:///{DEFAULT_SQLITE_PATH}").strip() or f"sqlite:///{DEFAULT_SQLITE_PATH}"

# For legacy helpers that still expect a filesystem path, keep DB_PATH pointing
# at the SQLite file. When DATABASE_URL points at Postgres we will ignore
# DB_PATH and connect directly with psycopg2 in get_db().
DB_PATH = DEFAULT_SQLITE_PATH

# -------------------- setup --------------------
def ensure_dirs() -> None:
    os.makedirs(DATA_DIR, exist_ok=True)
    os.makedirs(UPLOADS_DIR, exist_ok=True)
# === Y1: Retrieval (chunks • embeddings • citations) ===
def _ensure_y1_schema(conn: "sqlite3.Connection") -> None:
    try:
        with closing(conn.cursor()) as cur:
            cur.execute("""
                CREATE TABLE IF NOT EXISTS rfp_chunks(
                    id INTEGER PRIMARY KEY,
                    rfp_id INTEGER,
                    rfp_file_id INTEGER,
                    file_name TEXT,
                    page INTEGER,
                    chunk_idx INTEGER,
                    text TEXT,
                    emb TEXT
                );
            """)
            cur.execute("CREATE INDEX IF NOT EXISTS idx_chunks_rfp ON rfp_chunks(rfp_id);")
            cur.execute("CREATE UNIQUE INDEX IF NOT EXISTS uq_chunk_key ON rfp_chunks(rfp_file_id, page, chunk_idx);")
            conn.commit()
    except Exception:
        pass

def _resolve_embed_model() -> str:
    try:
        import streamlit as _st
        for k in ("OPENAI_EMBED_MODEL","openai_embed_model","EMBED_MODEL"):
            v = _st.secrets.get(k)
            if isinstance(v, str) and v.strip():
                return v.strip()
    except Exception:
        pass
    return os.getenv("OPENAI_EMBED_MODEL", "text-embedding-3-small")
# === PHASE 6: Embedding cache (sha256 of model:text) ===
def _embed_cache_dir() -> str:
    d = os.path.join(DATA_DIR, "embed_cache")
    try:
        os.makedirs(d, exist_ok=True)
    except Exception:
        pass
    return d

def _embed_cache_key(model: str, text: str) -> str:
    h = hashlib.sha256(f"{model}:{text}".encode("utf-8")).hexdigest()
    return h

def _embed_cache_get(model: str, texts: list[str]) -> tuple[list[list[float]|None], list[int]]:
    r"""
    Return (vecs_or_None, missing_idx). For each input text, either a vector or None.
    Also returns indices of missing items for batch computation.
    r"""
    out = []
    missing = []
    d = _embed_cache_dir()
    for i, t in enumerate(texts):
        key = _embed_cache_key(model, t or " ")
        fp = os.path.join(d, f"{key}.json")
        try:
            with open(fp, "r", encoding="utf-8") as fh:
                v = json.load(fh)
            if isinstance(v, list):
                out.append(v)
                continue
        except Exception:
            pass
        out.append(None)
        missing.append(i)
    return out, missing

def _embed_cache_put(model: str, texts: list[str], vecs: list[list[float]]) -> None:
    d = _embed_cache_dir()
    for t, v in zip(texts, vecs):
        try:
            key = _embed_cache_key(model, t or " ")
            fp = os.path.join(d, f"{key}.json")
            with open(fp, "w", encoding="utf-8") as fh:
                json.dump(v, fh)
        except Exception:
            pass
# === end PHASE 6 helpers ===

def _embed_texts(texts: list[str]) -> list[list[float]]:
    client = get_ai()
    model = _resolve_embed_model()
    clean = [t if (t or "").strip() else " " for t in texts]

    # cache lookup
    cached_vecs, missing_idx = _embed_cache_get(model, clean)
    to_compute = [clean[i] for i in missing_idx]

    new_vecs: list[list[float]] = []
    if to_compute:
        try:
            resp = client.embeddings.create(model=model, input=to_compute)
            for d in resp.data:
                try:
                    new_vecs.append(list(d.embedding))
                except Exception:
                    new_vecs.append([])
            # write cache
            _embed_cache_put(model, to_compute, new_vecs)
        except Exception:
            # fallback: return empty vecs for missing
            new_vecs = [[] for _ in to_compute]

    # merge back preserving order
    out: list[list[float]] = []
    it = iter(new_vecs)
    for v in cached_vecs:
        if v is None:
            out.append(next(it, []))
        else:
            out.append(v)
    return out

def _cos_sim(u: list[float], v: list[float]) -> float:
    if not u or not v:
        return 0.0
    try:
        import numpy as _np
        a = _np.array(u, dtype=float); b = _np.array(v, dtype=float)
        num = float((_np.dot(a, b)))
        den = float(_np.linalg.norm(a) * _np.linalg.norm(b))
        return (num / den) if den else 0.0
    except Exception:
        num = 0.0; su = 0.0; sv = 0.0
        for i in range(min(len(u), len(v))):
            x = float(u[i]); y = float(v[i])
            num += x*y; su += x*x; sv += y*y
        den = (su**0.5) * (sv**0.5)
        return (num / den) if den else 0.0

def _split_chunks(text: str, max_chars: int = 1200, overlap: int = 180) -> list[str]:
    t = (text or "").strip()
    if not t:
        return []
    chunks = []
    i = 0
    n = len(t)
    while i < n:
        j = min(n, i + max(200, max_chars))
        chunk = t[i:j]
        k = chunk.rfind(". ")
        if k > 900:
            chunk = chunk[:k+1]
            j = i + k + 1
        chunks.append(chunk)
        i = max(j - overlap, j)
    return chunks


def y1_index_rfp(conn: "sqlite3.Connection", rfp_id: int, max_pages: int = 100, rebuild: bool = False) -> dict:
    """
    Build or rebuild the semantic index for an RFP's files.
    """
    _ensure_y1_schema(conn)
    import sqlite3 as _sqlite3, time as _time
    df_bytes = None
    for _attempt in range(5):
        try:
            df_bytes = pd.read_sql_query(
                "SELECT id, filename, mime, bytes FROM rfp_files WHERE rfp_id=? ORDER BY id;",
                conn,
                params=(int(rfp_id),),
            )
            break
        except _sqlite3.OperationalError as e:
            if "database is locked" in str(e).lower() and _attempt < 4:
                _time.sleep(0.2)
                continue
            return {"ok": False, "error": str(e)}
        except Exception as e:
            return {"ok": False, "error": str(e)}
    if df_bytes is None or df_bytes.empty:
        return {"ok": False, "error": "No linked files"}
    added = 0
    skipped = 0
    for _, row in df_bytes.iterrows():
        fid = int(row["id"])
        name = row.get("filename") or f"file_{fid}"
        b = row.get("bytes")
        if b is None:
            # attempt to load from path if available
            p = row.get("path")
            if p:
                try:
                    with open(p, "rb") as fh:
                        b = fh.read()
                except Exception:
                    b = None
        if b is None:
            continue
        mime = row.get("mime") or _guess_mime_from_name(str(name).lower())
        try:
            index_bytes = b
            try:
                if b is not None and len(b) > MAX_RFP_INDEX_BYTES:
                    index_bytes = b[:MAX_RFP_INDEX_BYTES]
            except Exception:
                index_bytes = b
            pages = extract_text_pages(index_bytes, mime) or []
        except Exception:
            pages = []
        if pages:
            try:
                pages, _ocrn = ocr_pages_if_empty(b, mime, pages)
            except Exception:
                pass
        if not pages:
            continue
        pages = pages[:max_pages]
        for pi, txt in enumerate(pages, start=1):
            parts = _split_chunks(txt or "", 1600, 200)
            for ci, ch in enumerate(parts):
                if not ch.strip():
                    continue
                if not rebuild:
                    try:
                        existing = pd.read_sql_query(
                            "SELECT id FROM rfp_chunks WHERE rfp_file_id=? AND page=? AND chunk_idx=?;",
                            conn,
                            params=(fid, int(pi), int(ci)),
                        )
                        if existing is not None and not existing.empty:
                            skipped += 1
                            continue
                    except Exception:
                        pass
                emb = _embed_texts([ch])[0]
                import sqlite3 as _sqlite3, time as _time
                for _attempt in range(5):
                    try:
                        with closing(conn.cursor()) as cur:
                            cur.execute(
                                """
                                INSERT OR REPLACE INTO rfp_chunks(
                                    rfp_id, rfp_file_id, file_name, page, chunk_idx, text, emb
                                )
                                VALUES (?, ?, ?, ?, ?, ?, ?);
                                """,
                                (int(rfp_id), fid, name, int(pi), int(ci), ch, json.dumps(emb)),
                            )
                        break
                    except _sqlite3.OperationalError as e:
                        if "database is locked" in str(e).lower() and _attempt < 4:
                            _time.sleep(0.2)
                            continue
                        raise
                added += 1
    try:
        _fts_index_rfp(conn, int(rfp_id))
    except Exception:
        # FTS indexing is best-effort; ignore failures.
        pass
    return {"ok": True, "added": added, "skipped": skipped}
def _y1_search_uncached(conn: "sqlite3.Connection", rfp_id: int, query: str, k: int = 6) -> list[dict]:
    _ensure_y1_schema(conn)
    if not (query or "").strip():
        return []
    try:
        df = pd.read_sql_query("SELECT id, rfp_file_id, file_name, page, chunk_idx, text, emb FROM rfp_chunks WHERE rfp_id=?;", conn, params=(int(rfp_id),))
    except Exception:
        return []
    if df is None or df.empty:
        return []
    # clamp
    try:
        k = int(k)
    except Exception:
        k = 6
    k = max(1, min(8, k))

    q_emb = _embed_texts([query])[0]
    rows = []
    for _, r in df.iterrows():
        try:
            emb = json.loads(r.get("emb") or "[]")
        except Exception:
            emb = []
        sim = _cos_sim(q_emb, emb)
        rows.append({
            "id": int(r["id"]),
            "fid": int(r["rfp_file_id"]),
            "file": r.get("file_name"),
            "page": int(r.get("page") or 0),
            "chunk": int(r.get("chunk_idx") or 0),
            "text": r.get("text") or "",
            "score": float(sim),
        })
    # primary rank
    rows.sort(key=lambda x: x["score"], reverse=True)
    # de-duplicate by (file,page) to strengthen citation control
    seen = set()
    dedup = []
    for h in rows:
        key = (h.get("file") or "", int(h.get("page") or 0))
        if key in seen:
            continue
        seen.add(key)
        dedup.append(h)
        if len(dedup) >= 32:  # limit working set
            break
    # light re-rank: prefer balanced page coverage then score
    dedup.sort(key=lambda x: (-(x["score"]>0.70), -x["score"]), reverse=False)
    return dedup[:k]
# --- Safe Y1 dispatcher to avoid NameError at runtime ---
def _safe_y1_search(conn, rfp_id, query, k=6):
    try:
        return y1_search(conn, int(rfp_id), query or "", int(k)) or []
    except NameError:
        try:
            return _y1_search_uncached(conn, int(rfp_id), query or "", int(k)) or []
        except NameError:
            return []
# --- Robust Y1 shim: guarantees y1_search exists ---
if 'y1_search' not in globals():
    def y1_search(conn, rfp_id: int, query: str, k: int = 6):
        try:
            snap = _y1_snapshot(conn, int(rfp_id))
        except Exception:
            snap = None
        try:
            db_path = DB_PATH
        except Exception:
            db_path = "data/govcon.db"
        try:
            return _y1_search_cached(db_path, int(rfp_id), query or "", int(k), snap)
        except Exception:
            try:
                return _y1_search_uncached(conn, int(rfp_id), query or "", int(k))
            except Exception:
                return []



def ask_ai_with_citations(conn: "sqlite3.Connection", rfp_id: int, question: str, k: int = 6, temperature: float = 0.2, mode: str = "Auto"):
    """
    Streams an answer grounded in top page hits from the One Page Analyzer.
    No citations are included in the output.

    This implementation avoids heavy PDF parsing / OCR work in the main
    Streamlit request by reusing the Y1 semantic index (rfp_chunks). If the
    index is missing, a short advisory message is streamed instead of blocking.
    """
    import re as _re

    q = (question or "").strip()
    if not q:
        yield "Enter a question first."
        return

    # Use existing Y1 index (rfp_chunks) instead of re-extracting PDF bytes
    try:
        hits = _safe_y1_search(conn, int(rfp_id), q, k=int(k) if k else 6)
    except Exception:
        hits = []

    pages: list[dict] = []
    for h in hits or []:
        try:
            fname = str(h.get("file") or h.get("file_name") or "")
            page_no = int(h.get("page") or 0)
            txt = str(h.get("text") or "")
        except Exception:
            continue
        if not txt.strip():
            continue
        pages.append({"file": fname, "page": page_no, "text": txt})

    if not pages:
        # Index missing or empty – avoid doing heavy PDF parsing here.
        yield "[system] No One Page Analyzer index found for this RFP yet. Run 'Ingest & Analyze' or 'Rebuild index' on the Y1 tab, then retry."
        return

    # Build a clean context block without tags or citations
    snippets: list[str] = []
    for pg in pages:
        snip = (pg.get("text") or "").strip().replace("\n", " ")
        if snip:
            snippets.append(snip[:800])

    context_text = "\n\n".join(snippets)[:24000]

    # Structured system prompt with optional wash / mode specializations
    _intent_wash = bool(_re.search(r"\b(wash|clean|cleaning|rinse|rinsing)\b", q, _re.I))

    base_sys = (
        "You are a federal Contracting Officer technical reviewer. "
        "Answer only from the provided CONTEXT. No citations. "
        "Use short, direct sentences. "
        "Enumerate required services with PWS subsection numbers when present. "
        "If a data point is not found in CONTEXT, write 'Not specified in PWS'. "
    )

    # Mode-specific formatting rules
    mname = (mode or "Auto").lower()
    if mname.startswith("checklist"):
        fmt_rules = "Output a concise checklist with imperative items and sub bullets where needed."
    elif mname.startswith("phone"):
        fmt_rules = "Output a short phone call script with an opener, 6 to 10 targeted questions, and a closing commitment line."
    elif mname.startswith("email"):
        fmt_rules = "Draft a concise vendor email. Include Subject, Greeting, 2-3 short paragraphs, a numbered list of required attachments, and a clear ask with a date."
    elif mname.startswith("action"):
        fmt_rules = "Provide a step by step action plan with owners and due dates if present in context."
    elif mname.startswith("table"):
        fmt_rules = "Return a compact table when appropriate. Use Markdown table syntax."
    else:
        fmt_rules = "Choose the most efficient format to answer. Prefer lists for tasks. Use tables for verifications."

    # Expand formatting rules for wash / cleaning solicitations
    if _intent_wash:
        fmt_rules = (
            "List services as numbered items. For each, include: PWS ref, title, and a one-line requirement. "
            "Then list time standards, methods, materials, environmental controls, and acceptance exactly as bullets."
        )

    # Optional metadata derived from context
    try:
        due = _extract_due_date(context_text[:4000]) or ""
    except Exception:
        due = ""
    try:
        naics = _extract_naics(context_text[:4000]) or ""
    except Exception:
        naics = ""

    meta_lines = []
    if due:
        meta_lines.append(f"Due date: {due}")
    if naics:
        meta_lines.append(f"NAICS: {naics}")
    meta_block = "\n".join(meta_lines)

    sys_prompt = base_sys + fmt_rules
    if meta_block:
        sys_prompt += "\nUse this metadata if relevant:\n" + meta_block

    ctx_block = "CONTEXT\n" + (context_text or "[no context available]")
    user_prompt = q.strip()

    messages = [
        {"role": "system", "content": sys_prompt},
        {"role": "system", "content": ctx_block},
        {"role": "user", "content": user_prompt},
    ]

    try:
        ans = _chat_plus_call_openai(messages, temperature=temperature)
    except Exception as e:
        yield f"[AI error] {e}"
        return

    # Stream answer one character at a time for responsive UI
    for ch in ans:
        yield ch

def _y2_build_messages(conn: "sqlite3.Connection", rfp_id: int, thread_id: int, user_q: str, k: int = 6):
    """
    Build a minimal message set for CO chat, embedding local evidence as [C#].
    Returns a list of chat messages (no system role; ask_ai adds SYSTEM_CO).
    """
    q_in = (user_q or "").strip()
    hits = y1_search(conn, int(rfp_id), q_in or "Section L and Section M requirements", k=int(k)) or []
    ev_lines = []
    for i, h in enumerate(hits, start=1):
        tag = f"[C{i}]"
        src = f"{h.get('file','')} p.{h.get('page','')}"
        snip = (h.get('text') or '').strip().replace("\n", " ")
        ev_lines.append(f"{tag} {src} — {snip}")
    evidence = "\n".join(ev_lines)
    user = "QUESTION\n" + (q_in or "Provide a CO Readout.") + "\n\nEVIDENCE\n" + (evidence or "(none)")
    msgs = [{"role":"user","content": user}]
    return msgs
def _ensure_y2_schema(conn: "sqlite3.Connection") -> None:
    try:
        with closing(conn.cursor()) as cur:
            cur.execute("""
                CREATE TABLE IF NOT EXISTS y2_threads(
                    id INTEGER PRIMARY KEY,
                    rfp_id INTEGER NOT NULL,
                    title TEXT,
                    created_at TEXT
                );
            """)
            cur.execute("""
                CREATE TABLE IF NOT EXISTS y2_messages(
                    id INTEGER PRIMARY KEY,
                    thread_id INTEGER NOT NULL,
                    role TEXT CHECK(role in ('user','assistant')),
                    content TEXT,
                    created_at TEXT
                );
            """)
            cur.execute("CREATE INDEX IF NOT EXISTS idx_y2_threads_rfp ON y2_threads(rfp_id);")
            cur.execute("CREATE INDEX IF NOT EXISTS idx_y2_msgs_thread ON y2_messages(thread_id);")
            conn.commit()
    except Exception:
        pass

def y2_list_threads(conn: "sqlite3.Connection", rfp_id: int):
    _ensure_y2_schema(conn)
    try:
        df = pd.read_sql_query(
            "SELECT id, title, created_at FROM y2_threads WHERE rfp_id=? ORDER BY id DESC;",
            conn, params=(int(rfp_id),)
        )
    except Exception:
        return []
    if df is None or df.empty:
        return []
    out = []
    for _, row in df.iterrows():
        rid = int(row["id"])
        out.append({
            "id": rid,
            "title": (row.get('title') or f"Thread #{rid}"),
            "created_at": row.get('created_at') or ""
        })
    return out

def y2_create_thread(conn: "sqlite3.Connection", rfp_id: int, title: str = "CO guidance") -> int:
    _ensure_y2_schema(conn)
    from datetime import datetime as _dt
    now = _dt.utcnow().isoformat()
    with closing(conn.cursor()) as cur:
        cur.execute("INSERT INTO y2_threads(rfp_id, title, created_at) VALUES(?,?,?);",
                    (int(rfp_id), (title or "Untitled").strip(), now))
        conn.commit()
        return int(cur.lastrowid)

def y2_get_messages(conn: "sqlite3.Connection", thread_id: int):
    _ensure_y2_schema(conn)
    try:
        df = pd.read_sql_query(
            "SELECT role, content FROM y2_messages WHERE thread_id=? ORDER BY id;",
            conn, params=(int(thread_id),)
        )
    except Exception:
        return []
    if df is None or df.empty:
        return []
    return [{"role": str(r["role"]), "content": str(r["content"])} for _, r in df.iterrows()]

def y2_append_message(conn: "sqlite3.Connection", thread_id: int, role: str, content: str) -> None:
    _ensure_y2_schema(conn)
    from datetime import datetime as _dt
    now = _dt.utcnow().isoformat()
    role = "assistant" if str(role).strip().lower() != "user" else "user"
    with closing(conn.cursor()) as cur:
        cur.execute(
            "INSERT INTO y2_messages(thread_id, role, content, created_at) VALUES(?,?,?,?);",
            (int(thread_id), role, (content or "").strip(), now)
        )
        conn.commit()

def y2_rename_thread(conn: "sqlite3.Connection", thread_id: int, new_title: str) -> None:
    _ensure_y2_schema(conn)
    with closing(conn.cursor()) as cur:
        cur.execute("UPDATE y2_threads SET title=? WHERE id=?;", ((new_title or "Untitled").strip(), int(thread_id)))
        conn.commit()

def y2_delete_thread(conn: "sqlite3.Connection", thread_id: int) -> None:
    _ensure_y2_schema(conn)
    with closing(conn.cursor()) as cur:
        cur.execute("DELETE FROM y2_messages WHERE thread_id=?;", (int(thread_id),))
        cur.execute("DELETE FROM y2_threads WHERE id=?;", (int(thread_id),))
        conn.commit()
# === end Y2 thread storage helpers ===

def y2_ui_threaded_chat(conn: "sqlite3.Connection") -> None:
    st.caption("CO Chat with memory. Threads are stored per RFP.")
    df_rf = pd.read_sql_query("SELECT id, title FROM rfps ORDER BY id DESC;", conn, params=())
    if df_rf is None or df_rf.empty:
        st.info("No RFPs yet. Parse & save first.")
        return
    rfp_id = st.selectbox("RFP context", options=df_rf["id"].tolist(), format_func=lambda i: f"#{i} — {df_rf.loc[df_rf['id']==i,'title'].values[0]}", key="y2_rfp_sel")
    threads = y2_list_threads(conn, int(rfp_id))
    create = st.button("New thread", key="y2_new")
    if create:
        tid = y2_create_thread(conn, int(rfp_id), title="CO guidance")
        st.session_state["y2_thread_id"] = tid
        st.rerun()
    if threads:
        pick = st.selectbox("Thread", options=[t["id"] for t in threads], format_func=lambda i: next((f"#{t['id']} — {t.get('title') or 'Untitled'}" for t in threads if t['id']==i), f"#{i}"), key="y2_pick")
        thread_id = int(pick)
    else:
        st.info("No threads yet. Click New thread.")
        return
    with st.expander("Thread settings", expanded=False):
        cur_title = next((t.get("title") or "Untitled" for t in threads if t["id"]==thread_id), "Untitled")
        new_title = st.text_input("Rename", value=cur_title, key="y2_rename")
        colA, colB = st.columns([2,1])
        with colA:
            if st.button("Save name", key="y2_save_name"):
                y2_rename_thread(conn, int(thread_id), new_title or "Untitled")
                st.success("Renamed")
        with colB:
            if st.button("Delete thread", key="y2_del"):
                y2_delete_thread(conn, int(thread_id))
                st.success("Deleted")
    st.checkbox("Research mode (flex)", value=bool(st.session_state.get("y2_flex", False)), key="y2_flex")
    st.subheader("History")
    msgs = y2_get_messages(conn, int(thread_id))
    if msgs:
        chat_md = []
        for m in msgs:
            if m["role"] == "user":
                chat_md.append(f"**You:** {m['content']}")
            elif m["role"] == "assistant":
                chat_md.append(m["content"])
        st.markdown("\n\n".join(chat_md))
    else:
        st.caption("No messages yet.")
    q = st.text_area("Your question", height=120, key="y2_q")
    k = y_auto_k(q)
    fmt = st.selectbox("Answer format", options=["Auto","Checklist","Phone script","Email draft","Action plan","Table"], index=0, key="y2_fmt")
    if st.button("Ask (store to thread)", type="primary", key="y2_go"):
        if not (q or "").strip():
            st.warning("Enter a question")
        else:
            y2_append_message(conn, int(thread_id), "user", q.strip())
            ph = st.empty(); acc = []
            for tok in y2_stream_answer(conn, int(rfp_id), int(thread_id), q.strip(), k=int(k), mode=str(fmt or 'Auto')):
                acc.append(tok); ph.markdown("".join(acc))
            ans = "".join(acc).strip()
            if ans:
                y2_append_message(conn, int(thread_id), "assistant", ans)
                st.success("Saved to thread")
                with st.expander("Add to Proposal Drafts", expanded=False):
                    sec = st.text_input("Section label", value="CO Chat Notes", key="y5_sec_y2")
                    if st.button("Add to drafts", key="y5_add_y2"):
                        y5_save_snippet(conn, int(rfp_id), sec, ans, source="Y2 Chat")
                        st.success("Saved to drafts")
# === end Y2 ===

# === Research cache (R2) ===
def _research_cache_dir() -> str:
    d = os.path.join(DATA_DIR, "research_cache")
    try:
        os.makedirs(d, exist_ok=True)
    except Exception:
        pass
    return d

def _sha16(s: str) -> str:
    try:
        import hashlib
        return hashlib.sha256((s or "").encode("utf-8")).hexdigest()[:16]
    except Exception:
        return str(abs(hash((s or ""))))[:16]

def research_fetch(url: str, ttl_hours: int = 24) -> dict:
    """
    Fetch a URL with simple on-disk cache.
    Returns dict {url, path, cached, status, text}.
    """
    out = {"url": url, "cached": False, "status": 0, "path": "", "text": ""}
    if not (url or "").strip():
        return out
    key = _sha16(url)
    dirp = _research_cache_dir()
    meta_path = os.path.join(dirp, f"{key}.json")
    txt_path  = os.path.join(dirp, f"{key}.txt")

    # serve cache if fresh
    try:
        if os.path.exists(meta_path) and os.path.exists(txt_path):
            with open(meta_path, "r", encoding="utf-8") as fh:
                meta = json.load(fh)
            age = time.time() - float(meta.get("ts", 0))
            if age < ttl_hours * 3600:
                out.update(meta)
                with open(txt_path, "r", encoding="utf-8", errors="ignore") as fh:
                    out["text"] = fh.read()
                out["cached"] = True
                out["status"] = int(out.get("status", 200) or 200)
                out["path"] = txt_path
                return out
    except Exception:
        pass

    # fetch fresh
    try:
        import requests  # lazy import
        r = requests.get(url, timeout=20, headers={"User-Agent":"ELA-GovCon/1.0"})
        status = int(getattr(r, "status_code", 0) or 0)
        text = r.text if hasattr(r, "text") else ""
        # persist
        try:
            with open(txt_path, "w", encoding="utf-8") as fh:
                fh.write(text or "")
            with open(meta_path, "w", encoding="utf-8") as fh:
                json.dump({"url": url, "status": status, "ts": time.time(), "path": txt_path}, fh)
        except Exception:
            pass
        out.update({"status": status, "text": text, "path": txt_path})
        return out
    except Exception as e:
        out.update({"status": 0, "error": str(e)})
        return out

def research_extract_excerpt(text: str, query: str, window: int = 380) -> str:
    t = (text or "")
    if not t:
        return ""
    q = (query or "").strip()
    idx = -1
    if q:
        try:
            idx = t.lower().find(q.lower())
        except Exception:
            idx = -1
    if idx < 0:
        return t[:window]
    start = max(0, idx - window//2)
    end = min(len(t), idx + window//2)
    return t[start:end]

# === Y3: Proposal drafting from evidence (per-RFP) ===
def _y3_collect_ctx(conn: "sqlite3.Connection", rfp_id: int, max_items: int = 20) -> dict:
    ctx: dict = {}
    try:
        df_items = pd.read_sql_query("SELECT item_text FROM lm_items WHERE rfp_id=? ORDER BY id;", conn, params=(int(rfp_id),))
    except Exception:
        df_items = pd.DataFrame(columns=["item_text"])
    ctx["lm"] = df_items["item_text"].tolist()[:max_items] if isinstance(df_items, pd.DataFrame) and not df_items.empty else []
    try:
        df_clins = pd.read_sql_query("SELECT clin, description FROM clin_lines WHERE rfp_id=? ORDER BY id;", conn, params=(int(rfp_id),))
    except Exception:
        df_clins = pd.DataFrame(columns=["clin","description"])
    ctx["clins"] = [{"clin": str(r.get("clin","")), "desc": str(r.get("description",""))[:160]} for _, r in df_clins.head(max_items).iterrows()] if isinstance(df_clins, pd.DataFrame) and not df_clins.empty else []
    try:
        df_meta = pd.read_sql_query("SELECT key, value FROM rfp_meta WHERE rfp_id=?;", conn, params=(int(rfp_id),))
    except Exception:
        df_meta = pd.DataFrame(columns=["key","value"])
    if isinstance(df_meta, pd.DataFrame) and not df_meta.empty:
        ctx["meta"] = {str(k): str(v) for k,v in zip(df_meta["key"], df_meta["value"])}
    else:
        ctx["meta"] = {}
    try:
        df_rfp = pd.read_sql_query("SELECT title, solnum FROM rfps WHERE id=?;", conn, params=(int(rfp_id),))
        ctx["title"] = df_rfp.iloc[0]["title"] if not df_rfp.empty else ""
        ctx["solnum"] = df_rfp.iloc[0]["solnum"] if not df_rfp.empty else ""
    except Exception:
        ctx["title"] = ""; ctx["solnum"] = ""
    return ctx


def _y3_build_messages_psych(conn: "sqlite3.Connection", rfp_id: int, section_title: str, notes: str, k: int = 6, max_words: int | None = None) -> list[dict]:
    import pandas as _pd
    # Collect context via existing helper if available
    try:
        ctx = _y3_collect_ctx(conn, int(rfp_id))
    except Exception:
        ctx = {}
    # Direct pulls for dates, POCs, CLINs
    try:
        df_dates = _pd.read_sql_query("SELECT label, date_text FROM rfp_dates WHERE rfp_id=? ORDER BY id;", conn, params=(int(rfp_id),))
    except Exception:
        df_dates = _pd.DataFrame(columns=["label","date_text"])
    try:
        df_pocs = _pd.read_sql_query("SELECT name, role, email, phone FROM pocs WHERE rfp_id=? ORDER BY id;", conn, params=(int(rfp_id),))
    except Exception:
        df_pocs = _pd.DataFrame(columns=["name","role","email","phone"])
    try:
        df_clin = _pd.read_sql_query("SELECT clin, description, qty, unit FROM clines WHERE rfp_id=? ORDER BY id;", conn, params=(int(rfp_id),))
    except Exception:
        df_clin = _pd.DataFrame(columns=["clin","description","qty","unit"])
    try:
        df_org = _pd.read_sql_query("SELECT * FROM org_profile WHERE id=1;", conn, params=())
    except Exception:
        df_org = _pd.DataFrame()

    def _fmt_dates(df):
        try: return "\n".join(f"- {r['label']}: {r['date_text']}" for _, r in df.iterrows()) if not df.empty else "- n/a"
        except Exception: return "- n/a"
    def _fmt_pocs(df):
        try: return "\n".join(f"- {r['role']}: {r['name']} — {r.get('email','') or ''} {r.get('phone','') or ''}".strip() for _, r in df.iterrows()) if not df.empty else "- n/a"
        except Exception: return "- n/a"
    def _fmt_clins(df):
        try:
            cols = [c for c in ("clin","description","qty","unit") if c in df.columns]
            return "\n".join("- " + " | ".join(str(r.get(c,'')) for c in cols) for _, r in df.iterrows()) if not df.empty else "- n/a"
        except Exception: return "- n/a"

    org = df_org.iloc[0].to_dict() if isinstance(df_org, _pd.DataFrame) and not df_org.empty else {}
    org_lines = []
    for k in ("company_name","phone","email","website","cage","uei","duns","ein","address"):
        v = org.get(k) or org.get(k.upper()) or ""
        if v: org_lines.append(f"{k.replace('_',' ').title()}: {v}")
    org_block = "\n".join(org_lines) if org_lines else ""

    try:
        mw = int(max_words) if max_words else 0
    except Exception:
        mw = 0
    target_clause = f"Target 97 to 103 percent of {mw} words. Finish the last sentence even if up to 40 words over." if mw > 0 else "No word target."

    style = (
        "You are a veteran federal proposal writer with $70M+ in awards.\n"
        + _style_guide() + "\n\n"
        + _pb_psychology_framework() + "\n"
        "Output plain text only. No citations or snippet IDs.\n"
    ).strip()

    # Build user prompt
    lm_items = (ctx.get("lm") or [])[:30]
    lm_blob = "\n".join(f"- {t}" for t in lm_items) if lm_items else ""
    meta = ctx.get("meta") or {}
    meta_str = "\n".join(f"- {k}: {v}" for k,v in meta.items()) if isinstance(meta, dict) and meta else ""

    user = f"""
Draft the section: {section_title}

Intent: disarm evaluator anxiety, show certainty, map to L&M.

Notes from author:
{notes or '- none'}

Guidance:
- {target_clause}
- One idea per paragraph. Short sentences.
- Lead with mission empathy. Then logic. Then verification.
- Show reciprocity and mutual gains.
- Use concrete steps, QC, metrics, timeline, roles.

Apply this blueprint strictly:
{_section_blueprint(section_title)}

Company profile:
{org_block or '- none on file'}

RFP meta:
{meta_str or '- n/a'}

Key dates:
{_fmt_dates(df_dates)}

POCs:
{_fmt_pocs(df_pocs)}

CLINs:
{_fmt_clins(df_clin)}

Checklist items:
{lm_blob or '- none'}
""".strip()

    return [{"role":"system","content": style}, {"role":"user","content": user}]


def _y3_build_messages(conn: "sqlite3.Connection", rfp_id: int, section_title: str, notes: str, k: int = 6, max_words: int | None = None) -> list[dict]:
    import pandas as _pd
    ctx = _y3_collect_ctx(conn, int(rfp_id))
    try:
        df_dates = _pd.read_sql_query("SELECT label, date_text FROM key_dates WHERE rfp_id=? ORDER BY id;", conn, params=(int(rfp_id),))
    except Exception:
        df_dates = _pd.DataFrame(columns=["label","date_text"])
    try:
        df_pocs = _pd.read_sql_query("SELECT name, role, email, phone FROM pocs WHERE rfp_id=? ORDER BY id;", conn, params=(int(rfp_id),))
    except Exception:
        df_pocs = _pd.DataFrame(columns=["name","role","email","phone"])
    try:
        df_clin = _pd.read_sql_query("SELECT clin, description, qty, unit FROM clin_lines WHERE rfp_id=? ORDER BY id;", conn, params=(int(rfp_id),))
    except Exception:
        df_clin = _pd.DataFrame(columns=["clin","description","qty","unit"])
    try:
        _df_all = _pd.read_sql_query(
            "SELECT COALESCE(text, '') AS t FROM rfp_files_t WHERE rfp_id=? ORDER BY id;",
            conn, params=(int(rfp_id),),
        )
        if _df_all is not None and not _df_all.empty:
            full_text = "\n\n".join([str(x or "") for x in _df_all['t'].tolist()])
        else:
            raise ValueError("rfp_files_t empty")
    except Exception:
        try:
            full_text = y5_extract_from_rfp(conn, int(rfp_id)) or ""
        except Exception:
            full_text = ""
    full_text = "\n".join([ln.strip() for ln in str(full_text or "").splitlines() if ln.strip()])[:20000]
    bullets = "\n".join([f"- {it}" for it in (ctx.get("lm") or [])])
    clins = "\n".join([f"- {r['clin']}: {r['desc']}" for r in (ctx.get("clins") or [])])
    dates_str = "\n".join([f"- {r['label']}: {r['date_text']}"] for _, r in df_dates.iterrows()) if not df_dates.empty else ""
    pocs_str = "\n".join([f"- {r['name']} ({r['role']}) {r['email']} {r['phone']}"] for _, r in df_pocs.iterrows()) if not df_pocs.empty else ""
    meta = ctx.get("meta") or {}
    style = ("You are a veteran federal proposal writer with $70M+ in awards. "
             "Write in crisp, compliant federal style. Tailor to THIS RFP only. "
             "No citations. Provide procedures and concrete steps. Apply the style guide.")
    req_len = (f"Aim for {max_words} words. Do not go under {int(max_words*0.97)} words. You may exceed by one or two sentences to complete the thought.") if max_words else ""
    user = f"""
SECTION: {section_title}

{_style_guide()}

USE THIS SCAFFOLD:
- Section lead mirroring solicitation.
- Client need.
- Deliverables.
- Technical approach (step-by-step).
- Management approach (roles and RACI).
- Staffing and coverage.
- Equipment and materials.
- Timeline milestones.
- Quality control and metrics.
- Subcontractors and oversight.
- Risks and mitigations.
- Compliance crosswalk (bullets to L&M).
- Past performance tie-in.
- Price approach note.

RFP META:
{meta}

KEY DATES:
{dates_str or '- n/a'}

POCs:
{pocs_str or '- n/a'}

CLINs:
{clins or '- n/a'}

L&M ITEMS:
{bullets or '- n/a'}

FULL TEXT (truncated):
{full_text}

AUTHOR NOTES:
{notes or 'n/a'}

REQUIREMENTS:
- Write one paragraph per idea. Start a new paragraph when the topic shifts.
- No citations or 'References'.
- Map content to context.
- {req_len}
- Short sentences (<=10 words). Paragraphs <=10 sentences.
- Output only the requested section. No other sections.
- Do not include TOC or extra headings.
- Apply this blueprint:
{_section_blueprint(section_title)}
"""
    return [{"role":"system","content": style}, {"role":"user","content": user}]
def y3_stream_draft(conn: "sqlite3.Connection", rfp_id: int, section_title: str, notes: str, k: int = 6, max_words: int | None = None, temperature: float = 0.2):
    """
    Drafts a proposal section using ALL files in the selected RFP context.
    It appends the concatenated text of every parsed file to the user message.
    Falls back to on the fly extraction when rfp_files_t is not populated.
    """
    # 1 Build messages with the existing prompt scaffold
    _msgs = None
    try:
        if '_y3_build_messages_psych' in globals():
            _msgs = _y3_build_messages_psych(conn, int(rfp_id), section_title, notes, k=int(k), max_words=max_words)
        elif '_y3_build_messages' in globals():
            _msgs = _y3_build_messages(conn, int(rfp_id), section_title, notes, k=int(k), max_words=max_words)
        else:
            _msgs = [{"role":"system","content":"You are a veteran federal proposal writer."},
                     {"role":"user","content":f"Draft {section_title}. Notes: {notes}"}]
    except Exception:
        _msgs = [{"role":"system","content":"You are a veteran federal proposal writer."},
                 {"role":"user","content":f"Draft {section_title}. Notes: {notes}"}]

    # 2 Collect FULL TEXT from all files
    _full_text = ""
    try:
        import pandas as _pd
        _df_all = _pd.read_sql_query(
            "SELECT COALESCE(text,'') AS t FROM rfp_files_t WHERE rfp_id=? ORDER BY id;",
            conn, params=(int(rfp_id),),
        )
        if _df_all is not None and not _df_all.empty:
            _full_text = "\n\n".join([str(x or "") for x in _df_all['t'].tolist()])
    except Exception:
        _full_text = ""

    if not _full_text:
        # Fallback to direct extraction from rfp_files bytes
        try:
            if 'y5_extract_from_rfp' in globals():
                _full_text = y5_extract_from_rfp(conn, int(rfp_id)) or ""
        except Exception:
            _full_text = ""

    _full_text = "\n".join([ln.strip() for ln in str(_full_text or "").splitlines() if ln.strip()])
    if len(_full_text) > 20000:
        _full_text = _full_text[:20000]

    # 3 Append to the user message
    try:
        for _m in _msgs:
            if isinstance(_m, dict) and _m.get("role") == "user":
                _m["content"] = str(_m.get("content") or "") + "\n\nFULL TEXT from all files:\n" + _full_text
    except Exception:
        pass

    # 4 Call the model
    try:
        from openai import OpenAI as _OpenAI
        _client = _OpenAI()
        _model = globals().get('_resolve_model', lambda: 'gpt-4o-mini')()
        _resp = _client.chat.completions.create(
            model=_model,
            temperature=float(temperature or 0.1),
            messages=_msgs
        )
        _text = (_resp.choices[0].message.content or "").strip()
    except Exception as _e:
        _text = f"[AI unavailable] {_e}"

    # 5 Finalize using existing helper
    try:
        return _finalize_section(section_title, _text)
    except Exception:
        return _text


def _y3_top_off_precise(conn, rfp_id: int, section_title: str, notes: str, drafted: str, max_words: int | None):
    def wc(t: str) -> int:
        try:
            import re as _re_wc
            return len(_re_wc.findall(r"\b\w+\b", str(t or "")))
        except Exception:
            return len(str(t or "").split())
    try:
        mw = int(max_words) if max_words else 0
    except Exception:
        mw = 0
    if not mw or mw <= 0:
        return drafted or ""
    lower = int(max(1, round(0.97 * mw)))
    hard_cap = mw + 60
    text_acc = (drafted or "").strip()
    if wc(text_acc) >= lower:
        return text_acc
    client = get_ai()
    model_name = _resolve_model()
    attempts = 0
    while wc(text_acc) < lower and attempts < 6 and wc(text_acc) < hard_cap:
        attempts += 1
        cur = wc(text_acc)
        remaining = max(0, lower - cur)
        ask_up_to = max(80, min(200, remaining + 40))
        system = (
            "You are a veteran federal proposal writer with $70M+ in awards. "
            "Append continuation only. Do not repeat or modify prior text. "
            "Preserve bullets and paragraph breaks. Keep style unchanged."
        )
        user = (
            f"Continue the following section for RFP {int(rfp_id)}.\n"
            f"Goal: reach at least {lower} words and at most {hard_cap} words. Current count: {cur}. Append up to {ask_up_to} words.\n"
            "Rules:\n"
            "- Append continuation only. No rewrites.\n"
            "- Finish the last sentence if near the end, even if slightly over the soft cap.\n"
            "- Keep one idea per paragraph. Short sentences.\n"
            "- Include concrete steps, QC points, metrics, and verification.\n\n"
            "--- EXISTING DRAFT START ---\n"
            f"{text_acc}\n"
            "--- EXISTING DRAFT END ---\n\n"
            "Append continuation only:"
        )
        try:
            resp = client.chat.completions.create(
                model=model_name,
                messages=[{"role":"system","content": system}, {"role":"user","content": user}],
                temperature=0.15,
                stream=False,
            )
            add = ""
            if hasattr(resp, "choices") and resp.choices:
                add = getattr(resp.choices[0].message, "content", "") or ""
        except Exception:
            add = ""
        add = str(add).strip()
        if add:
            text_acc = (text_acc + "\n\n" + add).strip()
        else:
            break
    return text_acc.strip()


def _y3_top_off_precise(conn, rfp_id: int, section_title: str, notes: str, drafted: str, max_words: int | None):
    """
    Iteratively append continuation until total length ≈ target.
    Never rewrite existing text. Stop at a sentence boundary.
    """
    try:
        mw = int(max_words) if max_words else 0
    except Exception:
        mw = 0
    if not mw or mw <= 0:
        return drafted or ""

    def wc(t: str) -> int:
        try:
            import re as _re_wc
            return len(_re_wc.findall(r"\b\w+\b", str(t or "")))
        except Exception:
            return len(str(t or "").split())

    low = int(mw * (0.97 if mw >= 120 else 0.95))
    text_acc = (drafted or "").strip()
    attempts = 0

    client = get_ai()
    model_name = _resolve_model()
    while wc(text_acc) < low and attempts < 3:
        attempts += 1
        cur = wc(text_acc)
        remaining = max(0, mw - cur)
        chunk_target = 220 if mw >= 800 else 140
        ask_up_to = max(80, min(chunk_target, remaining + 60))
        system = (
            "You are a veteran federal proposal writer with $70M+ in awards. "
            "Append continuation only. Do not repeat or modify prior text. "
            "Keep style and structure unchanged."
        )
        user = (
            f"Continue the following section for RFP {int(rfp_id)}.\n"
            f"Goal: bring the TOTAL length near {mw} words. Current count: {cur}. Append up to {ask_up_to} words.\n"
            "Rules:\n"
            "- Do not modify prior text. Append continuation only.\n"
            "- Keep one paragraph per idea. Short sentences (<=12 words).\n"
            "- Add concrete steps, schedule, QC, staffing, and compliance mapping.\n"
            "- Stop after a natural sentence boundary. Slightly exceed target if needed.\n\n"
            "--- EXISTING DRAFT START ---\n"
            f"{text_acc}\n"
            "--- EXISTING DRAFT END ---\n\n"
            "Append continuation only:"
        )
        try:
            resp = client.chat.completions.create(
                model=model_name,
                messages=[{"role":"system","content": system}, {"role":"user","content": user}],
                temperature=0.15,
                stream=False,
            )
            add = ""
            if hasattr(resp, "choices") and resp.choices:
                add = getattr(resp.choices[0].message, "content", "") or ""
        except Exception:
            try:
                resp = client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[{"role":"system","content": system}, {"role":"user","content": user}],
                    temperature=0.15,
                    stream=False,
                )
                add = ""
                if hasattr(resp, "choices") and resp.choices:
                    add = getattr(resp.choices[0].message, "content", "") or ""
            except Exception:
                add = ""

        add = (add or "").strip()
        if add:
            text_acc = (text_acc + "\n\n" + add).strip()
        else:
            break

    return text_acc.strip()

    # === end Y3 length top-off helper ===

# === end Y3 ===

# === Y4: CO Review (scored compliance with citations) ===
def _y4_build_messages(conn: "sqlite3.Connection", rfp_id: int, draft_text: str, k: int = 6) -> list[dict]:
    """
    Build messages for a Contracting Officer style review.
    Output must include: Score 0–100, Strengths, Gaps, Risks, Required fixes, and short Conclusion.
    Use [C#] next to claims grounded in EVIDENCE.
    """
    # Reuse Section L/M + compliance as retrieval query
    q = f"Section L Section M compliance checklist evaluation {draft_text[:400]}"
    hits = y1_search(conn, int(rfp_id), q, k=int(k)) or []
    ev_lines = []
    for i, h in enumerate(hits, start=1):
        tag = f"[C{i}]"
        src = f"{h['file']} p.{h['page']}"
        snip = (h.get("text") or "").strip().replace("\\n"," ")
        ev_lines.append(f"{tag} {src} — {snip}")
    evidence = "\\n".join(ev_lines)

    system = (SYSTEM_CO
              + " Score the draft 0-100 for compliance and clarity."
              + " Use short, direct sentences. No fluff."
              + " Structure exactly as:\\n"
              + "Score: <0-100>\\n"
              + "Strengths: <bullets>\\n"
              + "Gaps: <bullets>\\n"
              + "Risks: <bullets>\\n"
              + "Required fixes: <bullets>\\n"
              + "Conclusion: <2-3 lines>\\n"
              + "Map factual or requirement statements to EVIDENCE using [C#]."
              + " Hard caps: max 5 bullets per list. Total output ≤ 220 words. If you exceed a cap, remove items to comply."
             )

    user = f"DRAFT TO REVIEW:\\n{draft_text or '(empty)'}\\n\\nEVIDENCE:\\n{evidence or '(no evidence found)'}"
    return [{"role":"system","content": system}, {"role":"user","content": user}]

def y4_postprocess_brevity(text: str, max_words: int = 220, max_bullets: int = 5) -> str:
    """Enforce ≤5 bullets for key sections and a global word cap."""
    if not text:
        return ""
    lines = text.splitlines()
    out = []
    sections = {"Strengths:": "Strengths:", "Gaps:": "Gaps:", "Risks:": "Risks:", "Required fixes:": "Required fixes:"}
    current = None
    bullet_count = 0
    i = 0
    while i < len(lines):
        ln = lines[i]
        ln_stripped = ln.strip()
        # detect headers
        if any(ln_stripped.lower().startswith(h.lower()) for h in sections):
            current = ln_stripped.split(":")[0].lower()
            bullet_count = 0
            out.append(ln)
            i += 1
            continue
        if re.match(r"^(Score:|Conclusion:)", ln_stripped, re.I):
            current = None
            out.append(ln)
            i += 1
            continue
        # cap bullets in the four sections
        if current in {"strengths", "gaps", "risks", "required fixes"}:
            if re.match(r"^\s*[-*\u2022]\s+", ln):
                if bullet_count < max_bullets:
                    out.append(ln)
                    bullet_count += 1
                # else drop extra bullets
            else:
                out.append(ln)
            i += 1
            continue
        out.append(ln)
        i += 1
    text2 = "\n".join(out).strip()
    words = text2.split()
    if len(words) <= max_words:
        return text2
    return " ".join(words[:max_words]).strip()

    client = get_ai()
    model_name = _resolve_model()
    try:
        resp = client.chat.completions.create(model=model_name, messages=msgs, temperature=float(temperature), stream=True)
    except Exception as _e:
        if "model_not_found" in str(_e) or "does not exist" in str(_e):
            resp = client.chat.completions.create(model="gpt-4o-mini", messages=msgs, temperature=float(temperature), stream=True)
        else:
            yield f"AI unavailable: {type(_e).__name__}: {_e}"; return
    for ch in resp:
        try:
            delta = ch.choices[0].delta
            if hasattr(delta, "content") and delta.content:
                yield delta.content
        except Exception:
            pass

def y4_ui_review(conn: "sqlite3.Connection") -> None:
# [removed]     st.caption("CO Review with score, strengths, gaps, risks, and required fixes. Citations auto-selected.")
    df_rf = pd.read_sql_query("SELECT id, title FROM rfps ORDER BY id DESC;", conn, params=())
    if df_rf is None or df_rf.empty:
        st.info("No RFPs yet. Parse & save first."); return
    rfp_id = st.selectbox("RFP context", options=df_rf["id"].tolist(),
                          format_func=lambda i: f"#{i} — {df_rf.loc[df_rf['id']==i,'title'].values[0]}",
                          key="y4_rfp_sel")
    mode = st.radio("Mode", ["Paste", "Upload", "Use RFP files"], horizontal=True, key="y5_mode")
    st.subheader("Draft to review")

    draft_text = ""
    uploaded = None
    if mode == "Paste":
        draft_text = st.text_area("Paste your draft here", value=st.session_state.get("y4_draft",""), height=260, key="y4_draft_ta")
    elif mode == "Upload":
        uploaded = st.file_uploader("Upload DOCX/PDF/TXT", type=["docx","pdf","txt"], accept_multiple_files=True, key="y5_up")
        if uploaded:
            with st.spinner("Extracting"):
                draft_text = y5_extract_from_uploads(uploaded)[:400000]
            st.text_area("Preview", value=draft_text[:20000], height=240)
    else:
        if st.button("Assemble from linked RFP files", key="y5_from_rfp"):
            with st.spinner("Collecting text from linked files"):
                draft_text = y5_extract_from_rfp(conn, int(rfp_id))[:400000]
            st.session_state["y5_rfp_text"] = draft_text
        draft_text = st.session_state.get("y5_rfp_text","")
        if draft_text:
            st.text_area("Preview", value=draft_text[:20000], height=240)

    k = y_auto_k(draft_text or "review")
    chunking = st.checkbox("Auto-chunk long text", value=True, key="y5_chunk_on")
    clicked_run_co_review = st.button("Run CO Review", type="primary", key="y4_go")
    if clicked_run_co_review:
        ok_gate, missing_gate = require_LM_minimum(conn, int(rfp_id))
        if not ok_gate:
            st.error("L/M compliance gate failed. Missing: " + ", ".join(missing_gate))
            return

        if not (draft_text or "").strip():
            st.warning("Provide input text."); return
        texts = y5_chunk_text(draft_text) if chunking else [draft_text]
        ph = st.empty()
        all_out = []
        for idx, t in enumerate(texts, start=1):
            acc = []
            ph.caption(f"Reviewing chunk {idx}/{len(texts)}")
            for tok in y4_stream_review(conn, int(rfp_id), t.strip(), k=int(k)):
                acc.append(tok)
                ph.markdown("".join(acc))
            all_out.append("".join(acc))
        final = "\n\n".join(all_out).strip()
        final = y4_postprocess_brevity(final, max_words=220, max_bullets=5)
        st.session_state["y4_last_review"] = final
        st.subheader("Combined result")
        st.markdown(final or "_no output_")

        # Sources table
        hits = y1_search(conn, int(rfp_id), f"Section L Section M compliance {draft_text[:200]}", k=int(k))
        if hits:
            import pandas as _pd
            dfh = _pd.DataFrame([{"Tag": f"[C{i+1}]", "File": h["file"], "Page": h["page"], "Score": h["score"]} for i,h in enumerate(hits)])
            st.subheader("Sources used")
            _styled_dataframe(dfh, use_container_width=True, hide_index=True)

        with st.expander("Add to Proposal Drafts", expanded=True):
            section = st.text_input("Section label", value="CO Review Notes", key="y5_sec_rev")
            if st.button("Add review to drafts", key="y5_add_rev"):
                y5_save_snippet(conn, int(rfp_id), section, final or draft_text, source="Y4 Review")
                st.success("Saved to drafts")

# === Y5: Upload Review + Drafts plumbing ===
from contextlib import closing
import io

def ensure_y5_tables(conn: "sqlite3.Connection") -> None:
    try:
        with closing(conn.cursor()) as cur:
            cur.execute(
                "CREATE TABLE IF NOT EXISTS draft_snippets("
                "id INTEGER PRIMARY KEY,"
                "rfp_id INTEGER,"
                "section TEXT,"
                "source TEXT,"
                "text TEXT,"
                "created_at TEXT DEFAULT (datetime('now'))"
                ");"
            )
            conn.commit()
    except Exception:
        pass


def _ensure_fts_docs(conn: "sqlite3.Connection") -> None:
    """
    Ensure the full-text search virtual table for large documents exists.

    This table is intended to index RFPs, proposals, and large notes so they
    can be searched quickly by title and content. Metadata columns are stored
    as UNINDEXED so searches stay focused on the text fields.
    """
    try:
        from contextlib import closing as _closing
        with _closing(conn.cursor()) as cur:
            cur.execute(
                """
                CREATE VIRTUAL TABLE IF NOT EXISTS fts_docs USING fts5(
                    id UNINDEXED,
                    ref_type UNINDEXED,
                    ref_id UNINDEXED,
                    title,
                    content
                );
                """
            )
    except Exception:
        # Some SQLite builds may not have FTS5 enabled; fail silently so
        # the rest of the app still works even without FTS.
        pass

def y5_chunk_text(text: str, target_chars: int = 9000, overlap: int = 500) -> list[str]:
    t = (text or "").strip()
    if not t:
        return []
    if len(t) <= target_chars:
        return [t]
    out: list[str] = []
    i = 0
    n = len(t)
    while i < n:
        j = min(n, i + max(2000, target_chars))
        chunk = t[i:j]
        k = chunk.rfind(". ")
        if k > len(chunk) * 0.6:
            chunk = chunk[:k+1]
            j = i + k + 1
        out.append(chunk)
        i = max(j - overlap, j)
    return out

def _extract_docx_bytes(data: bytes) -> str:
    try:
        import docx  # python-docx
        doc = docx.Document(io.BytesIO(data))
        return "\n".join(p.text for p in doc.paragraphs)
    except Exception:
        return ""

def _extract_pdf_bytes(data: bytes) -> str:
    try:
        from pypdf import PdfReader
        reader = PdfReader(io.BytesIO(data))
        return "\n\n".join((page.extract_text() or "") for page in reader.pages)
    except Exception:
        return ""

def y5_extract_from_uploads(files) -> str:
    if not files:
        return ""
    parts: list[str] = []
    for f in files:
        try:
            try:
                data = f.getbuffer().tobytes()
            except Exception:
                data = f.read()
            name = (getattr(f, "name", "") or "").lower()
            if name.endswith(".txt"):
                try:
                    parts.append(data.decode("utf-8", "ignore"))
                except Exception:
                    parts.append(str(data))
            elif name.endswith(".docx"):
                parts.append(_extract_docx_bytes(data))
            elif name.endswith(".pdf"):
                parts.append(_extract_pdf_bytes(data))
        except Exception:
            continue
    return "\n\n".join([p for p in parts if p]).strip()

def y5_extract_from_rfp(conn: "sqlite3.Connection", rfp_id: int) -> str:
    # Expect rfp_files(filename TEXT, mime TEXT, bytes BLOB, rfp_id INT)
    try:
        df = pd.read_sql_query(
            "SELECT filename, mime, bytes FROM rfp_files WHERE rfp_id=? ORDER BY id;",
            conn, params=(int(rfp_id),)
        )
    except Exception:
        df = pd.DataFrame()
    if df is None or df.empty:
        return ""
    parts: list[str] = []
    for _, r in df.iterrows():
        data = r.get("bytes")
        if data is None:
            continue
        name = (r.get("filename") or "").lower()
        if name.endswith(".txt"):
            if isinstance(data, (bytes, bytearray)):
                try:
                    parts.append(bytes(data).decode("utf-8", "ignore"))
                except Exception:
                    continue
        elif name.endswith(".docx"):
            parts.append(_extract_docx_bytes(bytes(data)))
        elif name.endswith(".pdf"):
            parts.append(_extract_pdf_bytes(bytes(data)))
    return "\n\n".join([p for p in parts if p]).strip()


def _fts_upsert_doc(conn: "sqlite3.Connection", ref_type: str, ref_id: int | str, title: str, content: str) -> None:
    """
    Insert or replace a document in the FTS index.

    We de-duplicate by (ref_type, ref_id) so each logical document only has
    one row in fts_docs.
    """
    if not content:
        return
    text = (content or "").strip()
    if not text:
        return
    _ensure_fts_docs(conn)
    from contextlib import closing as _closing
    try:
        with _closing(conn.cursor()) as cur:
            cur.execute("DELETE FROM fts_docs WHERE ref_type=? AND ref_id=?;", (ref_type, str(ref_id)))
            cur.execute(
                "INSERT INTO fts_docs(id, ref_type, ref_id, title, content) VALUES(?,?,?,?,?);",
                (f"{ref_type}:{ref_id}", ref_type, str(ref_id), title or "", text),
            )
        conn.commit()
    except Exception:
        # FTS is best-effort; never block the main workflow.
        pass


def _fts_index_rfp(conn: "sqlite3.Connection", rfp_id: int) -> None:
    """
    Build or refresh the FTS record for a given RFP from its linked files.
    """
    try:
        import pandas as _pd
        df = _pd.read_sql_query(
            "SELECT title, solnum FROM rfps WHERE id=?;",
            conn,
            params=(int(rfp_id),),
        )
    except Exception:
        df = None
    title = f"RFP #{int(rfp_id)}"
    try:
        if df is not None and not df.empty:
            _t = str(df.iloc[0].get("title") or "").strip()
            _s = str(df.iloc[0].get("solnum") or "").strip()
            if _t:
                title = _t
            if _s:
                title = f"{title} ({_s})"
    except Exception:
        pass

    try:
        content = y5_extract_from_rfp(conn, int(rfp_id)) or ""
    except Exception:
        content = ""
    if not content or not content.strip():
        return
    _fts_upsert_doc(conn, "rfp", int(rfp_id), title, content)


def _fts_index_proposal(conn: "sqlite3.Connection", proposal_id: int) -> None:
    """
    Build or refresh the FTS record for a proposal by concatenating its sections.
    """
    try:
        import pandas as _pd
        df_p = _pd.read_sql_query(
            "SELECT title FROM proposals WHERE id=?;",
            conn,
            params=(int(proposal_id),),
        )
    except Exception:
        df_p = None
    title = f"Proposal #{int(proposal_id)}"
    try:
        if df_p is not None and not df_p.empty:
            _t = str(df_p.iloc[0].get("title") or "").strip()
            if _t:
                title = _t
    except Exception:
        pass

    try:
        import pandas as _pd
        df_s = _pd.read_sql_query(
            "SELECT title, content FROM proposal_sections WHERE proposal_id=? ORDER BY ord ASC;",
            conn,
            params=(int(proposal_id),),
        )
    except Exception:
        df_s = None

    parts: list[str] = []
    if df_s is not None and not df_s.empty:
        for _, r in df_s.iterrows():
            stitle = str(r.get("title") or "")
            sbody = str(r.get("content") or "")
            if stitle.strip():
                parts.append(stitle.strip())
            if sbody.strip():
                parts.append(sbody.strip())
    content = "\n\n".join(parts).strip()
    if not content:
        return
    _fts_upsert_doc(conn, "proposal", int(proposal_id), title, content)

def y5_save_snippet(conn: "sqlite3.Connection", rfp_id: int, section: str, text: str, source: str = "Y5") -> None:
    if not (text or "").strip():
        return
    try:
        with closing(conn.cursor()) as cur:
            cur.execute(
                "INSERT INTO draft_snippets(rfp_id, section, source, text) VALUES(?,?,?,?);",
                (int(rfp_id), (section or "General").strip(), (source or "Y5").strip(), text.strip())
            )
            conn.commit()
    except Exception:
        pass
# === end Y5 ===

# === Phase 2: Dedicated finders and status chips ===
def _collect_full_text(conn: "sqlite3.Connection", rfp_id: int) -> str:
    try:
        df_bytes = pd.read_sql_query("SELECT id, filename, mime, bytes FROM rfp_files WHERE rfp_id=?;", conn, params=(int(rid),))
    except Exception:
        df = pd.DataFrame()
    parts = []
    for _, r in (df if (df is not None and not df.empty) else pd.DataFrame()).iterrows():
        b = r.get("bytes"); mime = r.get("mime") or ""
        pages = extract_text_pages(b, mime) or []
        if pages:
            pages, _ = ocr_pages_if_empty(b, mime, pages)
        parts.append("\n\n".join(pages))
    return "\n\n".join([p for p in parts if p]).strip()

def _upsert_meta(conn, rfp_id: int, key: str, value: str):
    if not value:
        return
    with closing(conn.cursor()) as cur:
        cur.execute("DELETE FROM rfp_meta WHERE rfp_id=? AND key=?;", (int(rfp_id), key))
        cur.execute("INSERT INTO rfp_meta(rfp_id, key, value) VALUES(?,?,?);", (int(rfp_id), key, value))
        conn.commit()

def find_due_date(conn: "sqlite3.Connection", rfp_id: int) -> str:
    # Check SAM facts first
    try:
        row = pd.read_sql_query("SELECT extracted_json FROM sam_versions WHERE rfp_id=? ORDER BY id DESC LIMIT 1;", conn, params=(int(rfp_id),)).iloc[0]
        facts = json.loads(row["extracted_json"] or "{}")
        if isinstance(facts, dict) and facts.get("offers_due"):
            dd = str(facts["offers_due"]).strip()
            _upsert_meta(conn, int(rfp_id), "offers_due", dd)
            return dd
    except Exception:
        pass
    # Search chunks
    try:
        dfc = pd.read_sql_query("SELECT text FROM rfp_chunks WHERE rfp_id=?;", conn, params=(int(rfp_id),))
    except Exception:
        dfc = pd.DataFrame()
    t = " ".join((dfc["text"].tolist() if not dfc.empty else []))[:500000]
    m = re.search(r"(?i)(offers|quotes|proposals)\s+due[^\d]{0,20}(\d{1,2}[/-]\d{1,2}[/-]\d{2,4}|[A-Za-z]{3,9}\s+\d{1,2},\s*\d{4})", t)
    if m:
        dd = m.group(2).strip()
        _upsert_meta(conn, int(rfp_id), "offers_due", dd); return dd
    return ""

def find_naics_setaside(conn: "sqlite3.Connection", rfp_id: int) -> dict:
    full = _collect_full_text(conn, int(rfp_id))
    naics = _extract_naics(full)
    set_aside = _extract_set_aside(full)
    if naics: _upsert_meta(conn, int(rfp_id), "naics", naics)
    if set_aside: _upsert_meta(conn, int(rfp_id), "set_aside", set_aside)
    return {"naics": naics, "set_aside": set_aside}

def find_pop(conn: "sqlite3.Connection", rfp_id: int) -> dict:
    full = _collect_full_text(conn, int(rfp_id))
    pop = extract_pop_structure(full) or {}
    for k, v in pop.items():
        _upsert_meta(conn, int(rfp_id), k, str(v))
    return pop

def find_section_M(conn: "sqlite3.Connection", rfp_id: int) -> int:
    full = _collect_full_text(conn, int(rfp_id))
    sec = extract_sections_L_M(full)
    cnt = 0
    with closing(conn.cursor()) as cur:
        if sec.get("L"):
            cur.execute("DELETE FROM rfp_sections WHERE rfp_id=? AND section='L';", (int(rfp_id),))
            cur.execute("INSERT INTO rfp_sections(rfp_id, section, content) VALUES(?,?,?);", (int(rfp_id), "L", sec["L"][:200000]))
            cnt += 1
        if sec.get("M"):
            cur.execute("DELETE FROM rfp_sections WHERE rfp_id=? AND section='M';", (int(rfp_id),))
            cur.execute("INSERT INTO rfp_sections(rfp_id, section, content) VALUES(?,?,?);", (int(rfp_id), "M", sec["M"][:200000]))
            cnt += 1
        conn.commit()
    return cnt

def find_clins_all(conn: "sqlite3.Connection", rfp_id: int) -> int:
    full = _collect_full_text(conn, int(rfp_id))
    rows = extract_clins(full)
    try:
        df = pd.read_sql_query("SELECT clin, description FROM clin_lines WHERE rfp_id=?;", conn, params=(int(rfp_id),))
        existing = set((str(r.get("clin","")), str(r.get("description",""))) for _, r in df.iterrows())
    except Exception:
        existing = set()
    added = 0
    with closing(conn.cursor()) as cur:
        for r in rows:
            key = (r.get("clin",""), r.get("description",""))
            if key in existing:
                continue
            cur.execute("INSERT INTO clin_lines(rfp_id, clin, description, qty, unit, unit_price, extended_price) VALUES (?, ?, ?, ?, ?, ?, ?);",
                        (int(rfp_id), r.get("clin"), r.get("description"), r.get("qty"), r.get("unit"), r.get("unit_price"), r.get("extended_price")))
            added += 1
        conn.commit()
    return added

def _parse_money(x):
    try:
        s = str(x or "").replace(",", "").replace("$","").strip()
        return float(s) if s else 0.0
    except Exception:
        return 0.0

def clin_totals_df(conn: "sqlite3.Connection", rfp_id: int):
    try:
        df = pd.read_sql_query("SELECT clin, description, qty, unit_price, extended_price FROM clin_lines WHERE rfp_id=? ORDER BY id;", conn, params=(int(rfp_id),))
    except Exception:
        return pd.DataFrame()
    if df is None or df.empty:
        return df
    qn = []
    up = []
    ext = []
    for _, r in df.iterrows():
        try:
            qn.append(float(str(r.get("qty","") or "0").replace(",","")))
        except Exception:
            qn.append(0.0)
        up.append(_parse_money(r.get("unit_price")))
        ext_val = _parse_money(r.get("extended_price"))
        if not ext_val and qn[-1] and up[-1]:
            ext_val = qn[-1]*up[-1]
        ext.append(ext_val)
    df["qty_num"] = qn; df["unit_price_num"] = up; df["extended_num"] = ext
    return df

def render_status_and_gaps(conn: "sqlite3.Connection") -> None:
    st.subheader("Status & Gaps")
    try:
        df_rf = pd.read_sql_query("SELECT id, title FROM rfps ORDER BY id DESC;", conn, params=())
    except Exception:
        st.info("No RFPs yet."); return
    if df_rf is None or df_rf.empty:
        st.info("No RFPs yet."); return
    rid = st.selectbox("RFP context", options=df_rf["id"].tolist(),
                       format_func=lambda i: f"#{i} — {df_rf.loc[df_rf['id']==i,'title'].values[0]}",
                       key="p2_rfp")
    c1, c2 = st.columns([1,1])
    with c1:
        if st.button("Run finders", key="p2_find"):
            with st.spinner("Scanning RFP…"):
                find_due_date(conn, int(rid))
                find_naics_setaside(conn, int(rid))
                find_pop(conn, int(rid))
                find_section_M(conn, int(rid))
                find_clins_all(conn, int(rid))
            st.success("Updated metadata and sections.")
            st.rerun()
    # Chips
    try:
        dfm = pd.read_sql_query("SELECT key, value FROM rfp_meta WHERE rfp_id=?;", conn, params=(int(rid),))
        meta = {r["key"]: r["value"] for _, r in dfm.iterrows()} if dfm is not None and not dfm.empty else {}
    except Exception:
        meta = {}
    due = meta.get("offers_due","")
    naics = meta.get("naics","")
    sa = meta.get("set_aside","")
    pop = meta.get("pop_structure","") or meta.get("ordering_period_years","")
    def chip(label, ok):
        st.markdown(f"<span style='display:inline-block;padding:4px 8px;border-radius:12px;background:{'#DCF7E3' if ok else '#FBE8E8'};margin-right:6px'>{label}: {'OK' if ok else 'Missing'}</span>", unsafe_allow_html=True)
    st.write("")
    chips = []
    chips.append(("Due", bool(due)))
    chips.append(("NAICS", bool(naics)))
    chips.append(("Set-Aside", bool(sa)))
    chips.append(("POP", bool(pop)))
    try:
        has_M = pd.read_sql_query("SELECT 1 FROM rfp_sections WHERE rfp_id=? AND section='M' LIMIT 1;", conn, params=(int(rid),)).shape[0] > 0
    except Exception:
        has_M = False
    try:
        has_CLIN = pd.read_sql_query("SELECT 1 FROM clin_lines WHERE rfp_id=? LIMIT 1;", conn, params=(int(rid),)).shape[0] > 0
    except Exception:
        has_CLIN = False
    chips.append(("Section M", has_M))
    chips.append(("CLINs", has_CLIN))
    for label, ok in chips:
        chip(label, ok)
    # Suggested questions
    gaps = [lbl for lbl, ok in chips if not ok]
    if gaps:
        st.markdown("**Suggested questions**")
        qs = []
        if "Due" in gaps: qs.append("Confirm proposals due date and exact time zone.")
        if "NAICS" in gaps: qs.append("What NAICS code applies and size standard?")
        if "Set-Aside" in gaps: qs.append("What is the set-aside and eligibility?")
        if "POP" in gaps: qs.append("What is the base and option structure for POP?")
        if "Section M" in gaps: qs.append("List Section M factors and relative weights.")
        if "CLINs" in gaps: qs.append("Extract CLIN schedule and quantities.")
        st.write("\\n".join([f"- {q}" for q in qs]))
    # CLIN totals + CSV
    st.markdown("**CLIN totals**")
    dfc = clin_totals_df(conn, int(rid))
    if dfc is None or dfc.empty:
        st.caption("No CLINs yet.")
    else:
        total = float(dfc["extended_num"].sum())
        st.caption(f"Total Extended: ${total:,.2f}")
        _styled_dataframe(dfc[["clin","description","qty","unit_price","extended_price","extended_num"]], use_container_width=True, hide_index=True)
        csvb = dfc.to_csv(index=False).encode("utf-8")
        st.warning("Checking L/M gate before export")
        st.warning("Checking L/M gate before export")
        ok_gate, missing_gate = require_LM_minimum(conn, int(rid))
        if not ok_gate:
            st.button("Export CLIN CSV", key=f"p2_clin_csv_blocked_{rid}", disabled=True, help="Blocked: " + ", ".join(missing_gate))
        else:
            st.download_button("Export CLIN CSV", data=csvb, file_name=f"rfp_{int(rid)}_clins.csv", mime="text/csv", key=f"p2_clin_csv_{rid}")

def get_db() -> sqlite3.Connection:
    # Central database connector: prefers Postgres when DATABASE_URL is set,
    # otherwise falls back to the local SQLite file. The heavy SQLite schema
    # bootstrap and PRAGMAs only run for SQLite connections.
    import os as _os
    db_url = (_os.getenv("DATABASE_URL", "") or "").strip()

    # Normalise sqlite URLs to a filesystem path for the rest of the app.
    if db_url.startswith("sqlite://"):
        path = db_url.split("sqlite:///", 1)[1] if "///" in db_url else db_url.replace("sqlite://", "", 1)
        if path:
            global DB_PATH
            DB_PATH = path
        db_url = ""  # fall through to the SQLite branch below

    # Postgres path: connect and return immediately without running SQLite-only
    # PRAGMAs, FTS helpers, or schema migrations.
    if db_url.startswith("postgres://") or db_url.startswith("postgresql://"):
        try:
            import psycopg2  # type: ignore
        except Exception as e:  # pragma: no cover - defensive
            raise RuntimeError("DATABASE_URL points at Postgres, but psycopg2 is not installed.") from e
        conn = psycopg2.connect(db_url)
        # Most of the app assumes implicit commits; autocommit keeps behaviour
        # close to sqlite3's default.
        conn.autocommit = True
        return conn

    ensure_dirs()
    conn = _db_connect(DB_PATH, check_same_thread=False)
    with closing(conn.cursor()) as cur:
        cur.execute("PRAGMA foreign_keys = ON;")

        # Core
        cur.execute("""
            CREATE TABLE IF NOT EXISTS contacts(
                id INTEGER PRIMARY KEY,
                name TEXT,
                email TEXT,
                org TEXT,
                unsubscribe_all INTEGER DEFAULT 0,
                unsubscribe_reason TEXT,
                last_unsubscribed_at TEXT,
                engagement_score REAL DEFAULT 0,
                last_engagement_at TEXT
            );
        """)
        try:
            __p_ensure_column(conn, "contacts", "phone", "TEXT")
            __p_ensure_column(conn, "contacts", "title", "TEXT")
            __p_ensure_column(conn, "contacts", "organization", "TEXT")
            __p_ensure_column(conn, "contacts", "created_at", "TEXT")
            __p_ensure_column(conn, "contacts", "public_source", "TEXT")
            __p_ensure_column(conn, "contacts", "unsubscribe_all", "INTEGER DEFAULT 0")
            __p_ensure_column(conn, "contacts", "unsubscribe_reason", "TEXT")
            __p_ensure_column(conn, "contacts", "last_unsubscribed_at", "TEXT")
            __p_ensure_column(conn, "contacts", "engagement_score", "REAL DEFAULT 0")
            __p_ensure_column(conn, "contacts", "last_engagement_at", "TEXT")
        except Exception:
            pass
        cur.execute("""
            CREATE TABLE IF NOT EXISTS deals(
                id INTEGER PRIMARY KEY,
                title TEXT NOT NULL,
                rfp_id INTEGER REFERENCES rfps(id) ON DELETE SET NULL,
                amount REAL,
                stage TEXT,
                status TEXT DEFAULT 'Open',
                close_date TEXT,
                owner TEXT,
                created_at TEXT,
                updated_at TEXT,
                score REAL DEFAULT 0,
                score_reason TEXT,
                engagement_score REAL DEFAULT 0,
                last_engagement_at TEXT,
                first_entered_stage_date TEXT,
                last_stage_change_date TEXT
            );
        """)
        # Ensure new columns exist for Deals/SAM Watch
        try:
            _migrate_deals_columns(conn)
        except Exception:
            pass
        cur.execute("""
            CREATE TABLE IF NOT EXISTS app_settings(
                key TEXT PRIMARY KEY,
                val TEXT
            );
        """)

        # Org profile (Phase F - Capability Statement)
        cur.execute("""
            CREATE TABLE IF NOT EXISTS org_profile(
                id INTEGER PRIMARY KEY CHECK (id=1),
                company_name TEXT,
                tagline TEXT,
                address TEXT,
                phone TEXT,
                email TEXT,
                website TEXT,
                uei TEXT,
                cage TEXT,
                naics TEXT,
                core_competencies TEXT,
                differentiators TEXT,
                certifications TEXT,
                past_performance TEXT,
                primary_poc TEXT
            );
        """)

        # Phase B (RFP analyzer artifacts)

        cur.execute("""
            CREATE TABLE IF NOT EXISTS notices(
                id INTEGER PRIMARY KEY,
                notice_id TEXT UNIQUE,
                solnum TEXT,
                title TEXT,
                agency TEXT,
                office TEXT,
                naics TEXT,
                set_aside TEXT,
                posted_date TEXT,
                response_date TEXT,
                award_date TEXT,
                awardee TEXT,
                award_amount REAL,
                psc TEXT,
                sam_url TEXT,
                status TEXT,
                created_at TEXT,
                updated_at TEXT
            );
        """)
        try:
            __p_ensure_column(conn, "notices", "agency", "TEXT")
            __p_ensure_column(conn, "notices", "office", "TEXT")
            __p_ensure_column(conn, "notices", "naics", "TEXT")
            __p_ensure_column(conn, "notices", "set_aside", "TEXT")
            __p_ensure_column(conn, "notices", "posted_date", "TEXT")
            __p_ensure_column(conn, "notices", "response_date", "TEXT")
            __p_ensure_column(conn, "notices", "award_date", "TEXT")
            __p_ensure_column(conn, "notices", "awardee", "TEXT")
            __p_ensure_column(conn, "notices", "award_amount", "REAL")
            __p_ensure_column(conn, "notices", "psc", "TEXT")
            __p_ensure_column(conn, "notices", "sam_url", "TEXT")
            __p_ensure_column(conn, "notices", "status", "TEXT")
            __p_ensure_column(conn, "notices", "created_at", "TEXT")
            __p_ensure_column(conn, "notices", "updated_at", "TEXT")
        except Exception:
            pass


        cur.execute("""
            CREATE TABLE IF NOT EXISTS sam_attachments(
                id INTEGER PRIMARY KEY,
                tenant_id INTEGER DEFAULT 1,
                notice_id TEXT NOT NULL,
                file_name TEXT,
                file_url TEXT,
                url TEXT,
                file_type TEXT,
                mime_type TEXT,
                size_bytes INTEGER,
                last_updated TEXT,
                status TEXT DEFAULT 'not_downloaded',
                last_error TEXT,
                local_path TEXT,
                downloaded_at TEXT,
                created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                updated_at TEXT,
                UNIQUE(tenant_id, notice_id, file_url)
            );
        """)
        cur.execute("CREATE INDEX IF NOT EXISTS idx_sam_attachments_notice ON sam_attachments(notice_id);")
        try:
            __p_ensure_column(conn, "sam_attachments", "tenant_id", "INTEGER DEFAULT 1")
            __p_ensure_column(conn, "sam_attachments", "file_url", "TEXT")
            __p_ensure_column(conn, "sam_attachments", "file_type", "TEXT")
            __p_ensure_column(conn, "sam_attachments", "last_updated", "TEXT")
            __p_ensure_column(conn, "sam_attachments", "local_path", "TEXT")
            __p_ensure_column(conn, "sam_attachments", "mime_type", "TEXT")
            __p_ensure_column(conn, "sam_attachments", "size_bytes", "INTEGER")
            __p_ensure_column(conn, "sam_attachments", "status", "TEXT")
            __p_ensure_column(conn, "sam_attachments", "last_error", "TEXT")
            __p_ensure_column(conn, "sam_attachments", "downloaded_at", "TEXT")
            __p_ensure_column(conn, "sam_attachments", "created_at", "TEXT DEFAULT (datetime('now'))")
            __p_ensure_column(conn, "sam_attachments", "updated_at", "TEXT")
        except Exception:
            pass

        cur.execute("""
            CREATE TABLE IF NOT EXISTS rfp_documents(
                id INTEGER PRIMARY KEY,
                tenant_id INTEGER DEFAULT 1,
                rfp_id INTEGER NOT NULL,
                sam_attachment_id INTEGER,
                notice_id TEXT,
                file_name TEXT,
                local_path TEXT,
                file_type TEXT,
                source TEXT DEFAULT 'sam_watch',
                url TEXT,
                mime_type TEXT,
                size_bytes INTEGER,
                status TEXT DEFAULT 'pending',
                last_error TEXT,
                stored_path TEXT,
                downloaded_at TEXT,
                created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                updated_at TEXT
            );
        """)
        cur.execute("CREATE INDEX IF NOT EXISTS idx_rfp_documents_rfp ON rfp_documents(rfp_id);")
        cur.execute("CREATE INDEX IF NOT EXISTS idx_rfp_documents_sam ON rfp_documents(sam_attachment_id);")
        try:
            __p_ensure_column(conn, "rfp_documents", "tenant_id", "INTEGER DEFAULT 1")
            __p_ensure_column(conn, "rfp_documents", "notice_id", "TEXT")
            __p_ensure_column(conn, "rfp_documents", "file_name", "TEXT")
            __p_ensure_column(conn, "rfp_documents", "local_path", "TEXT")
            __p_ensure_column(conn, "rfp_documents", "file_type", "TEXT")
            __p_ensure_column(conn, "rfp_documents", "source", "TEXT DEFAULT 'sam_watch'")
            __p_ensure_column(conn, "rfp_documents", "url", "TEXT")
            __p_ensure_column(conn, "rfp_documents", "mime_type", "TEXT")
            __p_ensure_column(conn, "rfp_documents", "size_bytes", "INTEGER")
            __p_ensure_column(conn, "rfp_documents", "status", "TEXT")
            __p_ensure_column(conn, "rfp_documents", "last_error", "TEXT")
            __p_ensure_column(conn, "rfp_documents", "stored_path", "TEXT")
            __p_ensure_column(conn, "rfp_documents", "downloaded_at", "TEXT")
            __p_ensure_column(conn, "rfp_documents", "created_at", "TEXT DEFAULT (datetime('now'))")
            __p_ensure_column(conn, "rfp_documents", "updated_at", "TEXT")
        except Exception:
            pass

        cur.execute("""
            CREATE TABLE IF NOT EXISTS notice_contacts(
                id INTEGER PRIMARY KEY,
                notice_id TEXT,
                contact_id INTEGER REFERENCES contacts(id) ON DELETE CASCADE,
                role TEXT,
                owner_user TEXT,
                created_at TEXT
            );
        """)
        cur.execute("CREATE INDEX IF NOT EXISTS idx_notice_contacts_notice ON notice_contacts(notice_id);")
        cur.execute("CREATE INDEX IF NOT EXISTS idx_notice_contacts_contact ON notice_contacts(contact_id);")
        cur.execute("CREATE UNIQUE INDEX IF NOT EXISTS idx_notice_contacts_unique ON notice_contacts(notice_id, contact_id, role);")

        cur.execute("""
            CREATE TABLE IF NOT EXISTS rfps(
                id INTEGER PRIMARY KEY,
                title TEXT,
                solnum TEXT,
                notice_id TEXT,
                sam_url TEXT,
                file_path TEXT,
                created_at TEXT,
                tenant_id INTEGER
            );
        """)
        cur.execute("""
            CREATE TABLE IF NOT EXISTS rfp_sections(
                id INTEGER PRIMARY KEY,
                rfp_id INTEGER NOT NULL REFERENCES rfps(id) ON DELETE CASCADE,
                section TEXT,
                content TEXT
            );
        """)

        cur.execute("""
            CREATE TABLE IF NOT EXISTS rfp_files(
    id INTEGER PRIMARY KEY,
    rfp_id INTEGER REFERENCES rfps(id) ON DELETE SET NULL,
    filename TEXT,
    mime TEXT,
    sha256 TEXT UNIQUE,
    pages INTEGER,
    bytes BLOB,
    created_at TEXT
);
        """)
        cur.execute("CREATE INDEX IF NOT EXISTS idx_rfp_files_rfp ON rfp_files(rfp_id);")
        cur.execute("""
            CREATE TABLE IF NOT EXISTS lm_items(
                id INTEGER PRIMARY KEY,
                rfp_id INTEGER NOT NULL REFERENCES rfps(id) ON DELETE CASCADE,
                item_text TEXT,
                is_must INTEGER DEFAULT 1,
                status TEXT DEFAULT 'Open'
            );
        """)
        cur.execute("""
            CREATE TABLE IF NOT EXISTS clin_lines(
                id INTEGER PRIMARY KEY,
                rfp_id INTEGER NOT NULL REFERENCES rfps(id) ON DELETE CASCADE,
                clin TEXT,
                description TEXT,
                qty TEXT,
                unit TEXT,
                unit_price TEXT,
                extended_price TEXT
            );
        """)
        cur.execute("""
            CREATE TABLE IF NOT EXISTS key_dates(
                id INTEGER PRIMARY KEY,
                rfp_id INTEGER NOT NULL REFERENCES rfps(id) ON DELETE CASCADE,
                label TEXT,
                date_text TEXT,
                date_iso TEXT
            );
        """)
        cur.execute("""
            CREATE TABLE IF NOT EXISTS pocs(
                id INTEGER PRIMARY KEY,
                rfp_id INTEGER NOT NULL REFERENCES rfps(id) ON DELETE CASCADE,
                name TEXT,
                role TEXT,
                email TEXT,
                phone TEXT
            );
        """)

        # Phase D (vendors + outreach)
        cur.execute("""
            CREATE TABLE IF NOT EXISTS vendors(
                id INTEGER PRIMARY KEY,
                name TEXT NOT NULL,
                cage TEXT,
                uei TEXT,
                naics TEXT,
                city TEXT,
                state TEXT,
                phone TEXT,
                email TEXT,
                website TEXT,
                notes TEXT,
                last_seen_award TEXT
            );
        """)
        cur.execute("CREATE INDEX IF NOT EXISTS idx_vendors_naics_state ON vendors(naics, state);")
        cur.execute("""
            CREATE TABLE IF NOT EXISTS vendor_contacts(
                id INTEGER PRIMARY KEY,
                vendor_id INTEGER NOT NULL REFERENCES vendors(id) ON DELETE CASCADE,
                name TEXT,
                email TEXT,
                phone TEXT,
                role TEXT
            );
        """)

        # Phase E (quotes + pricing)
        cur.execute("""
            CREATE TABLE IF NOT EXISTS quotes(
                id INTEGER PRIMARY KEY,
                rfp_id INTEGER NOT NULL REFERENCES rfps(id) ON DELETE CASCADE,
                vendor TEXT NOT NULL,
                received_date TEXT,
                notes TEXT
            );
        """)
        cur.execute("""
            CREATE TABLE IF NOT EXISTS quote_lines(
                id INTEGER PRIMARY KEY,
                quote_id INTEGER NOT NULL REFERENCES quotes(id) ON DELETE CASCADE,
                clin TEXT,
                description TEXT,
                qty REAL,
                unit_price REAL,
                extended_price REAL
            );
        """)
        cur.execute("CREATE INDEX IF NOT EXISTS idx_quote_lines ON quote_lines(quote_id, clin);")

        cur.execute("""
            CREATE TABLE IF NOT EXISTS pricing_scenarios(
                id INTEGER PRIMARY KEY,
                rfp_id INTEGER NOT NULL REFERENCES rfps(id) ON DELETE CASCADE,
                name TEXT NOT NULL,
                overhead_pct REAL DEFAULT 0.0,
                gna_pct REAL DEFAULT 0.0,
                fee_pct REAL DEFAULT 0.0,
                contingency_pct REAL DEFAULT 0.0,
                created_at TEXT
            );
        """)
        cur.execute("""
            CREATE TABLE IF NOT EXISTS pricing_labor(
                id INTEGER PRIMARY KEY,
                scenario_id INTEGER NOT NULL REFERENCES pricing_scenarios(id) ON DELETE CASCADE,
                labor_cat TEXT,
                hours REAL,
                rate REAL,
                fringe_pct REAL DEFAULT 0.0
            );
        """)
        cur.execute("""
            CREATE TABLE IF NOT EXISTS pricing_other(
                id INTEGER PRIMARY KEY,
                scenario_id INTEGER NOT NULL REFERENCES pricing_scenarios(id) ON DELETE CASCADE,
                label TEXT,
                cost REAL
            );
        """)

        cur.execute("""
            CREATE TABLE IF NOT EXISTS past_perf(
                id INTEGER PRIMARY KEY,
                project_title TEXT NOT NULL,
                customer TEXT,
                contract_no TEXT,
                naics TEXT,
                role TEXT,
                pop_start TEXT,
                pop_end TEXT,
                value NUMERIC,
                scope TEXT,
                results TEXT,
                cpars_rating TEXT,
                contact_name TEXT,
                contact_email TEXT,
                contact_phone TEXT,
                keywords TEXT,
                notes TEXT
            );
        """)

        # Phase H (White Paper Builder)
        cur.execute("""
            CREATE TABLE IF NOT EXISTS white_templates(
                id INTEGER PRIMARY KEY,
                name TEXT NOT NULL,
                description TEXT,
                created_at TEXT
            );
        """)
        cur.execute("""
            CREATE TABLE IF NOT EXISTS white_template_sections(
                id INTEGER PRIMARY KEY,
                template_id INTEGER NOT NULL REFERENCES white_templates(id) ON DELETE CASCADE,
                position INTEGER NOT NULL,
                title TEXT,
                body TEXT
            );
        """)
        cur.execute("""
            CREATE TABLE IF NOT EXISTS white_papers(
                id INTEGER PRIMARY KEY,
                title TEXT NOT NULL,
                subtitle TEXT,
                rfp_id INTEGER REFERENCES rfps(id) ON DELETE SET NULL,
                created_at TEXT,
                updated_at TEXT
            );
        """)
        cur.execute("""
            CREATE TABLE IF NOT EXISTS white_paper_sections(
                id INTEGER PRIMARY KEY,
                paper_id INTEGER NOT NULL REFERENCES white_papers(id) ON DELETE CASCADE,
                position INTEGER NOT NULL,
                title TEXT,
                body TEXT,
                image_path TEXT
            );
        """)

        cur.execute("""
            CREATE TABLE IF NOT EXISTS activities(
                id INTEGER PRIMARY KEY,
                ts TEXT,
                type TEXT,
                subject TEXT,
                notes TEXT,
                deal_id INTEGER REFERENCES deals(id) ON DELETE SET NULL,
                contact_id INTEGER REFERENCES contacts(id) ON DELETE SET NULL
            );
        """)
        cur.execute("CREATE INDEX IF NOT EXISTS idx_activities_ts ON activities(ts);")
        cur.execute("CREATE INDEX IF NOT EXISTS idx_activities_rel ON activities(deal_id, contact_id);")

        # CRM activity log (normalized)
        cur.execute("""
            CREATE TABLE IF NOT EXISTS activity(
                id INTEGER PRIMARY KEY,
                contact_id INTEGER REFERENCES contacts(id) ON DELETE SET NULL,
                deal_id INTEGER REFERENCES deals(id) ON DELETE SET NULL,
                type TEXT,
                status TEXT,
                created_at TEXT,
                meta_json TEXT
            );
        """)
        cur.execute("CREATE INDEX IF NOT EXISTS idx_activity_contact ON activity(contact_id);")
        cur.execute("CREATE INDEX IF NOT EXISTS idx_activity_deal ON activity(deal_id);")
        cur.execute("CREATE INDEX IF NOT EXISTS idx_activity_type ON activity(type);")
        cur.execute("CREATE INDEX IF NOT EXISTS idx_activity_created ON activity(created_at);")
        cur.execute("""
            CREATE TABLE IF NOT EXISTS tasks(
                id INTEGER PRIMARY KEY,
                owner_user TEXT,
                title TEXT NOT NULL,
                due_date TEXT,
                status TEXT DEFAULT 'Open',
                priority TEXT DEFAULT 'Normal',
                deal_id INTEGER REFERENCES deals(id) ON DELETE SET NULL,
                contact_id INTEGER REFERENCES contacts(id) ON DELETE SET NULL,
                created_at TEXT,
                completed_at TEXT
            );
        """)
        try:
            cur.execute("ALTER TABLE tasks ADD COLUMN owner_user TEXT")
        except Exception:
            pass
        cur.execute("CREATE INDEX IF NOT EXISTS idx_tasks_due ON tasks(due_date, status);")
        cur.execute("CREATE INDEX IF NOT EXISTS idx_tasks_owner ON tasks(owner_user);")
        try:
            cur.execute("DROP VIEW IF EXISTS tasks_t;")
            cur.execute("""
                CREATE VIEW IF NOT EXISTS tasks_t AS
                SELECT
                    id,
                    title,
                    due_date,
                    status,
                    priority,
                    deal_id,
                    contact_id,
                    created_at,
                    completed_at,
                    COALESCE(owner_user, '') AS owner_user
                FROM tasks;
            """)
        except Exception:
            # If the database is locked or view recreation fails, do not block app startup.
            pass

        cur.execute("""
            CREATE TABLE IF NOT EXISTS deal_stage_log(
                id INTEGER PRIMARY KEY,
                deal_id INTEGER NOT NULL REFERENCES deals(id) ON DELETE CASCADE,
                stage TEXT NOT NULL,
                changed_at TEXT
            );
        """)

        # Phase J (File Manager)
        cur.execute("""
            CREATE TABLE IF NOT EXISTS files(
                id INTEGER PRIMARY KEY,
                owner_type TEXT,            -- 'RFP' | 'Deal' | 'Vendor' | 'Other'
                owner_id INTEGER,           -- nullable when owner_type='Other'
                filename TEXT,
                path TEXT,
                size INTEGER,
                mime TEXT,
                tags TEXT,
                notes TEXT,
                uploaded_at TEXT
            );
        """)
        cur.execute("CREATE INDEX IF NOT EXISTS idx_files_owner ON files(owner_type, owner_id);")
        cur.execute("CREATE INDEX IF NOT EXISTS idx_files_tags ON files(tags);")

        # Phase L (RFQ Pack)
        cur.execute("""
            CREATE TABLE IF NOT EXISTS rfq_packs(
                id INTEGER PRIMARY KEY,
                rfp_id INTEGER REFERENCES rfps(id) ON DELETE SET NULL,
                deal_id INTEGER REFERENCES deals(id) ON DELETE SET NULL,
                title TEXT NOT NULL,
                instructions TEXT,
                due_date TEXT,
                created_at TEXT,
                updated_at TEXT
            );
        """)
        cur.execute("CREATE INDEX IF NOT EXISTS idx_rfq_packs_ctx ON rfq_packs(rfp_id, deal_id);");
        cur.execute("""
            CREATE TABLE IF NOT EXISTS rfq_lines(
                id INTEGER PRIMARY KEY,
                pack_id INTEGER NOT NULL REFERENCES rfq_packs(id) ON DELETE CASCADE,
                clin_code TEXT,
                description TEXT,
                qty REAL,
                unit TEXT,
                naics TEXT,
                psc TEXT
            );
        """)
        cur.execute("CREATE INDEX IF NOT EXISTS idx_rfq_lines_pack ON rfq_lines(pack_id);");
        cur.execute("""
            CREATE TABLE IF NOT EXISTS rfq_vendors(
                id INTEGER PRIMARY KEY,
                pack_id INTEGER NOT NULL REFERENCES rfq_packs(id) ON DELETE CASCADE,
                vendor_id INTEGER NOT NULL REFERENCES vendors(id) ON DELETE CASCADE
            );
        """)
        cur.execute("CREATE UNIQUE INDEX IF NOT EXISTS idx_rfq_vendors_unique ON rfq_vendors(pack_id, vendor_id);");
        cur.execute("""
            CREATE TABLE IF NOT EXISTS rfq_attach(
                id INTEGER PRIMARY KEY,
                pack_id INTEGER NOT NULL REFERENCES rfq_packs(id) ON DELETE CASCADE,
                file_id INTEGER REFERENCES files(id) ON DELETE SET NULL,
                name TEXT,
                path TEXT
            );
        """)
        cur.execute("CREATE INDEX IF NOT EXISTS idx_rfq_attach_pack ON rfq_attach(pack_id);");
        # Fast RFQ one-page quote tables
        cur.execute("""
            CREATE TABLE IF NOT EXISTS fast_rfq_quotes(
                id INTEGER PRIMARY KEY,
                title TEXT NOT NULL,
                customer TEXT,
                customer_ref TEXT,
                solnum TEXT,
                notice_id INTEGER REFERENCES notices(id) ON DELETE SET NULL,
                deal_id INTEGER REFERENCES deals(id) ON DELETE SET NULL,
                rfp_id INTEGER REFERENCES rfps(id) ON DELETE SET NULL,
                contact_name TEXT,
                contact_email TEXT,
                contact_phone TEXT,
                due_date TEXT,
                valid_through TEXT,
                terms TEXT,
                notes TEXT,
                created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                updated_at TEXT DEFAULT CURRENT_TIMESTAMP
            );
        """)
        cur.execute("""
            CREATE TABLE IF NOT EXISTS fast_rfq_lines(
                id INTEGER PRIMARY KEY,
                quote_id INTEGER NOT NULL REFERENCES fast_rfq_quotes(id) ON DELETE CASCADE,
                clin_label TEXT,
                description TEXT,
                qty REAL,
                unit TEXT,
                unit_price REAL,
                extended_price REAL
            );
        """)
        cur.execute("CREATE INDEX IF NOT EXISTS idx_fast_rfq_lines_quote ON fast_rfq_lines(quote_id);");


        # Phase M (Tenancy 1)

        # Robust multi-tenant migration. Adds tenant_id to core tables, creates scoped views and insert triggers.

        def __p_table_exists(cur, name: str) -> bool:

            try:

                cur.execute("SELECT 1 FROM sqlite_master WHERE type IN ('table','view') AND name=?;", (name,))

                return cur.fetchone() is not None

            except Exception:

                return False

        

        def __p_get_cols(cur, name: str):

            cur.execute(f"PRAGMA table_info({name});")

            return [r[1] for r in cur.fetchall()]

        

        def __p_add_tenant(cur, table: str):

            try:

                cols = __p_get_cols(cur, table)

                if "tenant_id" not in cols:

                    cur.execute(f"ALTER TABLE {table} ADD COLUMN tenant_id INTEGER;")

            except Exception:

                pass

        

        def __p_add_owner_user(cur, table: str):
            """Ensure an owner_user TEXT column exists on the given table."""
            try:
                cols = __p_get_cols(cur, table)
                if "owner_user" not in cols:
                    cur.execute(f"ALTER TABLE {table} ADD COLUMN owner_user TEXT;")
            except Exception:
                # If ALTER fails, continue without breaking startup
                pass

        def __p_add_visibility(cur, table: str):
            """Ensure a visibility TEXT column exists on the given table."""
            try:
                cols = __p_get_cols(cur, table)
                if "visibility" not in cols:
                    cur.execute(f"ALTER TABLE {table} ADD COLUMN visibility TEXT DEFAULT 'private';")
            except Exception:
                # If ALTER fails, continue without breaking startup
                pass

        def __p_create_scoped_view(cur, table: str):

            try:

                cols = __p_get_cols(cur, table)

                sel_cols = ", ".join(cols)

                view = f"{table}_t"

                cur.execute(f"DROP VIEW IF EXISTS {view};")

                cur.execute(f"CREATE VIEW IF NOT EXISTS {view} AS SELECT {sel_cols} FROM {table} WHERE tenant_id=(SELECT ctid FROM current_tenant WHERE id=1);")

            except Exception:

                pass

        

        def __p_create_insert_trigger(cur, table: str):

            try:

                trg = f"{table}_ai_tenant"

                cur.execute(f"""

                    CREATE TRIGGER IF NOT EXISTS {trg}

                    AFTER INSERT ON {table}

                    WHEN NEW.tenant_id IS NULL

                    BEGIN

                        UPDATE {table} SET tenant_id=(SELECT ctid FROM current_tenant WHERE id=1) WHERE rowid=NEW.rowid;

                    END;

                """)

            except Exception:

                pass

        

        # Ensure tenants and current pointer exist
        try:
            # If an old view named 'tenants' exists from a previous version, drop it so we can
            # create the real tenants table. This avoids sqlite3.OperationalError like
            # "cannot modify tenants because it is a view" at startup.
            cur.execute("SELECT type FROM sqlite_master WHERE name='tenants';")
            row = cur.fetchone()
            if row and row[0] != 'table':
                cur.execute("DROP VIEW IF EXISTS tenants;")

            cur.execute("""
                CREATE TABLE IF NOT EXISTS tenants(
                    id INTEGER PRIMARY KEY,
                    name TEXT UNIQUE NOT NULL,
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP
                );
            """)

            cur.execute("""
                CREATE TABLE IF NOT EXISTS current_tenant(
                    id INTEGER PRIMARY KEY CHECK(id=1),
                    ctid INTEGER
                );
            """)

            # Best-effort tenant bootstrap; tolerate concurrent writers.
            try:
                cur.execute("INSERT OR IGNORE INTO tenants(id, name) VALUES(1, 'ELA');")
                cur.execute("UPDATE tenants SET name='ELA' WHERE id=1 AND (name IS NULL OR name='Default');")
                cur.execute("INSERT OR IGNORE INTO current_tenant(id, ctid) VALUES(1, 1);")
            except Exception as _ex:
                # If the database is locked, skip without treating as startup failure.
                if "database is locked" not in str(_ex).lower():
                    raise
        except Exception as _ex2:
            try:
                _msg = str(_ex2).lower()
                if "database is locked" in _msg:
                    # Skip noisy stacktrace when another writer holds the DB.
                    logger.warning("Tenant bootstrap skipped because database is locked; will retry later")
                else:
                    logger.exception("Tenant bootstrap failed")
            except Exception:
                pass


        

        # Discover existing base tables to scope

        cur.execute("SELECT name FROM sqlite_master WHERE type='table';")

        existing_tables = set(r[0] for r in cur.fetchall())

        

        # Recommended core tables

        candidate_tables = [

            "rfps","rfp_meta","rfp_files","lm_items","lm_meta","clin_lines","key_dates","pocs",

            "quotes","quote_lines","quote_totals",

            "deals","deal_stage_log","activities","tasks",

            "contacts","vendors","files",

            "rfq_packs","rfq_lines","rfq_vendors","rfq_attach","fast_rfq_quotes","fast_rfq_lines",

            "saved_searches","alerts","sam_versions","sam_extracts","notices",

            "rtm_requirements","rtm_links",

            "pricing_scenarios","pricing_rates","pricing_other","past_perf","org_profile"

        ]

        

        core_tables = [t for t in candidate_tables if t in existing_tables]

        

        # Add tenant_id where missing and backfill NULLs to current tenant

        # Core tenant-scoped tables
        tenant_tables = list(core_tables)

        # Extra tables that should also be tenant-scoped even if they were
        # not part of the original recommendation set.
        extra_tenant_tables = [
            "outreach_blasts",
            "outreach_log",
            "outreach_sequences",
            "outreach_steps",
            "outreach_templates",
            "proposals",
        ]
        for t in extra_tenant_tables:
            if t in existing_tables and t not in tenant_tables:
                tenant_tables.append(t)

        # Apply tenant_id column and backfill for all tenant-scoped tables
        for t in tenant_tables:
            __p_add_tenant(cur, t)
            try:
                cur.execute(
                    f"UPDATE {t} "
                    "SET tenant_id=(SELECT ctid FROM current_tenant WHERE id=1) "
                    "WHERE tenant_id IS NULL;"
                )
            except Exception:
                pass

        # User-owned tables get an owner_user column for per-user isolation
        user_owned_tables = [
            "deals",
            "contacts",
            "notice_contacts",
            "tasks",
            "activities",
            "outreach_log",
            "outreach_blasts",
            "saved_searches",
            "outreach_sequences",
            "outreach_steps",
            "rfps",
        ]
        for t in user_owned_tables:
            if t in existing_tables:
                __p_add_owner_user(cur, t)
                try:
                    cur.execute(
                        f"UPDATE {t} "
                        "SET owner_user='Quincy' "
                        "WHERE owner_user IS NULL OR owner_user='';"
                    )
                except Exception:
                    pass

        # Content-oriented tables that may later be shared get a visibility flag
        content_tables = [
            "rfps",
            "proposals",
            "outreach_templates",
            "vendors",
        ]
        for t in content_tables:
            if t in existing_tables:
                __p_add_visibility(cur, t)
                try:
                    cur.execute(
                        f"UPDATE {t} "
                        "SET visibility='private' "
                        "WHERE visibility IS NULL OR visibility='';"
                    )
                except Exception:
                    pass

        # Create scoped views and insert triggers
        for t in tenant_tables:
            __p_create_scoped_view(cur, t)
            __p_create_insert_trigger(cur, t)

        # Phase N (Persist): Pragmas
        try:
            cur.execute("PRAGMA journal_mode=WAL;")
            cur.execute("PRAGMA synchronous=NORMAL;")
            cur.execute("PRAGMA foreign_keys=ON;")
            cur.execute("PRAGMA busy_timeout=5000;")
        except Exception:
            pass

        # RTM (Requirements Traceability Matrix)
        cur.execute("""
            CREATE TABLE IF NOT EXISTS rtm_requirements(
                id INTEGER PRIMARY KEY,
                rfp_id INTEGER NOT NULL REFERENCES rfps(id) ON DELETE CASCADE,
                req_key TEXT,
                source_type TEXT,       -- 'L/M' | 'SOW' | 'CLIN' | 'Other'
                source_file TEXT,
                page INTEGER,
                text TEXT NOT NULL,
                status TEXT DEFAULT 'Open', -- Open | Covered | N/A
                created_at TEXT,
                updated_at TEXT
            );
        """)
        cur.execute("""
            CREATE TABLE IF NOT EXISTS rtm_links(
                id INTEGER PRIMARY KEY,
                rtm_id INTEGER NOT NULL REFERENCES rtm_requirements(id) ON DELETE CASCADE,
                link_type TEXT,         -- 'Proposal' | 'Pricing' | 'Subcontractor' | 'Clause' | 'Other'
                target TEXT,            -- pointer to evidence (file name+page or section id)
                note TEXT,
                created_at TEXT,
                updated_at TEXT
            );
        """)
        cur.execute("CREATE INDEX IF NOT EXISTS idx_rtm_rfp ON rtm_requirements(rfp_id);")
        cur.execute("CREATE INDEX IF NOT EXISTS idx_rtm_links ON rtm_links(rtm_id);")

        # SAM Amendment snapshots
        cur.execute("""
            CREATE TABLE IF NOT EXISTS sam_versions(
                id INTEGER PRIMARY KEY,
                rfp_id INTEGER NOT NULL REFERENCES rfps(id) ON DELETE CASCADE,
                url TEXT,
                sha256 TEXT,
                extracted_json TEXT,
                created_at TEXT
            );
        """ )
        cur.execute("""
            CREATE TABLE IF NOT EXISTS sam_extracts(
                id INTEGER PRIMARY KEY,
                sam_version_id INTEGER NOT NULL REFERENCES sam_versions(id) ON DELETE CASCADE,
                key TEXT,
                value TEXT
            );
        """)
# Schema version for migrations
        try:
            # If an old view named 'schema_version' exists from a previous build, drop it
            # so we can create the real schema_version table. This avoids errors like
            # "cannot modify schema_version because it is a view".
            cur.execute("SELECT type FROM sqlite_master WHERE name='schema_version';")
            _sv_row = cur.fetchone()
            if _sv_row and _sv_row[0] != 'table':
                cur.execute("DROP VIEW IF EXISTS schema_version;")
            cur.execute("""
                CREATE TABLE IF NOT EXISTS schema_version(
                    id INTEGER PRIMARY KEY CHECK(id=1),
                    ver INTEGER
                );
            """)
            # Best-effort schema_version bootstrap; tolerate concurrent writers.
            try:
                cur.execute("INSERT OR IGNORE INTO schema_version(id, ver) VALUES(1, 0);")
                conn.commit()
            except Exception as _ex:
                if "database is locked" not in str(_ex).lower():
                    raise
        except Exception as _ex2:
            try:
                _msg = str(_ex2).lower()
                if "database is locked" in _msg:
                    logger.warning("Schema version bootstrap skipped because database is locked; will retry later")
                else:
                    logger.exception("Schema version bootstrap failed")
            except Exception:
                pass
    try:
        ensure_y5_tables(conn)
    except Exception:
        pass
    try:
        _ensure_fts_docs(conn)
    except Exception:
        pass
    try:
        migrate(conn)
    except Exception:
        pass
    return conn

# Compatibility shim for vendors_t
try:
    with conn:
        conn.execute("CREATE TABLE IF NOT EXISTS vendors (id INTEGER PRIMARY KEY, name TEXT, cage TEXT, uei TEXT, naics TEXT, city TEXT, state TEXT, phone TEXT, email TEXT, website TEXT, notes TEXT, place_id TEXT UNIQUE)")
        conn.execute("CREATE VIEW IF NOT EXISTS vendors_t AS SELECT id, name, email, phone, city, state, naics, cage, uei, website, notes FROM vendors")
except Exception:
    pass

def _file_hash() -> str:
    try:
        import hashlib
        with open(__file__, 'rb') as f:
            return hashlib.sha256(f.read()).hexdigest()[:12]
    except Exception:
        return "unknown"

def save_uploaded_file(uploaded_file, subdir: str = "") -> Optional[str]:
    if not uploaded_file:
        return None
    base_dir = UPLOADS_DIR if not subdir else os.path.join(UPLOADS_DIR, subdir)
    os.makedirs(base_dir, exist_ok=True)
    path = os.path.join(base_dir, uploaded_file.name)
    with open(path, "wb") as f:
        f.write(uploaded_file.getbuffer())
    return path

# -------------------- SAM Watch helpers (Phase A) --------------------
def get_sam_api_key() -> Optional[str]:
    key = st.session_state.get("temp_sam_key")
    if key:
        return key
    try:
        key = st.secrets.get("sam", {}).get("api_key")
        if key:
            return key
    except Exception:
        pass
    try:
        key = st.secrets.get("SAM_API_KEY")
        if key:
            return key
    except Exception:
        pass
    return os.getenv("SAM_API_KEY")

@st.cache_data(show_spinner=False, ttl=300)

def _sam_http_search_uncached(params: Dict[str, Any]) -> Dict[str, Any]:
    """Low-level SAM.gov HTTP search without any caching.

    This function is pure with respect to its input dictionary (it copies
    before mutating) so that higher-level caches can safely key on params.
    """
    p = dict(params) if isinstance(params, dict) else {}
    api_key = p.get("api_key")
    if not api_key:
        return {"totalRecords": 0, "records": [], "error": "Missing API key"}

    limit = int(p.get("limit", 100))
    max_pages = int(p.pop("_max_pages", 3))
    p["limit"] = min(max(1, limit), 1000)

    all_records: List[Dict[str, Any]] = []
    offset = int(p.get("offset", 0))

    for _ in range(max_pages):
        q = {**p, "offset": offset}
        try:
            resp = requests.get(SAM_ENDPOINT, params=q, headers={"X-Api-Key": api_key}, timeout=30)
        except Exception as ex:
            return {"totalRecords": 0, "records": [], "error": f"Request error: {ex}"}

        if resp.status_code != 200:
            try:
                j = resp.json() or {}
                msg = j.get("message") or j.get("error") or str(j)
            except Exception:
                msg = resp.text or f"HTTP {resp.status_code}"
            return {"totalRecords": 0, "records": [], "error": msg}

        try:
            data = resp.json() or {}
        except Exception as ex:
            return {"totalRecords": 0, "records": [], "error": f"JSON decode error: {ex}"}

        records = data.get("opportunitiesData", data.get("data", []))
        if not isinstance(records, list):
            records = []
        all_records.extend(records)

        total = data.get("totalRecords", len(all_records))
        if len(all_records) >= total:
            break
        offset += p["limit"]

    return {"totalRecords": len(all_records), "records": all_records, "error": None}


# Streamlit-aware caching wrapper so repeated SAM searches with the same parameters
# (API key, date range, NAICS, keywords, pages, etc.) reuse the same HTTP results
# within a short TTL. When Streamlit is unavailable (e.g., worker-only context),
# this falls back to the uncached HTTP helper.
try:
    import streamlit as _st_sam_cache  # type: ignore

    @_st_sam_cache.cache_data(show_spinner=False, ttl=600)
    def _sam_http_search_cached(params: Dict[str, Any]) -> Dict[str, Any]:
        return _sam_http_search_uncached(params)
except Exception:
    def _sam_http_search_cached(params: Dict[str, Any]) -> Dict[str, Any]:
        return _sam_http_search_uncached(params)


def sam_search_cached(params: Dict[str, Any]) -> Dict[str, Any]:
    """High-level SAM search entry point used by jobs.

    Keeps the existing public name but delegates to the cached HTTP helper.
    """
    return _sam_http_search_cached(dict(params) if isinstance(params, dict) else {})
def sam_persist_notices(conn: "sqlite3.Connection", records: List[Dict[str, Any]]) -> None:
    """
    Best-effort upsert of SAM notices into the local `notices` table.

    Stores richer metadata so that SAM Watch, RFP Analyzer, and the Deals
    pipeline can reuse prior search results without re-calling the API.
    """
    if not records:
        return
    try:
        import sqlite3  # noqa: F401
    except Exception:
        pass
    try:
        with conn:
            cur = conn.cursor()
            for r in records:
                # Core identifiers
                notice_id = (r.get("noticeId") or r.get("noticeid") or r.get("id") or "").strip()
                solnum = (r.get("solicitationNumber") or r.get("solnum") or "").strip()
                title = (r.get("title") or "").strip()

                # Dates
                posted_date = (r.get("postedDate") or "").strip()
                response_date = (
                    r.get("reponseDeadLine")
                    or r.get("responseDeadline")
                    or ""
                )
                response_date = (response_date or "").strip()
                award_date = (r.get("awardDate") or "").strip()

                # Organization path → agency / office split
                org_path = (r.get("fullParentPathName") or r.get("organizationName") or "").strip()
                agency = ""
                office = ""
                if org_path:
                    parts = [p.strip() for p in re.split(r"[>/]", org_path) if p.strip()]
                    if parts:
                        agency = parts[0]
                        if len(parts) > 1:
                            office = parts[-1]
                    else:
                        agency = org_path

                # Codes and statuses
                naics = (r.get("naicsCode") or r.get("ncode") or "").strip()
                set_aside = (r.get("setAside") or "").strip()
                psc = (r.get("classificationCode") or r.get("ccode") or "").strip()
                status = (
                    r.get("noticeType")
                    or r.get("type")
                    or r.get("baseType")
                    or ""
                ).strip()

                # Award info (best effort; many open notices will not have these)
                awardee = (r.get("awardee") or r.get("awardeeName") or "").strip()
                raw_award = (
                    r.get("awardAmount")
                    or r.get("awardAmountDecimal")
                    or r.get("awardAmountNumeric")
                    or r.get("awardAmountString")
                )
                award_amount = None
                if raw_award not in (None, ""):
                    try:
                        award_amount = float(str(raw_award).replace(",", "").replace("$", "").strip())
                    except Exception:
                        award_amount = None

                sam_url = ""
                if notice_id:
                    sam_url = f"https://sam.gov/opp/{notice_id}/view"

                cur.execute(
                    """
                    INSERT INTO notices(
                        notice_id, solnum, title,
                        agency, office,
                        naics, set_aside,
                        posted_date, response_date,
                        award_date, awardee, award_amount,
                        psc, sam_url, status,
                        created_at, updated_at
                    )
                    VALUES(?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,datetime('now'),datetime('now'))
                    ON CONFLICT(notice_id) DO UPDATE SET
                        solnum=excluded.solnum,
                        title=excluded.title,
                        agency=excluded.agency,
                        office=excluded.office,
                        naics=excluded.naics,
                        set_aside=excluded.set_aside,
                        posted_date=excluded.posted_date,
                        response_date=excluded.response_date,
                        award_date=excluded.award_date,
                        awardee=excluded.awardee,
                        award_amount=excluded.award_amount,
                        psc=excluded.psc,
                        sam_url=excluded.sam_url,
                        status=excluded.status,
                        updated_at=excluded.updated_at;
                    """,
                    (
                        notice_id or None,
                        solnum,
                        title,
                        agency,
                        office,
                        naics,
                        set_aside,
                        posted_date,
                        response_date,
                        award_date,
                        awardee,
                        award_amount,
                        psc,
                        sam_url,
                        status,
                    ),
                )
            try:
                cur.close()
            except Exception:
                pass
    except Exception:
        # Best-effort only: do not block UI or search if persistence fails.
        return


def flatten_records(records: List[Dict[str, Any]]) -> pd.DataFrame:
    rows = []
    for r in records:
        title = r.get("title") or ""
        solnum = r.get("solicitationNumber") or r.get("solnum") or ""
        posted = r.get("postedDate") or ""
        ptype = r.get("type") or r.get("baseType") or ""
        set_aside = r.get("setAside") or ""
        set_aside_code = r.get("setAsideCode") or ""
        naics = r.get("naicsCode") or r.get("ncode") or ""
        psc = r.get("classificationCode") or r.get("ccode") or ""
        deadline = r.get("reponseDeadLine") or r.get("responseDeadline") or ""
        org_path = r.get("fullParentPathName") or r.get("organizationName") or ""
        notice_id = r.get("noticeId") or r.get("noticeid") or r.get("id") or ""
        sam_url = f"https://sam.gov/opp/{notice_id}/view" if notice_id else ""
        rows.append(
            {
                "Title": title,
                "Solicitation": solnum,
                "Type": ptype,
                "Posted": posted,
                "Response Due": deadline,
                "Set-Aside": set_aside,
                "Set-Aside Code": set_aside_code,
                "NAICS": naics,
                "PSC": psc,
                "Agency Path": org_path,
                "Notice ID": notice_id,
                "SAM Link": sam_url,
            }
        )
    df = pd.DataFrame(rows)
    wanted = [
        "Title", "Solicitation", "Type", "Posted", "Response Due",
        "Set-Aside", "Set-Aside Code", "NAICS", "PSC",
        "Agency Path", "Notice ID", "SAM Link",
    ]
    return df[wanted] if not df.empty else df

def sam_try_fetch_attachments(notice_id: str) -> List[Tuple[str, bytes]]:
    """Best-effort attempt to fetch attachments for a SAM notice.
    Returns list of (filename, bytes). Falls back to saving the notice description HTML
    when public attachment download isn't available.
    """
    import io, zipfile, os
    files: List[Tuple[str, bytes]] = []
    if not notice_id:
        return files

    # Attempt 1: Use Opportunity Management API 'download all' if system creds are present.
    sys_key = None
    sys_auth = None
    try:
        sys_key = (st.secrets.get("sam", {}).get("system_api_key")
                   or st.secrets.get("SAM_SYSTEM_API_KEY")
                   or os.getenv("SAM_SYSTEM_API_KEY"))
        sys_auth = (st.secrets.get("sam", {}).get("system_auth")
                    or st.secrets.get("SAM_SYSTEM_AUTH")
                    or os.getenv("SAM_SYSTEM_AUTH"))
    except Exception:
        # Fall back to env only
        sys_key = os.getenv("SAM_SYSTEM_API_KEY")
        sys_auth = os.getenv("SAM_SYSTEM_AUTH")

    try:
        if sys_key and sys_auth:
            # Per docs: GET /{opportunityId}/resources/download/zip with Authorization header
            url = f"https://api.sam.gov/prod/opportunity/v1/api/{notice_id}/resources/download/zip"
            params = {"api_key": sys_key}
            headers = {"Authorization": sys_auth}
            r = requests.get(url, headers=headers, params=params, timeout=60)
            if r.ok and (r.headers.get("content-type","").lower().endswith("zip") or r.content[:2] == b'PK'):
                zf = zipfile.ZipFile(io.BytesIO(r.content))
                for zi in zf.infolist():
                    if zi.is_dir():
                        continue
                    try:
                        data = zf.read(zi)
                        files.append((os.path.basename(zi.filename) or "attachment.bin", data))
                    except Exception:
                        continue
                if files:
                    return files
            else:
                # If unauthorized or not found, silently fall back
                pass
    except Exception:
        # Swallow and fall back
        pass

    # Attempt 2: Save description HTML as a helpful "attachment"
    try:
        api_key = get_sam_api_key()
        desc_url = f"https://api.sam.gov/prod/opportunities/v1/noticedesc"
        params = {"noticeid": notice_id}
        if api_key:
            params["api_key"] = api_key
        resp = requests.get(desc_url, params=params, timeout=30)
        if resp.ok and resp.text:
            files.append((f"{notice_id}_description.sig_html", resp.text.encode("utf-8", errors="ignore")))
    except Exception:
        pass

    return files

# ---------------------- Phase B: RFP parsing helpers ----------------------

def _fetch_sam_attachments(notice, api_key: str | None, target_dir: str, cookie: str | None = None):
    """
    Legacy helper used by the RFP Analyzer 'Fetch from SAM.gov' button.

    It delegates to `sam_try_fetch_attachments` (which already knows how to use
    system-level SAM credentials from st.secrets) and saves the resulting
    (filename, bytes) pairs into `target_dir`.

    Returns:
        pulled_paths: list of file paths written
        errors:       list of error strings (best-effort)
    """
    from typing import List

    pulled_paths: List[str] = []
    errors: List[str] = []

    try:
        # Extract a notice ID from either a dict or a raw string.
        notice_id = None
        if isinstance(notice, dict):
            for key in ("Notice ID", "NoticeID", "notice_id", "id", "NoticeId", "Sol Number", "Solicitation"):
                v = str(notice.get(key) or "").strip()
                if v:
                    notice_id = v
                    break
        elif isinstance(notice, str):
            notice_id = notice.strip()

        if not notice_id:
            errors.append("No Notice ID available to fetch attachments.")
            return [], errors

        fetcher = globals().get("sam_try_fetch_attachments")
        if not callable(fetcher):
            errors.append("sam_try_fetch_attachments helper is not available.")
            return [], errors

        files = fetcher(str(notice_id)) or []
        if not files:
            return [], errors

        import os
        os.makedirs(target_dir, exist_ok=True)

        for fname, data in files:
            name = (fname or f"{notice_id}.bin").strip() or f"{notice_id}.bin"
            safe_name = name.replace("/", "_").replace("\\", "_")
            path = os.path.join(target_dir, safe_name)
            try:
                with open(path, "wb") as fh:
                    fh.write(data or b"")
                pulled_paths.append(path)
            except Exception as e:
                errors.append(f"Failed to write {safe_name}: {e}")

    except Exception as e:
        errors.append(str(e))

    return pulled_paths, errors



# Phase X1 PDF extractor imports (safe/optional)
try:
    import pypdf as _pypdf  # modern fork of PyPDF2
except Exception:  # pragma: no cover - environment may not have pypdf
    _pypdf = None

try:
    import pdfplumber as _pdfplumber
except Exception:  # pragma: no cover - environment may not have pdfplumber
    _pdfplumber = None


def _safe_import_pdf_extractors():
    if _pypdf is not None:
        return ('pypdf', _pypdf)
    if _pdfplumber is not None:
        return ('pdfplumber', _pdfplumber)
    return None

def extract_text_from_file(path: str) -> str:
    """Unified extractor. Uses Phase X1 backends. Safe on environments missing PDF libs."""
    try:
        with open(path, 'rb') as fh:
            b = fh.read()
    except Exception:
        return ''
    name = os.path.basename(path)
    mime = _detect_mime_light(name)
    pages = extract_text_pages(b, mime)
    if not pages:
        try:
            return b.decode('utf-8', errors='ignore')
        except Exception:
            try:
                return b.decode('latin-1', errors='ignore')
            except Exception:
                return ''
    return "\n\n".join(pages)

def _detect_mime_light(name: str) -> str:
    n = (name or "").lower()
    if n.endswith(".pdf"): return "application/pdf"
    if n.endswith(".docx"): return "application/vnd.openxmlformats-officedocument.wordprocessingml.document"
    if n.endswith(".xlsx"): return "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
    if n.endswith(".txt"): return "text/plain"
    return "application/octet-stream"

def _tesseract_ok() -> bool:
    try:
        import pytesseract  # type: ignore
        return True
    except Exception:
        return False

MAX_RFP_INDEX_BYTES = 10 * 1024 * 1024  # cap bytes used for text extraction/indexing per file

def extract_text_pages(file_bytes: bytes, mime: str) -> list:
    """Return a list of page texts. Best-effort. Up to 100 pages to keep fast."""
    out = []
    m = (mime or "").lower()
    if "pdf" in m:
        # Try vector: pdfplumber then pypdf. Never reference missing names.
        if _pdfplumber is not None:
            try:
                import io as _io
                with _pdfplumber.open(_io.BytesIO(file_bytes)) as pdf:
                    for i, pg in enumerate(pdf.pages[:100]):
                        try:
                            txt = pg.extract_text() or ""
                        except Exception:
                            txt = ""
                        out.append(txt)
            except Exception:
                pass
        if not out and _pypdf is not None:
            try:
                import io as _io
                reader = _pypdf.PdfReader(_io.BytesIO(file_bytes))
                for i, p in enumerate(reader.pages[:100]):
                    try:
                        out.append(p.extract_text() or "")
                    except Exception:
                        out.append("")
            except Exception:
                pass
        # As a very last resort, try a naive decode of bytes to avoid empty output.
        if not out:
            try:
                out = [file_bytes.decode("utf-8", errors="ignore")]
            except Exception:
                out = []
    elif "wordprocessingml" in m or (mime == "" and file_bytes[:4] == b"PK\x03\x04"):
        try:
            import io as _io, docx  # type: ignore
            doc = docx.Document(_io.BytesIO(file_bytes))
            txt = "\n".join(p.text or "" for p in doc.paragraphs)
            out = [txt] if txt else []
        except Exception:
            out = []
    elif "spreadsheetml" in m:
        try:
            import io as _io, pandas as _pd  # type: ignore
            x = _pd.read_excel(_io.BytesIO(file_bytes), sheet_name=None, dtype=str)
            for sname, df in list(x.items())[:10]:
                txt = sname + "\n" + df.fillna("").astype(str).to_csv(sep="\t", index=False)
                out.append(txt)
        except Exception:
            out = []
    elif "text/plain" in m:
        try:
            out = [file_bytes.decode("utf-8", errors="ignore")]
        except Exception:
            out = [file_bytes.decode("latin-1", errors="ignore")]
    return out

def ocr_pages_if_empty(file_bytes: bytes, mime: str, pages_text: list) -> tuple:
    """Run OCR on empty PDF pages if pytesseract available. Returns (new_pages, ocr_count)."""
    if "pdf" not in (mime or "").lower():
        return pages_text, 0
    if not _tesseract_ok():
        return pages_text, 0
    try:
        import io as _io
        pdfplumber = _pdfplumber  # type: ignore
        import pytesseract  # type: ignore
        from PIL import Image  # type: ignore
        new_pages = list(pages_text)
        ocr_count = 0
        if pdfplumber is None:
            return pages_text, 0
        with pdfplumber.open(_io.BytesIO(file_bytes)) as pdf:
            for i, pg in enumerate(pdf.pages[:min(len(new_pages) or 100, 100)]):
                if i >= len(new_pages): new_pages.append("")
                if (new_pages[i] or "").strip():
                    continue
                try:
                    img = pg.to_image(resolution=200).original
                    if img is None:
                        continue
                    if not isinstance(img, Image.Image):
                        img = Image.fromarray(img)
                    txt = pytesseract.image_to_string(img) or ""
                    if txt.strip():
                        new_pages[i] = txt
                        ocr_count += 1
                except Exception:
                    pass
        return new_pages, ocr_count
    except Exception:
        return pages_text, 0


def compute_sha256(data: bytes) -> str:
    """Return hex sha256 of bytes. Small dedicated helper so we can reuse hashing consistently."""
    import hashlib
    try:
        return hashlib.sha256(data or b"" ).hexdigest()
    except Exception:
        # Avoid hard failures on weird blobs; fall back to empty hash
        return hashlib.sha256(b"" ).hexdigest()


def save_rfp_file_db(conn: "sqlite3.Connection", rfp_id: int | None, name: str, file_bytes: bytes) -> dict:
    """Dedup by sha256. Store bytes and basic stats. Return dict with id and stats."""
    mime = _detect_mime_light(name)
    sha = compute_sha256(file_bytes)
    with closing(conn.cursor()) as cur:
        # Dedup
        cur.execute("SELECT id, pages FROM rfp_files WHERE sha256=?;", (sha,))
        row = cur.fetchone()
        if row:
            rid = int(row[0]); pages = int(row[1]) if row[1] is not None else None
            # if not linked to RFP yet, link now; if linked to a different RFP, duplicate row for this RFP
            if rfp_id is not None:
                try:
                    cur.execute("SELECT rfp_id FROM rfp_files WHERE id=?;", (rid,))
                    linked = cur.fetchone()
                    existing_rfp = int(linked[0]) if linked and linked[0] is not None else None
                except Exception:
                    existing_rfp = None
                if existing_rfp is None:
                    try:
                        cur.execute("UPDATE rfp_files SET rfp_id=? WHERE id=?;", (int(rfp_id), rid))
                        conn.commit()
                        return {"id": rid, "sha256": sha, "filename": name, "mime": mime, "pages": pages, "dedup": True, "ocr_pages": 0}
                    except Exception:
                        pass
                elif existing_rfp != int(rfp_id):
                    try:
                        cur.execute(
                            "INSERT INTO rfp_files(rfp_id, filename, mime, sha256, pages, bytes, created_at) "
                            "SELECT ?, ?, mime, sha256, pages, bytes, datetime('now') FROM rfp_files WHERE id=?;",
                            (int(rfp_id), name, rid)
                        )
                        new_rid = cur.lastrowid
                        conn.commit()
                        return {"id": int(new_rid), "sha256": sha, "filename": name, "mime": mime, "pages": pages, "dedup": True, "ocr_pages": 0}
                    except Exception:
                        pass
            return {"id": rid, "sha256": sha, "filename": name, "mime": mime, "pages": pages, "dedup": True, "ocr_pages": 0}
        # New insert
        index_bytes = file_bytes
        try:
            if file_bytes is not None and len(file_bytes) > MAX_RFP_INDEX_BYTES:
                index_bytes = file_bytes[:MAX_RFP_INDEX_BYTES]
        except Exception:
            index_bytes = file_bytes
        pages_text = extract_text_pages(index_bytes, mime)
        pages_before = len(pages_text)
        pages_text, ocr_count = ocr_pages_if_empty(index_bytes, mime, pages_text)
        pages = len(pages_text) if pages_text else None
        cur.execute(
            "INSERT INTO rfp_files(rfp_id, filename, mime, sha256, pages, bytes, created_at) VALUES (?, ?, ?, ?, ?, ?, datetime('now'));",
            (int(rfp_id) if rfp_id is not None else None, name, mime, sha, pages or 0, sqlite3.Binary(file_bytes))
        )
        rid = cur.lastrowid
        conn.commit()
        return {"id": rid, "sha256": sha, "filename": name, "mime": mime, "pages": pages or 0, "dedup": False, "ocr_pages": ocr_count}


# === Data layer: RFP records ==================================================
def data_insert_rfp(conn, title: str, solnum: str, sam_url: str) -> int:
    """Insert an RFP shell row and return its id."""
    from contextlib import closing as _closing
    try:
        with _closing(conn.cursor()) as cur:
            cur.execute(
                "INSERT INTO rfps(title, solnum, notice_id, sam_url, file_path, created_at) "
                "VALUES (?,?,?,?,?, datetime('now'));",
                (
                    (title or "Untitled RFP").strip(),
                    (solnum or "").strip(),
                    (_parse_sam_notice_id(sam_url) or ""),
                    (sam_url or "").strip(),
                    "",
                ),
            )
            new_id = cur.lastrowid
            # Best-effort per-user ownership tagging; ignore if column does not exist yet
            owner_name = None
            try:
                owner_name = get_current_user_name()
            except Exception:
                owner_name = None
            if owner_name:
                try:
                    cur.execute(
                        "UPDATE rfps SET owner_user = COALESCE(owner_user, ?) WHERE id = ?;",
                        (owner_name, int(new_id)),
                    )
                except Exception:
                    # If the column is missing or update fails, continue without blocking RFP creation
                    pass
            conn.commit()
        return int(new_id)
    except Exception as e:
        logger.exception("data_insert_rfp failed")
        raise


def data_save_rfp_uploads(conn, rfp_id: int, uploads) -> int:
    """Persist uploaded files for an RFP, expanding ZIPs, and return count saved."""
    import io as _io
    import zipfile as _zip

    saved = 0
    for f in (uploads or []):
        try:
            name = (getattr(f, "name", "upload") or "").lower()
            b = f.getbuffer().tobytes() if hasattr(f, "getbuffer") else f.read()
        except Exception:
            logger.exception("Failed to read upload bytes")
            name, b = "upload", b""
        if not b:
            continue
        if name.endswith(".zip"):
            try:
                zf = _zip.ZipFile(_io.BytesIO(b))
                for zname in zf.namelist()[:80]:
                    if zname.endswith("/"):
                        continue
                    try:
                        save_rfp_file_db(conn, int(rfp_id), zname.split("/")[-1], zf.read(zname))
                        saved += 1
                    except Exception:
                        logger.exception("Failed to save file from ZIP")
            except Exception:
                logger.exception("Failed to expand ZIP upload")
        else:
            try:
                save_rfp_file_db(conn, int(rfp_id), getattr(f, "name", "upload"), b)
                saved += 1
            except Exception:
                logger.exception("Failed to save direct upload")
    return saved

# === Service layer: RFP Analyzer =============================================
def svc_create_rfp_and_ingest(conn, title: str, solnum: str, sam_url: str, uploads):
    """Create an RFP record, ingest uploads, and run the One‑Page analysis.

    Returns (rfp_id, saved_count). Analysis errors are logged but do not block
    RFP creation so you always keep the record and uploaded files.
    """
    new_id = data_insert_rfp(conn, title, solnum, sam_url)
    saved = data_save_rfp_uploads(conn, new_id, uploads)
    # Immediately run the same ingest/analyze pipeline used by the toolbar button
    try:
        ensure_jobs_schema(conn)
        try:
            _user_name = get_current_user_name()
        except Exception:
            _user_name = ""
        payload = {
            "scope": "rfp_ingest_analyze",
            "rfp_id": int(new_id),
        }
        if sam_url:
            payload["sam_url"] = sam_url
        jobs_enqueue(
            conn,
            job_type="rfp_ingest_analyze",
            payload=payload,
            created_by=_user_name or None,
        )
    except Exception as e:
        try:
            logger.exception("svc_create_rfp_and_ingest: enqueue analyze job failed")
        except Exception:
            pass
    return new_id, saved


# === End RFP data/service helpers ============================================

def extract_sections_L_M(text: str) -> dict:
    out = {}
    if not text:
        return out
    mL = re.search(r'(SECTION\s+L[\s\S]*?)(?=SECTION\s+[A-Z]|\Z)', text, re.IGNORECASE)
    if mL:
        out['L'] = mL.group(1)
    mM = re.search(r'(SECTION\s+M[\s\S]*?)(?=SECTION\s+[A-Z]|\Z)', text, re.IGNORECASE)
    if mM:
        out['M'] = mM.group(1)
    return out

def derive_lm_items(section_text: str) -> list:
    if not section_text:
        return []
    items = []
    for line in section_text.splitlines():
        s = line.strip()
        if len(s) < 4:
            continue
        if re.match(r'^([\-\u2022\*]|\(?[a-zA-Z0-9]\)|[0-9]+\.)\s+', s):
            items.append(s)
    seen = set()
    uniq = []
    for it in items:
        if it not in seen:
            uniq.append(it)
            seen.add(it)
    return uniq[:500]

def extract_clins(text: str) -> list:
    if not text:
        return []
    lines = text.splitlines()
    rows = []
    for i, ln in enumerate(lines):
        m = re.search(r'\bCLIN\s*([A-Z0-9\-]+)', ln, re.IGNORECASE)
        if m:
            clin = m.group(1)
            desc = lines[i+1].strip() if i+1 < len(lines) else ''
            mqty = re.search(r'\b(QTY|Quantity)[:\s]*([0-9,.]+)', ln + ' ' + desc, re.IGNORECASE)
            qty = mqty.group(2) if mqty else ''
            munit = re.search(r'\b(UNIT|Units?)[:\s]*([A-Za-z/]+)', ln + ' ' + desc, re.IGNORECASE)
            unit = munit.group(2) if munit else ''
            rows.append({
                'clin': clin,
                'description': desc[:300],
                'qty': qty,
                'unit': unit,
                'unit_price': '',
                'extended_price': ''
            })
    seen = set()
    uniq = []
    for r in rows:
        if r['clin'] not in seen:
            uniq.append(r)
            seen.add(r['clin'])
    return uniq[:500]

def extract_dates(text: str) -> list:
    if not text:
        return []
    patterns = [
        r'(Questions(?:\s+Due)?|Q&A(?:\s+Due)?|Inquiry Deadline)[:\s]*([A-Za-z]{3,9}\s+\d{1,2},\s*\d{4}|\d{1,2}/\d{1,2}/\d{2,4})',
        r'(Proposals?\s+Due|Offers?\s+Due|Closing Date)[:\s]*([A-Za-z]{3,9}\s+\d{1,2},\s*\d{4}|\d{1,2}/\d{1,2}/\d{2,4})',
        r'(Site\s+Visit)[:\s]*([A-Za-z]{3,9}\s+\d{1,2},\s*\d{4}|\d{1,2}/\d{1,2}/\d{2,4})',
        r'(Period\s+of\s+Performance|POP)[:\s]*([A-Za-z]{3,9}\s+\d{1,2},\s*\d{4}|\d{1,2}/\d{1,2}/\d{2,4})',
    ]
    out = []
    for pat in patterns:
        for m in re.finditer(pat, text, re.IGNORECASE):
            out.append({'label': m.group(1).strip(), 'date_text': m.group(2).strip(), 'date_iso': ''})
    return out[:200]

def extract_pocs(text: str) -> list:
    if not text:
        return []
    emails = list(set(re.findall(r'[\w\.-]+@[\w\.-]+\.[A-Za-z]{2,}', text)))
    phones = list(set(re.findall(r'(?:\+?1\s*)?(?:\(\d{3}\)|\d{3})[\s\-]?\d{3}[\s\-]?\d{4}', text)))
    poc_blocks = re.findall(r'(Contracting Officer|Contract Specialist|Point of Contact|POC).*?(?:\n\n|$)', text, re.IGNORECASE|re.DOTALL)
    names = []
    for blk in poc_blocks:
        for nm in re.findall(r'([A-Z][a-zA-Z\-]+\s+[A-Z][a-zA-Z\-]+)', blk):
            names.append(nm)
    out = []
    for i in range(max(len(names), len(emails), len(phones))):
        out.append({
            'name': names[i] if i < len(names) else '',
            'role': 'POC',
            'email': emails[i] if i < len(emails) else '',
            'phone': phones[i] if i < len(phones) else '',
        })
    return out[:100]
def _num_from_words(s: str) -> int | None:
    m = re.search(r'\d+', s or '')
    if m:
        return int(m.group(0))
    words = {'one':1,'two':2,'three':3,'four':4,'five':5,'six':6,'seven':7,'eight':8,'nine':9,'ten':10,'eleven':11,'twelve':12}
    t = (s or '').strip().lower()
    return words.get(t)

def extract_pop_structure(text: str) -> dict:
    if not text:
        return {}
    tl = text.lower()
    base = 1 if re.search(r'\bbase\s+(year|period)\b', tl) else 0
    oy = 0
    for pat in [r'(\d+)\s+option\s+(?:years?|periods?)',
                r'(one|two|three|four|five|six|seven|eight|nine|ten)\s+option\s+(?:years?|periods?)',
                r'option\s+years?\s+(one|two|three|four|five|six|seven|eight|nine|ten|\d+)\s*(?:through|-|to)\s*(one|two|three|four|five|six|seven|eight|nine|ten|\d+)\b']:
        m = re.search(pat, tl)
        if m:
            if len(m.groups())==2 and m.group(2):
                a = _num_from_words(m.group(1)) or 0
                b = _num_from_words(m.group(2)) or 0
                if b>=a and a>0:
                    oy = max(oy, b - a + 1)
            else:
                oy = max(oy, _num_from_words(m.group(1)) or 0)
    total_years = None
    m = re.search(r'(ordering|contract)\s+period[^\d]{0,20}(\d+)\s+year', tl)
    if m:
        try:
            total_years = int(m.group(2))
        except Exception:
            total_years = None
    base_months = None
    m = re.search(r'base\s+(?:year|period)[^\d]{0,12}(\d{1,2})\s+month', tl)
    if m:
        try:
            base_months = int(m.group(1))
        except Exception:
            base_months = None
    out = {}
    if base or oy:
        label = "Base" if base else ""
        if oy:
            if label:
                label += " + "
            label += f"{oy} Option Year{'s' if oy!=1 else ''}"
        out['pop_structure'] = label.strip()
    if total_years:
        out['ordering_period_years'] = total_years
    if base_months:
        out['base_months'] = base_months
    return out

# === Y5.5: AI-assisted Parse & Save (safe indent) ===

# === Phase 3: Parser schema and normals (Y55) ===
_Y55_SCHEMA = {
    "title": str,
    "solnum": str,
    "meta": dict,          # may include: naics, set_aside, place_of_performance, due_offer, due_questions
    "l_items": list,       # list[str]
    "clins": list,         # list[dict]
    "dates": list,         # list[dict]
    "pocs": list           # list[dict]
}

def _y55_norm_date(s: str) -> str:
    """
    Normalize many date strings to 'YYYY MM DD'. Return '' if unknown.
    """
    import re, datetime
    if not s:
        return ""
    t = str(s).strip()
    # Remove time parts
    t = re.sub(r'(\d{1,2}:\d{2}.*)$', '', t).strip()
    # Try formats explicitly
    fmts = ["%B %d, %Y", "%b %d, %Y", "%m/%d/%Y", "%m/%d/%y", "%Y-%m-%d", "%Y/%m/%d"]
    for fmt in fmts:
        try:
            dt = datetime.datetime.strptime(t, fmt).date()
            return f"{dt.year:04d} {dt.month:02d} {dt.day:02d}"
        except Exception:
            pass
    # Fallback: grab mm/dd/yy or Month D, YYYY via regex
    m = re.search(r'([A-Za-z]{3,9})\s+(\d{1,2}),\s*(\d{4})', t)
    if m:
        try:
            dt = datetime.datetime.strptime(m.group(0), "%B %d, %Y").date()
        except Exception:
            try:
                dt = datetime.datetime.strptime(m.group(0), "%b %d, %Y").date()
            except Exception:
                dt = None
        if dt:
            return f"{dt.year:04d} {dt.month:02d} {dt.day:02d}"
    m = re.search(r'(\d{1,2})[/-](\d{1,2})[/-](\d{2,4})', t)
    if m:
        mm = int(m.group(1)); dd = int(m.group(2)); yy = int(m.group(3))
        if yy < 100:
            yy = 2000 + yy if yy < 50 else 1900 + yy
        try:
            dt = datetime.date(yy, mm, dd)
            return f"{dt.year:04d} {dt.month:02d} {dt.day:02d}"
        except Exception:
            return ""
    return ""

def _y55_coerce_money(x):
    try:
        s = str(x or "").replace(",", "").replace("$","").strip()
        return float(s) if s else 0.0
    except Exception:
        return 0.0

def _y55_validate(d: dict) -> dict:
    """
    Ensure structure matches _Y55_SCHEMA.
    Coerce money to numeric. Normalize date strings to 'YYYY MM DD' in dates[].date_iso and meta due_*.
    Replace malformed parts with safe defaults.
    """
    import re
    safe = {"title":"", "solnum":"", "meta":{}, "l_items":[], "clins":[], "dates":[], "pocs":[]}
    if not isinstance(d, dict):
        return safe
    out = dict(safe)
    # Scalars
    out["title"] = str(d.get("title") or "")[:200]
    out["solnum"] = str(d.get("solnum") or "")[:80]
    # Meta
    meta = d.get("meta") if isinstance(d.get("meta"), dict) else {}
    meta2 = {}
    for k in ("naics","set_aside","place_of_performance","due_offer","due_questions"):
        v = meta.get(k, "")
        if k in ("due_offer","due_questions"):
            meta2[k] = _y55_norm_date(v) if v else ""
        else:
            meta2[k] = str(v) if v is not None else ""
    out["meta"] = meta2
    # Lists
    out["l_items"] = [str(x).strip() for x in (d.get("l_items") or []) if isinstance(x, (str,int,float))]
    clins_in = d.get("clins") if isinstance(d.get("clins"), list) else []
    clins_out = []
    for r in clins_in:
        if not isinstance(r, dict):
            continue
        clins_out.append({
            "clin": str(r.get("clin") or ""),
            "description": str(r.get("description") or "")[:300],
            "qty": str(r.get("qty") or ""),
            "unit": str(r.get("unit") or ""),
            "unit_price": _y55_coerce_money(r.get("unit_price")),
            "extended_price": _y55_coerce_money(r.get("extended_price")),
        })
    out["clins"] = clins_out
    dates_in = d.get("dates") if isinstance(d.get("dates"), list) else []
    dates_out = []
    for r in dates_in:
        if not isinstance(r, dict):
            continue
        lbl = str(r.get("label") or "").strip()
        txt = str(r.get("date_text") or "").strip()
        iso = _y55_norm_date(r.get("date_iso") or txt)
        dates_out.append({"label": lbl, "date_text": txt, "date_iso": iso})
    out["dates"] = dates_out
    pocs_in = d.get("pocs") if isinstance(d.get("pocs"), list) else []
    pocs_out = []
    for r in pocs_in:
        if not isinstance(r, dict):
            continue
        pocs_out.append({
            "name": str(r.get("name") or ""),
            "role": str(r.get("role") or "POC"),
            "email": str(r.get("email") or ""),
            "phone": str(r.get("phone") or ""),
        })
    out["pocs"] = pocs_out
    # Derive due_offer / due_questions if not set, from dates[] labels
    if not out["meta"].get("due_offer"):
        for r in out["dates"]:
            if re.search(r"(Offer|Proposal|Quote|Closing|Response Due)", r["label"], re.I):
                if r.get("date_iso"):
                    out["meta"]["due_offer"] = r["date_iso"]; break
    if not out["meta"].get("due_questions"):
        for r in out["dates"]:
            if re.search(r"(Question|Q&A|Inquiry)", r["label"], re.I):
                if r.get("date_iso"):
                    out["meta"]["due_questions"] = r["date_iso"]; break
    return out

def y55_ai_parse(text: str) -> dict:
    out = {"title":"", "solnum":"", "meta":{}, "l_items":[], "clins":[], "dates":[], "pocs":[]}
    t = (text or "").strip()
    if not t:

        # Phase 3 post-process: validate schema, normalize dates/money, derive due_* fields
        out = _y55_validate(out)
        return out

    try:
        client = get_ai()
        model_name = _resolve_model()
        sys_msg = SYSTEM_CO + " You extract procurement data exactly and return strict JSON."
        user_msg = (
            "Extract these from the RFP text. Return JSON with keys: "
            "title, solnum, meta:{naics,set_aside,place_of_performance}, "
            "l_items: [bulleted requirement lines], "
            "clins: [{clin, description, qty, unit, unit_price, extended_price}], "
            "dates: [{label, date_text}], "
            "pocs: [{name, role, email, phone}]. "
            "Be terse. Do not hallucinate unknown numbers; leave fields empty if not present.\n\n"
            "RFP TEXT START\n" + t[:180000] + "\nRFP TEXT END"
        )
        try:
            resp = client.chat.completions.create(
                model=model_name,
                messages=[{"role":"system","content":sys_msg},{"role":"user","content":user_msg}],
                temperature=0.0
            )
        except Exception as _e:
            resp = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role":"system","content":sys_msg},{"role":"user","content":user_msg}],
                temperature=0.0
            )
        raw = ""
        try:
            raw = resp.choices[0].message.content or ""
        except Exception:
            raw = ""
        import json, re
        m = re.search(r'\{.*\}', raw, re.S)
        data = {}
        if m:
            try:
                data = json.loads(m.group(0))
            except Exception:
                data = {}
        if not data and raw.strip():
            raw2 = raw.strip().strip("`").strip()
            try:
                data = json.loads(raw2)
            except Exception:
                data = {}
        if isinstance(data, dict):
            for k in out.keys():
                if k in data and data[k]:
                    out[k] = data[k]
        if not isinstance(out.get("l_items"), list): out["l_items"] = []
        if not isinstance(out.get("clins"), list): out["clins"] = []
        if not isinstance(out.get("dates"), list): out["dates"] = []
        if not isinstance(out.get("pocs"), list): out["pocs"] = []
        if not isinstance(out.get("meta"), dict): out["meta"] = {}
        out["title"] = (out.get("title") or "").strip()[:200]
        out["solnum"] = (out.get("solnum") or "").strip()[:80]

        # Phase 3 post-process: validate schema, normalize dates/money, derive due_* fields
        out = _y55_validate(out)
        return out

    except Exception:
        pass

        # Phase 3 post-process: validate schema, normalize dates/money, derive due_* fields
        out = _y55_validate(out)
        return out

def _y55_norm_str(x):
    import re
    try:
        return re.sub(r'\s+',' ', str(x or '')).strip()
    except Exception:
        return str(x)

def y55_merge_lm(base_list, ai_list):
    base = [_y55_norm_str(x) for x in (base_list or []) if _y55_norm_str(x)]
    ai = [_y55_norm_str(x) for x in (ai_list or []) if _y55_norm_str(x)]
    seen = set(); out = []
    for it in base + ai:
        if it not in seen:
            seen.add(it); out.append(it)
    return out[:1000]

def y55_merge_clins(base_rows, ai_rows):
    def norm_row(r):
        return {
            "clin": _y55_norm_str((r or {}).get("clin")),
            "description": _y55_norm_str((r or {}).get("description"))[:300],
            "qty": _y55_norm_str((r or {}).get("qty")),
            "unit": _y55_norm_str((r or {}).get("unit")),
            "unit_price": _y55_norm_str((r or {}).get("unit_price")),
            "extended_price": _y55_norm_str((r or {}).get("extended_price")),
        }
    base = [norm_row(r) for r in (base_rows or [])]
    ai = [norm_row(r) for r in (ai_rows or [])]
    out = []; seen = set()
    for r in base + ai:
        key = (r["clin"], r["description"], r["qty"], r["unit_price"], r["extended_price"])
        if key in seen:
            continue
        seen.add(key); out.append(r)
    return out[:2000]

def y55_merge_dates(base_rows, ai_rows):
    def norm(r):
        return {"label": _y55_norm_str((r or {}).get("label")), "date_text": _y55_norm_str((r or {}).get("date_text")), "date_iso": _y55_norm_str((r or {}).get("date_iso"))}
    base = [norm(r) for r in (base_rows or [])]
    ai = [norm(r) for r in (ai_rows or [])]
    out = []; seen = set()
    for r in base + ai:
        key = (r["label"], r["date_text"])
        if key in seen:
            continue
        seen.add(key); out.append(r)
    return out[:300]

def y55_merge_pocs(base_rows, ai_rows):
    def norm(r):
        return {"name": _y55_norm_str((r or {}).get("name")), "role": _y55_norm_str((r or {}).get("role") or "POC"),
                "email": _y55_norm_str((r or {}).get("email")), "phone": _y55_norm_str((r or {}).get("phone"))}
    base = [norm(r) for r in (base_rows or [])]
    ai = [norm(r) for r in (ai_rows or [])]
    out = []; seen = set()
    for r in base + ai:
        key = (r["name"], r["email"], r["phone"])
        if key in seen:
            continue
        seen.add(key); out.append(r)
    return out[:300]

def y55_apply_enhancement(text, l_items, clins, dates, pocs, meta, title, solnum):

    _notice = st.session_state.get("x3_modal_notice", {}) or {}
    _nid = _safe_int(_notice.get("Notice ID"))
    with st.modal("RFP Analyzer", key=f"x3_modal_{_nid}"):
        try:
            _x3_render_modal(conn, _notice)
        except Exception as e:
            st.error(f"Analyzer error: {e}")
            st.stop()

    if st.session_state.get("x3_show_modal"):
        _notice = st.session_state.get("x3_modal_notice", {}) or {}
        _nid = _safe_int(_notice.get("Notice ID"))
        with st.modal("RFP Analyzer", key=f"x3_modal_{_nid}"):
            try:
                _x3_render_modal(conn, _notice)
            except Exception as e:
                st.error(f"Analyzer error: {e}")
                st.stop()
    ai = y55_ai_parse(text or "")
    l_items2 = y55_merge_lm(l_items, ai.get("l_items", []))
    clins2 = y55_merge_clins(clins, ai.get("clins", []))
    dates2 = y55_merge_dates(dates, ai.get("dates", []))
    pocs2 = y55_merge_pocs(pocs, ai.get("pocs", []))
    meta2 = dict(meta or {})
    for k in ("naics","set_aside","place_of_performance"):
        v = (ai.get("meta", {}) or {}).get(k)
        if v:
            meta2[k] = _y55_norm_str(v)
    title2 = (ai.get("title") or "").strip() or (title or "")
    solnum2 = (ai.get("solnum") or "").strip() or (solnum or "")
    return (l_items2, clins2, dates2, pocs2, meta2, title2, solnum2)
# === end Y5.5 ===

def extract_clins_xlsx(file_bytes: bytes) -> list:
    try:
        import io, pandas as _pd
        wb = _pd.read_excel(io.BytesIO(file_bytes), sheet_name=None, dtype=str)
    except Exception:
        return []
    rows = []
    for sname, df in wb.items():
        if df is None or df.empty:
            continue
        df2 = df.copy()
        df2.columns = [str(c).strip() for c in df2.columns]
        low = [c.lower() for c in df2.columns]
        def _pick(fn):
            for i,c in enumerate(low):
                try:
                    if fn(c):
                        return df2.columns[i]
                except Exception:
                    continue
            return None
        col_clin = _pick(lambda c: 'clin' in c or 'line item' in c or c.startswith('clin') or c.startswith('line'))
        col_desc = _pick(lambda c: 'desc' in c or c=='item' or 'description' in c)
        col_qty  = _pick(lambda c: 'qty' in c or 'quantity' in c)
        col_unit = _pick(lambda c: c in ('unit','u/i','uom') or 'unit ' in c or 'uom' in c)
        col_upr  = _pick(lambda c: 'unit price' in c or (('unit' in c or 'price' in c) and 'total' not in c and 'ext' not in c and 'extended' not in c))
        col_ext  = _pick(lambda c: 'extended' in c or 'amount' in c or c=='total' or 'total price' in c or 'ext price' in c)
        if (col_clin or col_desc) and (col_qty or col_upr or col_ext):
            for _, r in df2.iterrows():
                def gv(col):
                    return "" if col is None else str(r.get(col, "")).strip()
                clin = gv(col_clin); desc = gv(col_desc); qty  = gv(col_qty)
                unit = gv(col_unit); upr  = gv(col_upr);  ext  = gv(col_ext)
                if any([clin, desc, qty, upr, ext]):
                    rows.append({'clin': clin, 'description': desc[:300] if desc else "", 'qty': qty, 'unit': unit, 'unit_price': upr, 'extended_price': ext})
    seen = set(); uniq = []
    for r in rows:
        key = (r['clin'], r['description'], r['qty'], r['unit_price'], r['extended_price'])
        if key in seen: continue
        seen.add(key); uniq.append(r)
    return uniq[:2000]

# -------------------- Modules --------------------
def run_contacts(conn: "sqlite3.Connection") -> None:
    st.header("Contacts")
    st.caption("Use this page to manage contracting officers, partners, and agency contacts linked to your opportunities.")
    with st.form("add_contact", clear_on_submit=True):
        c1, c2, c3 = st.columns([2, 2, 2])
        with c1:
            name = st.text_input("Name")
        with c2:
            email = st.text_input("Email")
        with c3:
            org = st.text_input("Organization")
        c4, c5 = st.columns([2, 2])
        with c4:
            phone = st.text_input("Phone", value="")
        with c5:
            title = st.text_input("Title", value="")
        submitted = st.form_submit_button("Add Contact")
    if submitted:
        try:
            with closing(conn.cursor()) as cur:
                cur.execute(
                    "INSERT INTO contacts(name, email, org, phone, title, public_source, owner_user, created_at) "
                    "VALUES (?, ?, ?, ?, ?, ?, ?, datetime('now'));",
                    (
                        name.strip(),
                        email.strip(),
                        org.strip(),
                        phone.strip(),
                        title.strip(),
                        "manual_entry",
                        get_current_user_name(),
                    ),
                )
                conn.commit()
            st.success(f"Added contact {name}")
        except Exception as e:
            st.error(f"Error saving contact {e}")

    try:
        source_filter = st.selectbox(
            "Source filter",
            ["All", "SAM notice", "Manual entry", "Other"],
            index=0,
        )
        base_sql = (
            "SELECT name, email, org, phone, title, public_source, "
            "COALESCE(engagement_score, 0) AS engagement_score, "
            "COALESCE(last_engagement_at, '') AS last_engagement_at, "
            "COALESCE(created_at, '') AS created_at "
            "FROM contacts_t"
        )
        params = []
        if source_filter == "SAM notice":
            base_sql += " WHERE public_source = ?"
            params.append("sam_notice")
        elif source_filter == "Manual entry":
            base_sql += " WHERE COALESCE(public_source, '') IN ('', 'manual_entry')"
        elif source_filter == "Other":
            base_sql += " WHERE COALESCE(public_source, '') NOT IN ('', 'manual_entry', 'sam_notice')"
        base_sql += " ORDER BY name;"
        df = pd.read_sql_query(
            base_sql,
            conn,
            params=params or None,
        )

        st.subheader("Contact List")
        if df.empty:
            st.write("No contacts yet")
        else:
            _styled_dataframe(df, use_container_width=True, hide_index=True)

        st.subheader("Edit contacts")
        try:
            df_edit = pd.read_sql_query(
                "SELECT id, name, email, org, phone, title FROM contacts_t ORDER BY name;",
                conn,
                params=(),
            )
        except Exception as e:
            st.error(f"Failed to load contacts for editing: {e}")
            df_edit = None

        if df_edit is not None and not df_edit.empty:
            df_edit = df_edit.fillna("")
            try:
                edited = st.data_editor(
                    df_edit,
                    key="contacts_editor",
                    use_container_width=True,
                    hide_index=True,
                    column_config={
                        "id": st.column_config.NumberColumn("ID", disabled=True),
                    },
                )
            except Exception:
                # Fallback without column_config if running on older Streamlit
                edited = st.data_editor(
                    df_edit,
                    key="contacts_editor",
                    use_container_width=True,
                    hide_index=True,
                )

            if st.button("Save contact edits", key="contacts_save_edits"):
                from contextlib import closing as _closing
                try:
                    with _closing(conn.cursor()) as cur:
                        for _, r in edited.iterrows():
                            try:
                                cid = int(r.get("id") or 0)
                            except Exception:
                                cid = 0
                            name_e = str(r.get("name") or "").strip()
                            email_e = str(r.get("email") or "").strip()
                            org_e = str(r.get("org") or "").strip()
                            phone_e = str(r.get("phone") or "").strip()
                            title_e = str(r.get("title") or "").strip()

                            if cid > 0:
                                cur.execute(
                                    "UPDATE contacts SET name=?, email=?, org=?, phone=?, title=? WHERE id=?;",
                                    (name_e, email_e, org_e, phone_e, title_e, cid),
                                )
                            else:
                                # Insert new contacts entered via the editor
                                if not (name_e or email_e or org_e or phone_e or title_e):
                                    continue
                                cur.execute(
                                    "INSERT INTO contacts(name, email, org, phone, title, public_source, owner_user, created_at) "
                                    "VALUES (?, ?, ?, ?, ?, ?, ?, datetime('now'));",
                                    (
                                        name_e,
                                        email_e,
                                        org_e,
                                        phone_e,
                                        title_e,
                                        "manual_entry",
                                        get_current_user_name(),
                                    ),
                                )
                    conn.commit()
                    st.success("Saved contact edits.")
                    try:
                        st.rerun()
                    except Exception:
                        pass
                except Exception as e:
                    st.error(f"Failed to save contact edits: {e}")
        elif df_edit is not None:
            st.caption("No contacts available to edit yet.")

        st.subheader("Delete contacts")
        try:
            df_del = pd.read_sql_query(
                "SELECT id, name, email FROM contacts_t ORDER BY name;",
                conn,
                params=(),
            )
        except Exception as e:
            st.error(f"Failed to load contacts for deletion: {e}")
            df_del = None

        if df_del is not None and not df_del.empty:
            opts = {
                int(r["id"]): f"#{int(r['id'])} — {str(r.get('name') or '').strip()} ({str(r.get('email') or '').strip()})"
                for _, r in df_del.iterrows()
            }
            sel_del = st.multiselect(
                "Select contacts to delete",
                options=list(opts.keys()),
                format_func=lambda k: opts.get(k, str(k)),
                key="contacts_delete_sel",
            )
            if sel_del and st.button("Delete selected contacts", key="contacts_delete_btn"):
                from contextlib import closing as _closing
                try:
                    with _closing(conn.cursor()) as cur:
                        for cid in sel_del:
                            cur.execute("DELETE FROM contacts WHERE id=?;", (int(cid),))
                    conn.commit()
                    st.success("Deleted selected contacts.")
                    try:
                        st.rerun()
                    except Exception:
                        pass
                except Exception as e:
                    st.error(f"Failed to delete contacts: {e}")
        elif df_del is not None:
            st.caption("No contacts available to delete.")

        st.subheader("Contact detail")
        try:
            df_ids = pd.read_sql_query(
                "SELECT id, name FROM contacts_t ORDER BY name;",
                conn,
                params=(),
            )
        except Exception as e:
            st.error(f"Failed to load contact details: {e}")
            df_ids = None

        if df_ids is not None and not df_ids.empty:
            sel_cid = st.selectbox(
                "Select a contact",
                options=df_ids["id"].tolist(),
                format_func=lambda i: f"#{i} — " + str(df_ids.loc[df_ids["id"] == i, "name"].values[0]),
                key="contact_detail_select",
            )

            try:
                df_one = pd.read_sql_query(
                    "SELECT id, name, email, org, phone, title, public_source, "
                    "COALESCE(engagement_score, 0) AS engagement_score, "
                    "COALESCE(last_engagement_at, '') AS last_engagement_at, "
                    "COALESCE(created_at, '') AS created_at "
                    "FROM contacts_t WHERE id = ?;",
                    conn,
                    params=(int(sel_cid),),
                )
                if not df_one.empty:
                    row_c = df_one.iloc[0]
                    body = (
                        f"Email: {row_c.get('email') or '—'}\n\n"
                        f"Organization: {row_c.get('org') or '—'}\n\n"
                        f"Phone: {row_c.get('phone') or '—'}\n\n"
                        f"Title: {row_c.get('title') or '—'}\n\n"
                        f"Engagement score: {float(row_c.get('engagement_score') or 0):.1f}\n"
                        f"Last engagement: {row_c.get('last_engagement_at') or '—'}\n"
                    )
                    footer = f"Source: {row_c.get('public_source') or '—'} | Created: {row_c.get('created_at') or '—'}"
                    render_card(f"Contact #{int(row_c['id'])} — {row_c.get('name') or ''}", body=body, footer=footer)
            except Exception as e:
                st.error(f"Failed to load selected contact: {e}")
                sel_cid = None

            # Activity timeline for this contact
            if sel_cid:
                try:
                    owner_scope = st.session_state.get("deal_owner_ctx", get_current_user_name())
                    sql = "SELECT ts, type, subject, notes, deal_id FROM activities_t WHERE contact_id = ?"
                    params_act = [int(sel_cid)]
                    if owner_scope and owner_scope != "All":
                        sql += " AND owner_user = ?"
                        params_act.append(owner_scope)
                    sql += " ORDER BY ts DESC LIMIT 200"
                    df_act_c = pd.read_sql_query(sql, conn, params=params_act)
                except Exception as e:
                    st.error(f"Failed to load activities for this contact: {e}")
                    df_act_c = None

                st.subheader("Activity timeline")
                if df_act_c is None or df_act_c.empty:
                    st.caption("No activities logged for this contact yet.")
                else:
                    # Attach deal titles when possible
                    try:
                        df_deals_tl = pd.read_sql_query(
                            "SELECT id, title FROM deals_t;",
                            conn,
                            params=(),
                        )
                        dmap = {int(r["id"]): str(r.get("title") or "") for _, r in df_deals_tl.iterrows()}
                        df_act_c["deal_title"] = df_act_c["deal_id"].map(dmap).fillna("")
                    except Exception:
                        df_act_c["deal_title"] = ""

                    df_act_c = df_act_c.rename(
                        columns={
                            "ts": "When",
                            "type": "Type",
                            "subject": "Subject",
                            "notes": "Notes",
                            "deal_title": "Deal",
                        }
                    )
                    cols_order = [c for c in ["When", "Type", "Subject", "Notes", "Deal"] if c in df_act_c.columns]
                    _styled_dataframe(df_act_c[cols_order], use_container_width=True, hide_index=True)

    except Exception as e:
        st.error(f"Failed to load contacts {e}")

def run_deals(conn: "sqlite3.Connection") -> None:
    # Deals merged into CRM. Keep a single source of truth.
    return run_crm(conn)




def _ensure_deal_for_notice_and_rfp(conn, tenant_id, notice, rfp_id):
    """Ensure there is a deals row linked to this notice/rfp_id and update rfp_id if missing.

    This keeps SAM Watch, RFP Analyzer, Deals, and Proposal Builder in sync so you can
    move from notice ▶ RFP ▶ pipeline ▶ proposal without re-entering data.
    """
    from contextlib import closing as _closing

    # Best-effort lookup of notice_id from the notice dict
    notice_id = ""
    try:
        if "_extract_notice_id_from_obj" in globals():
            notice_id = _extract_notice_id_from_obj(notice)
        else:
            if isinstance(notice, dict):
                notice_id = str(
                    notice.get("notice_id")
                    or notice.get("Notice ID")
                    or notice.get("NoticeID")
                    or notice.get("id")
                    or ""
                ).strip()
    except Exception:
        notice_id = ""
    try:
        rid_int = int(rfp_id) if rfp_id is not None else None
    except Exception:
        rid_int = None

    if not notice_id and not rid_int:
        return

    # Try to update an existing deal first
    try:
        with _closing(conn.cursor()) as cur:
            if notice_id:
                cur.execute(
                    "SELECT id, rfp_id FROM deals WHERE notice_id = ? ORDER BY id DESC LIMIT 1;",
                    (notice_id,),
                )
                row = cur.fetchone()
            else:
                row = None
        if row:
            did, cur_rfp = row
            if rid_int and (not cur_rfp):
                try:
                    with _closing(conn.cursor()) as cur2:
                        cur2.execute(
                            "UPDATE deals SET rfp_id = ?, updated_at = datetime('now') WHERE id = ?;",
                            (int(rid_int), int(did)),
                        )
                    conn.commit()
                except Exception:
                    pass
            return
    except Exception:
        # If lookup fails, fall through and attempt a create below.
        pass

    # No existing deal found; create a lightweight one so the pipeline stays in sync
    try:
        with _closing(conn.cursor()) as cur:
            try:
                cur.execute("PRAGMA table_info(deals);")
                cols = [r[1] for r in cur.fetchall() or []]
            except Exception:
                cols = []
    except Exception:
        cols = []
    colset = set(cols)

    title = ""
    agency = ""
    sam_url = ""
    solnum = ""
    posted = ""
    due = ""
    naics = ""
    psc = ""
    try:
        if isinstance(notice, dict):
            title = str(
                notice.get("Title")
                or notice.get("title")
                or notice.get("Notice Title")
                or f"Notice {notice_id}"
                or "New deal"
            )
            agency = str(
                notice.get("Agency Path")
                or notice.get("Agency")
                or notice.get("agency")
                or ""
            )
            sam_url = str(
                notice.get("SAM Link")
                or notice.get("sam_url")
                or ""
            )
            solnum = str(
                notice.get("Solicitation")
                or notice.get("Sol Number")
                or notice.get("solicitation_number")
                or ""
            )
            posted = str(
                notice.get("Posted")
                or notice.get("Post Date")
                or notice.get("Posted Date")
                or ""
            )
            due = str(
                notice.get("Response Due")
                or notice.get("Response Date")
                or notice.get("due_date")
                or ""
            )
            naics = str(
                notice.get("NAICS")
                or notice.get("naics")
                or ""
            )
            psc = str(
                notice.get("PSC")
                or notice.get("psc")
                or ""
            )
    except Exception:
        pass

    owner = None
    try:
        if "get_current_user_name" in globals():
            owner = get_current_user_name()
    except Exception:
        owner = None
    if not owner:
        owner = "Quincy"

    cols_ins = []
    vals = []

    # Required-ish fields
    if "title" in colset:
        cols_ins.append("title")
        vals.append(title or f"Notice {notice_id}" or "New deal")
    if "agency" in colset:
        cols_ins.append("agency")
        vals.append(agency or "")
    if "status" in colset:
        cols_ins.append("status")
        vals.append("Open")
    if "stage" in colset:
        cols_ins.append("stage")
        vals.append("Bidding")
    if "value" in colset:
        cols_ins.append("value")
        vals.append(0.0)
    if "owner" in colset:
        cols_ins.append("owner")
        vals.append(owner)
    if "owner_user" in colset:
        cols_ins.append("owner_user")
        vals.append(owner)

    # Linkage fields
    if "notice_id" in colset and notice_id:
        cols_ins.append("notice_id")
        vals.append(str(notice_id))
    if "solnum" in colset and solnum:
        cols_ins.append("solnum")
        vals.append(solnum)
    if "posted_date" in colset and posted:
        cols_ins.append("posted_date")
        vals.append(posted)
    if "rfp_deadline" in colset and due:
        cols_ins.append("rfp_deadline")
        vals.append(due)
    if "naics" in colset and naics:
        cols_ins.append("naics")
        vals.append(naics)
    if "psc" in colset and psc:
        cols_ins.append("psc")
        vals.append(psc)
    if "sam_url" in colset and sam_url:
        cols_ins.append("sam_url")
        vals.append(sam_url)
    if "rfp_id" in colset and rid_int:
        cols_ins.append("rfp_id")
        vals.append(int(rid_int))

    # Timestamps
    if "created_at" in colset:
        cols_ins.append("created_at")
        try:
            from datetime import datetime as _dt
            vals.append(_dt.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ"))
        except Exception:
            vals.append(None)

    if not cols_ins:
        return

    placeholders = ", ".join(["?"] * len(cols_ins))
    col_sql = ", ".join(cols_ins)
    sql = f"INSERT INTO deals({col_sql}) VALUES ({placeholders});"

    try:
        with _closing(conn.cursor()) as cur:
            cur.execute(sql, vals)
        conn.commit()
    except Exception:
        # Best-effort only; do not block the main SAM Watch flow.
        try:
            _debug_log(conn, "ensure_deal_for_notice_and_rfp", "insert_failed")
        except Exception:
            pass


def get_or_create_rfp_from_notice(conn, tenant_id, notice_row):
    """Create an RFP Analyzer rfps record from a SAM Watch row if it does not exist.

    notice_row is a dict and may use different key casings, for example:
      - notice_id or "Notice ID"
      - title or "Title"
      - solicitation_number, "Solicitation", or "Sol Number"
      - agency, "Agency"
      - due_date or "Response Date"

    The rfps table in this app uses columns: title, solnum, notice_id, sam_url, file_path, created_at, tenant_id.
    We map the incoming fields onto that schema and scope by tenant_id.
    """
    from contextlib import closing as _closing

    # Best-effort notice id extraction (aligned with _extract_notice_id_from_obj)
    nid = ""
    try:
        if isinstance(notice_row, dict):
            for key in (
                "notice_id",
                "Notice ID",
                "NoticeID",
                "id",
                "NoticeId",
                "NoticeIdentifier",
                "Solicitation",
                "Sol Number",
            ):
                v = notice_row.get(key)
                if v:
                    s = str(v).strip()
                    if s:
                        nid = s
                        break
    except Exception:
        nid = ""

    if not nid:
        raise ValueError("Missing notice_id for get_or_create_rfp_from_notice")

    # Title
    title = (
        (notice_row.get("title") if isinstance(notice_row, dict) else None)
        or (notice_row.get("Title") if isinstance(notice_row, dict) else None)
        or ""
    )

    # Solicitation / solicitation_number -> solnum
    solnum = ""
    if isinstance(notice_row, dict):
        solnum = (
            notice_row.get("solicitation_number")
            or notice_row.get("Solicitation")
            or notice_row.get("Sol Number")
            or notice_row.get("solnum")
            or ""
        ) or ""

    # SAM URL
    sam_url = ""
    if isinstance(notice_row, dict):
        sam_url = (
            notice_row.get("sam_url")
            or notice_row.get("SAM Link")
            or notice_row.get("URL")
            or ""
        ) or ""

    # Normalize tenant id
    try:
        tenant_id_int = int(tenant_id or 1)
    except Exception:
        tenant_id_int = 1

    with closing(conn.cursor()) as cur:
        # First look for an rfps row matching this notice and tenant (or legacy NULL tenant)
        cur.execute(
            """
            SELECT id
            FROM rfps
            WHERE notice_id = ?
              AND (tenant_id = ? OR tenant_id IS NULL)
            ORDER BY id DESC
            LIMIT 1;
            """,
            (nid, tenant_id_int),
        )
        row = cur.fetchone()
        if row:
            return int(row[0])

        # Insert a new rfps row scoped to this tenant
        cur.execute(
            """
            INSERT INTO rfps(title, solnum, notice_id, sam_url, file_path, created_at, tenant_id)
            VALUES (?,?,?,?,?, datetime('now'), ?);
            """,
            (title.strip() if isinstance(title, str) else str(title or ""),
             str(solnum or "").strip(),
             nid,
             str(sam_url or "").strip(),
             "",
             tenant_id_int,
            ),
        )
        conn.commit()
        return int(cur.lastrowid)

def _ensure_rfp_for_notice(conn, notice_row: dict) -> int:
    from contextlib import closing as _closing
    nid = str(notice_row.get('Notice ID') or "")
    if not nid:
        raise ValueError("Missing Notice ID")
    with _closing(conn.cursor()) as cur:
        cur.execute("SELECT id FROM rfps WHERE notice_id=? ORDER BY id DESC LIMIT 1;", (nid,))
        row = cur.fetchone()
        if row:
            return int(row[0])
        cur.execute(
            "INSERT INTO rfps(title, solnum, notice_id, sam_url, file_path, created_at) VALUES (?,?,?,?,?, datetime('now'));",
            (notice_row.get('Title') or "", notice_row.get('Solicitation') or "", nid, notice_row.get('SAM Link') or "", "")
        )
        rid = int(cur.lastrowid)
        conn.commit()
        return rid

def _fetch_and_save_now(conn, notice_id: str, rfp_id: int) -> int:
    saved = 0
    try:
        for fname, fbytes in (sam_try_fetch_attachments(str(notice_id)) or []):
            try:
                # de-dupe by sha256 via save_rfp_file_db
                save_rfp_file_db(conn, int(rfp_id), fname, fbytes)
                saved += 1
            except Exception:
                pass
    except Exception:
        pass
    return saved

def _rfp_build_fulltext_from_db(conn, rfp_id: int, max_files: int = 10, max_pages: int = 80) -> tuple[str, list]:
    """Return (full_text, sources) reading rfp_files; sources is list of (filename, page, text_snippet)."""
    try:
        import io
        from contextlib import closing as _closing
        with _closing(conn.cursor()) as cur:
            cur.execute("SELECT id, filename, bytes, mime FROM rfp_files WHERE rfp_id=? ORDER BY id;", (int(rfp_id),))
            rows = cur.fetchall() or []
    except Exception:
        rows = []
    sources = []
    parts = []
    count_files = 0
    for rid, name, bts, mime in rows:
        if count_files >= int(max_files):
            break
        try:
            pages = extract_text_pages(bts, mime or "")
            for pi, ptxt in enumerate(pages[:max_pages]):
                sources.append((name, pi+1, (ptxt or "")[:400]))
            parts.append("\n\n".join(pages[:max_pages]))
            count_files += 1
        except Exception:
            continue
    return ("\n\n".join([p for p in parts if p]).strip(), sources)

def _rfp_ai_summary(text: str, meta: dict) -> str:
    """Use y55_ai_parse to produce a structured summary in prose."""
    try:
        data = y55_ai_parse(text or "")
    except Exception as e:
        return f"AI parse failed: {e}"
    # format a brief summary
    lines = []
    t = data.get("title") or meta.get("Title") or ""
    s = data.get("solnum") or meta.get("Solicitation") or ""
    lines.append(f"**{t or meta.get('Title','')}**  ")
    if s: lines.append(f"Solicitation: {s}")
    m = data.get("meta") or {}
    if m:
        for k in ("set_aside","naics","psc","pop","place_of_performance","contract_type","vehicle","agency","office"):
            v = m.get(k) or meta.get(k.replace('_',' ').title(), "")
            if v: lines.append(f"- {k.replace('_',' ').title()}: {v}")
    dates = data.get("dates") or []
    if dates:
        lines.append("**Key Dates**")
        for d in dates[:6]:
            nm = (d.get("name") or "").title()
            val = d.get("date") or d.get("iso") or ""
            if nm or val: lines.append(f"- {nm}: {val}")
    lms = data.get("l_items") or []
    if lms:
        lines.append("**L/M Highlights**")
        for li in lms[:8]:
            lines.append(f"- {li.get('text') or str(li)}")
    return "\n".join(lines)

def _rfp_chat(conn, rfp_id: int, question: str, k: int = 6) -> str:
    """Lightweight RAG: use y1_search() hits to ground the answer."""
    try:
        hits = y1_search(conn, int(rfp_id), question or "", k=int(k))
    except Exception:
        hits = []
    context = []
    for h in hits or []:
        src = f"{h.get('file') or ''} p.{h.get('page') or ''}".strip()
        snippet = (h.get('text') or '')[:800]
        context.append(f"[{src}] {snippet}")
    sys = "You are an acquisitions analyst. Answer concisely and cite sources in brackets like [filename p.X]."
    prompt = "\n\n".join(context + [f"Question: {question}"])
    try:
        client = get_ai()
        model = _resolve_model()
        resp = client.chat.completions.create(model=model, messages=[
            {"role":"system","content": sys},
            {"role":"user","content": prompt}
        ], temperature=0.2)
        return (resp.choices[0].message.content or '').strip()
    except Exception as e:
        return f"AI error: {e}"

# ---------- SAM Watch (Phase A) ----------


def _sam_run_live_search_job(conn_jobs, job_id: int, params: dict):
    import pandas as pd
    """Execute a SAM.gov live search as a tracked job.

    For now this still runs in-process, but the job row in `jobs`
    keeps a record of progress and the outcome so a future worker
    can take over.
    """
    import pandas as _pd

    try:
        ensure_jobs_schema(conn_jobs)
    except Exception:
        # Best-effort only; do not hard fail if schema init has issues
        pass

    # Mark job as running
    try:
        jobs_update_status(
            conn_jobs,
            job_id,
            status="running",
            mark_started=True,
            progress=0.0,
        )
    except Exception:
        pass

    # Execute the actual SAM search
    out = sam_search_cached(dict(params) if isinstance(params, dict) else {})
    if out.get("error"):
        # Mark failed and propagate a clean exception
        try:
            jobs_update_status(
                conn_jobs,
                job_id,
                status="failed",
                error_message=str(out.get("error")),
                mark_finished=True,
            )
        except Exception:
            pass
        raise RuntimeError(str(out.get("error")))

    recs = out.get("records") or []
    try:
        sam_persist_notices(conn_jobs, recs)
    except Exception:
        pass
    df = flatten_records(recs) if recs else _pd.DataFrame()

    # Mark as done with a small summary in result_json
    # Serialize a lightweight view of the results into the job row so the UI can reload
    try:
        try:
            rows = df.to_dict(orient="records")
        except Exception:
            rows = []
        jobs_update_status(
            conn_jobs,
            job_id,
            status="done",
            mark_finished=True,
            progress=1.0,
            result={
                "records_fetched": int(len(df)),
                "rows": rows,
            },
        )
    except Exception:
        pass

    return df


def _sam_refresh_results_from_job(conn) -> None:
    """If a SAM search job finished in the background, load its results into session_state.

    This keeps the Streamlit thread lightweight while the heavy SAM.gov call runs
    in the background jobs worker.
    """
    import pandas as _pd
    import json as _json

    try:
        job_id = int(st.session_state.get("sam_last_job_id") or 0)
    except Exception:
        job_id = 0
    if not job_id:
        return

    try:
        ensure_jobs_schema(conn)
    except Exception:
        # Best-effort only; do not break the page if jobs schema is unavailable
        return

    try:
        df = _pd.read_sql_query(
            "SELECT id, status, result_json, error_message FROM jobs WHERE id = ?;",
            conn,
            params=(job_id,),
        )
    except Exception:
        return
    if df is None or df.empty:
        return

    row = df.iloc[0]
    status = str(row.get("status") or "").lower()
    err = row.get("error_message") or ""
    result_json = row.get("result_json") or ""
    data = {}
    if result_json:
        try:
            data = _json.loads(result_json)
        except Exception:
            data = {}

    if status == "done":
        rows = data.get("rows") or []
        if rows:
            try:
                st.session_state["sam_results_df"] = _pd.DataFrame(rows)
                st.session_state["sam_page"] = 1
                st.session_state.pop("sam_selected_idx", None)
            except Exception:
                # Do not hard fail if session state is not available
                pass
    elif status in ("failed", "error"):
        if err:
            try:
                st.error(f"SAM search job failed: {err}")
            except Exception:
                pass

def run_sam_watch(conn) -> None:
    """
    Phase 2 cleanup:
    - Always render the SAM Watch UI (no gating behind Save Search).
    - Adds tabs: Smart Search | Alerts.
    - Adds a Score column computed from NL query (fallback heuristics if scorer is missing).
    - Prevents accidental page jumps by clearing one-shot redirects.
    """
    import pandas as pd
    from datetime import datetime, timedelta

    st.header("SAM Watch")
    st.caption("Use this page to watch SAM for new opportunities, qualify good fits, and push the best notices into your deals pipeline.")
    st.markdown("**Primary action on this page:** run a smart search, then use Add to Deals on any strong notice you want in your pipeline.")


    # If pagination was clicked, scroll viewport to top on rerun

    if st.session_state.get('sam_scroll_to_top'):

        import streamlit.components.v1 as components

        components.html('<script>window.scrollTo(0,0);</script>', height=0)

        st.session_state['sam_scroll_to_top'] = False


    # Ensure Phase 2 DB schema (saved_searches.nl_query, alerts tables)
    try:
        _ensure_phase2_schema(conn)
    except Exception as _e:
        try: st.warning(f"Schema check failed: {_e}")
        except Exception: pass

    # --- Tabs ---
    tab_search, tab_alerts, tab_analyzer = st.tabs(["🔎 Smart Search", "🔔 Alerts", "🧠 Analyzer"])

    # Remember last search text for scoring context
    if "sam_nl_text" not in st.session_state:
        st.session_state["sam_nl_text"] = ""

    # ---------- SEARCH TAB ----------
    with tab_alerts:
        try:
            run_alerts_center(conn)
        except Exception as e:
            st.info("Alerts Center not available in this build.")
            st.caption(str(e))

    # Phase 3 Analyzer tab (renders even if Smart Search errors)
    with tab_analyzer:
        try:
            # Use the lightweight inline analyzer inside SAM Watch tabs
            _phase3_analyzer_inline(conn)
        except Exception as e:
            st.exception(e)
    
    with tab_search:
        api_key = get_sam_api_key()

        with st.expander("Search Filters", expanded=True):
            today = datetime.now().date()
            default_from = today - timedelta(days=30)

            c1, c2, c3 = st.columns([2, 2, 2])
            with c1:
                use_dates = st.checkbox("Filter by posted date", value=False, key="sam_use_dates", help="Limit results to notices posted within this date range.")
            with c2:
                active_only = st.checkbox("Active only", value=True, key="sam_active_only", help="When checked, hide notices that are already closed or cancelled.")
            with c3:
                org_name = st.text_input("Organization/Agency contains", key="sam_org", help="Optional: filter by agency name, office, or organization text.")

            if use_dates:
                d1, d2 = st.columns([2, 2])
                with d1:
                    posted_from = st.date_input("Posted From", value=default_from, key="sam_posted_from")
                with d2:
                    posted_to = st.date_input("Posted To", value=today, key="sam_posted_to")

            e1, e2, e3 = st.columns([2, 2, 2])
            with e1:
                keywords = st.text_input("Keywords (Title contains)", key="sam_keywords", help="Words that should appear in the notice title, such as janitorial, HVAC, or security.")
            with e2:
                naics = st.text_input("NAICS (6-digit)", key="sam_naics", help="Optional NAICS filter, for example 561720 for janitorial, 541512 for IT services.")
            with e3:
                psc = st.text_input("PSC", key="sam_psc", help="Optional PSC filter if you want to narrow by Product Service Code.")

            e4, e5, e6 = st.columns([2, 2, 2])
            with e4:
                state = st.text_input("Place of Performance State (e.g., TX)", key="sam_state", help="Two-letter state where the work will be performed, such as TX or CA.")
            with e5:
                set_aside = st.text_input("Set-Aside Code (SB, 8A, SDVOSB)", key="sam_set_aside", help="Optional set-aside filter, such as SB, 8A, WOSB, HUBZone, or SDVOSB.")

with e6:
    our_set_aside = st.text_input(
        "Our Set-Aside (for sub filter)",
        key="sam_our_set_aside",
        placeholder="e.g., WOSB or 8A",
        help="Used to identify notices that are outside your own set-aside when filtering results.",
    )

            ptype_map = {
                "Pre-solicitation": "p",
                "Sources Sought": "r",
                "Special Notice": "s",
                "Solicitation": "o",
                "Combined Synopsis/Solicitation": "k",
                "Justification (J&A)": "u",
                "Sale of Surplus Property": "g",
                "Intent to Bundle (DoD)": "i",
                "Award Notice": "a",
            }
            types = st.multiselect(
                "Notice Types",
                list(ptype_map.keys()),
                default=["Solicitation", "Combined Synopsis/Solicitation", "Sources Sought"],
                key="sam_types",
            )

            g1, g2 = st.columns([2, 2])
            with g1:
                limit = st.number_input("Results per page", min_value=1, max_value=1000, value=100, step=50, key="sam_limit")
            with g2:
                max_pages = st.slider("Pages to fetch", min_value=1, max_value=10, value=3, key="sam_max_pages")

            st.text_input("Smart search (natural language)", placeholder="ex: janitorial 561720 TX due < 30 days", key="sam_nl_text", help="Natural language search that is parsed into filters. Use plain English plus NAICS, state, and simple date expressions like \"due < 30 days\".")

            cbtn1, cbtn2 = st.columns([1,1])
            with cbtn1:
                _set_flag("clicked_run_search", st.button("Run Search", type="primary", key="sam_run_btn"))
            with cbtn2:
                clicked_save = st.button("💾 Save this search", key="sam_save_btn")

        # --- Saved search metadata controls ---
        st.session_state.setdefault('sam_save_name', (st.session_state.get('sam_keywords') or st.session_state.get('sam_nl_text') or 'Saved search').strip())
        name_col, cad_col = st.columns([2,1])
        with name_col:
            st.text_input('Search name', key='sam_save_name')
        with cad_col:
            st.selectbox('Alert cadence', ['daily','weekly','monthly'], key='sam_save_cadence', index=0)

        # Save search does NOT gate the rest of the UI

        if clicked_save:
            try:
                _ensure_phase2_schema(conn)
            except Exception:
                pass
            try:
                with conn:
                    conn.execute(
                        "INSERT INTO saved_searches(name, nl_query, cadence, created_at) VALUES(?, ?, ?, datetime('now'));",
                        (
                            (st.session_state.get("sam_save_name") or (st.session_state.get("sam_keywords") or st.session_state.get("sam_nl_text") or "Saved search")).strip(),
                            st.session_state.get("sam_nl_text") or "",
                            st.session_state.get("sam_save_cadence") or "daily",
                        ),
                    )
                st.success("Search saved.")
            except Exception as e:
                st.error(f"Save failed: {e}")


        # Run search (tracked as a job so heavy queries do not block the UI)
        results_df = st.session_state.get("sam_results_df")
        # If a background SAM search job finished, pull its results into session state
        try:
            _sam_refresh_results_from_job(conn)
            results_df = st.session_state.get("sam_results_df")
        except Exception:
            pass
        if _get_flag("clicked_run_search"):
            if not api_key:
                st.error("Missing SAM API key. Add SAM_API_KEY to your Streamlit secrets.")
                return

            # Build parameter payload from the current filters
            params = {
                "api_key": api_key,
                "limit": int(st.session_state.get("sam_limit", 100)),
                "offset": 0,
                "_max_pages": int(st.session_state.get("sam_max_pages", 3)),
            }
            if st.session_state.get("sam_active_only"):
                params["status"] = "active"
            if st.session_state.get("sam_use_dates"):
                params["postedFrom"] = st.session_state.get("sam_posted_from").strftime("%m/%d/%Y")
                params["postedTo"] = st.session_state.get("sam_posted_to").strftime("%m/%d/%Y")
            else:
                _today = datetime.now().date()
                _from = _today - timedelta(days=30)
                params["postedFrom"] = _from.strftime("%m/%d/%Y")
                params["postedTo"] = _today.strftime("%m/%d/%Y")
            if st.session_state.get("sam_keywords"):
                params["title"] = st.session_state["sam_keywords"]
            if st.session_state.get("sam_naics"):
                params["ncode"] = st.session_state["sam_naics"]
            if st.session_state.get("sam_psc"):
                params["ccode"] = st.session_state["sam_psc"]
            if st.session_state.get("sam_state"):
                params["state"] = st.session_state["sam_state"]
            if st.session_state.get("sam_set_aside"):
                params["typeOfSetAside"] = st.session_state["sam_set_aside"]
            if st.session_state.get("sam_org"):
                params["organizationName"] = st.session_state["sam_org"]
            if st.session_state.get("sam_types"):
                params["ptype"] = ",".join(ptype_map[t] for t in st.session_state["sam_types"] if t in ptype_map)

            # Enqueue a sam_live_search job that captures this payload
            try:
                conn_jobs = get_db()
            except Exception:
                conn_jobs = conn
            try:
                current_user = get_current_user_name()
            except Exception:
                current_user = ""
            ensure_jobs_schema(conn_jobs)
            job_id = jobs_enqueue(
                conn_jobs,
                job_type="sam_live_search",
                payload={"scope": "sam_live_search", "params": params},
                created_by=current_user or None,
            )
            st.session_state["sam_last_job_id"] = job_id

            # Execute the SAM search synchronously so results always appear,
            # even if no separate jobs worker process is running.
            try:
                df_res = _sam_run_live_search_job(conn_jobs, job_id, params)
            except Exception as _exc:
                try:
                    st.error(f"SAM search failed: {_exc}")
                except Exception:
                    pass
                df_res = None

            if df_res is not None:
                try:
                    st.session_state["sam_results_df"] = df_res
                    st.session_state["sam_page"] = 1
                    st.session_state.pop("sam_selected_idx", None)
                except Exception:
                    pass

                # Force a rerun so the SAM results table updates immediately
                try:
                    st.rerun()
                except Exception:
                    # If rerun is not available, the results will still appear
                    # on the next interaction.
                    pass

        # --- Always show the search body, even with no results yet ---
        if results_df is None or (hasattr(results_df, "empty") and results_df.empty):
            st.info("No results yet. Enter filters above and click **Run Search**.")
        else:
            # Compute a simple relevance Score (0-100). Use project's scorer if available.
            nl_text = st.session_state.get("sam_nl_text") or ""
            def _fallback_score(row):
                title = str(row.get("Title") or "").lower()
                desc = str(row.get("Description") or "").lower()
                q = nl_text.lower()
                terms = [t for t in re.split(r"[^a-z0-9]+", q) if t]
                if not terms:
                    return 0
                hits = sum(1 for t in terms if (t in title or t in desc))
                return int(min(100, round(100 * hits / max(1, len(set(terms))))))
            scorer = globals().get("relevance_score") or (lambda r, profile=None: _fallback_score(r))

            try:
                # Add Score column if missing
                if "Score" not in results_df.columns:
                    results_df = results_df.copy()
                    results_df["Score"] = [
                        (scorer(row.to_dict(), st.session_state.get("company_profile")) if hasattr(row, "to_dict")
                         else _fallback_score(row))
                        for _, row in results_df.iterrows()
                    ]
                    st.session_state["sam_results_df"] = results_df
            except Exception as e:
                st.warning(f"Scoring unavailable: {e}")


# Optional: Sub-opportunity filter based on our own set-aside (for finding sub work)
our_sa = (st.session_state.get("sam_our_set_aside") or "").strip().upper()
if our_sa and ("Set-Aside" in results_df.columns):
    only_non_our = st.checkbox(
        f"Show only notices NOT in our set-aside ({our_sa})",
        value=st.session_state.get("sam_only_non_our_set", False),
        key="sam_only_non_our_set",
    )
    if only_non_our:
        try:
            sa_series = results_df["Set-Aside"].astype(str).str.upper()
            mask = ~sa_series.str.contains(our_sa)
            results_df = results_df.loc[mask].reset_index(drop=True)
        except Exception:
            pass

# Optional: Table toggle
show_table = st.toggle("Show table view", value=False, key="sam_table_toggle")
            if show_table:
                cols = [c for c in ["Score","Title","Solicitation","Type","Set-Aside","NAICS","PSC","Posted","Response Due","Agency Path","SAM Link"] if c in results_df.columns]
                st.dataframe(results_df[cols].sort_values("Score", ascending=False), use_container_width=True, hide_index=True)

            # List view with simple pagination
            try:
                page_size = int(st.session_state.get("sam_limit", 100))
            except Exception:
                page_size = 100
            total = len(results_df)
            total_pages = max(1, (total + page_size - 1) // page_size)
            cur_page = int(st.session_state.get("sam_page", 1))
            cur_page = max(1, min(cur_page, total_pages))
            st.session_state["sam_page"] = cur_page

            def _sam_go(delta:int):

                cur = int(st.session_state.get('sam_page', 1))

                st.session_state['sam_page'] = cur + int(delta)

                st.session_state['sam_scroll_to_top'] = True

                st.rerun()


            p1, p2, p3 = st.columns([1, 3, 1])
            with p1:
                if st.button("◀ Prev", key="sam_prev_btn", disabled=(cur_page <= 1)):
                    _sam_go(-1)
            with p2:
                st.caption(f"Page {cur_page} of {total_pages} — showing {min(page_size, total - (cur_page - 1) * page_size)} of {total} results")
            with p3:
                if st.button("Next ▶", key="sam_next_btn", disabled=(cur_page >= total_pages)):
                    _sam_go(1)
            start_i = (cur_page - 1) * page_size
            end_i = min(start_i + page_size, total)

            for i in range(start_i, end_i):
                row = results_df.iloc[i]
                with st.container():
                    st.markdown(f"**{row.get('Title','')}**  \n:gray[Score:] **{int(row.get('Score',0))}**")
                    meta_line = " | ".join([
                        f"Solicitation: {row.get('Solicitation') or '—'}",
                        f"Type: {row.get('Type') or '—'}",
                        f"Set-Aside: {row.get('Set-Aside') or '—'}",
                        f"NAICS: {row.get('NAICS') or '—'}",
                        f"PSC: {row.get('PSC') or '—'}",
                    ])
                    st.caption(meta_line)
                    st.caption(f"Posted: {row.get('Posted') or '—'} · Due: {row.get('Response Due') or '—'} · Agency: {row.get('Agency Path') or '—'}")
                    try:
                        _render_ask_rfp_button(row.to_dict())  # expected to open the modal
                    except Exception:
                        pass
                    if row.get('SAM Link'):
                        st.markdown(f"[Open in SAM]({row.get('SAM Link')})")

                    c3, c4, c5 = st.columns([2, 2, 2])
                    with c3:
                        if st.button("View details", key=f"sam_view_{i}"):
                            st.session_state["sam_selected_idx"] = i
                            st.rerun()
                    with c4:
                        # Add to Deals (kept as-is; relies on project helpers)
                        if st.button("Add to Deals", key=f"add_to_deals_{i}"):
                            try:
                                from contextlib import closing as _closing
                                _db = globals().get('conn')
                                _owned = False
                                if _db is None:
                                    # Fall back to the central connector so Postgres / DATABASE_URL
                                    # configurations are respected.
                                    _owned = True
                                    _db = get_db()
                                with _closing(_db.cursor()) as cur:
                                    cur.execute(
                                        """
                                        INSERT INTO deals(title, agency, status, value, notice_id, solnum, posted_date, rfp_deadline, naics, psc, sam_url)
                                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?);
                                        """,
                                        (
                                            row.get('Title') or "",
                                            row.get('Agency Path') or "",
                                            "Bidding",
                                            None,
                                            row.get('Notice ID') or "",
                                            row.get('Solicitation') or "",
                                            row.get('Posted') or "",
                                            row.get('Response Due') or "",
                                            row.get('NAICS') or "",
                                            row.get('PSC') or "",
                                            row.get('SAM Link') or "",
                                        ),
                                    )
                                    deal_id = cur.lastrowid
                                    try:
                                        cur.execute("UPDATE deals SET status=?, stage=? rfp_deadline=?,  WHERE id=?", (STAGES_ORDERED[0], STAGES_ORDERED[0], str(pd.to_datetime(r.get('rfp_deadline')).date()) if pd.notnull(r.get('rfp_deadline')) else None, deal_id))
                                        cur.execute("INSERT INTO deal_stage_log(deal_id, stage, changed_at) VALUES(?, ?, datetime('now'))", (deal_id, STAGES_ORDERED[0]))
                                    except Exception:
                                        pass
                                    # Normalize new deal's stage/status so it appears in Kanban + summaries
                                    try:
                                        with _closing(_db.cursor()) as cur2:
                                            cur2.execute(
                                                "UPDATE deals SET status=?, stage=? WHERE id=?;",
                                                (STAGES_ORDERED[0], STAGES_ORDERED[0], deal_id),
                                            )

            _db.commit()
    except Exception:
        pass
    _db.commit()
# Ensure the new deal has a normalized rfp_deadline based on the SAM Response Due field
try:
    _due_raw = row.get('Response Due') or ""
    if _due_raw and deal_id:
        from datetime import datetime as _dt
        _due_norm = None
        try:
            _txt = str(_due_raw).split(" ")[0].replace(".", "/")
            for _fmt in ("%Y-%m-%d", "%m/%d/%Y", "%m/%d/%y"):
                try:
                    _dt_obj = _dt.strptime(_txt, _fmt)
                    _due_norm = _dt_obj.date().isoformat()
                    break
                except Exception:
                    continue
        except Exception:
            _due_norm = str(_due_raw)
        if _due_norm:
            with _closing(_db.cursor()) as _c2:
                _c2.execute(
                    "UPDATE deals SET rfp_deadline=? WHERE id=?;",
                    (_due_norm, deal_id),
                )
            _db.commit()
except Exception:
    pass
try:
    with _closing(_db.cursor()) as cur:
                                        cur.execute("INSERT INTO rfps(title, solnum, notice_id, sam_url, file_path, created_at) VALUES (?,?,?,?,?, datetime('now'));", (row.get('Title') or '', row.get('Solicitation') or '', row.get('Notice ID') or '', row.get('SAM Link') or '', ''))
                                        rfp_id = cur.lastrowid
                                        _db.commit()
                                    att_saved = 0
                                    try:
                                        for fname, fbytes in sam_try_fetch_attachments(str(row.get('Notice ID') or '')) or []:
                                            try:
                                                save_rfp_file_db(_db, rfp_id, fname, fbytes)
                                                att_saved += 1
                                            except Exception:
                                                pass
                                    except Exception:
                                        pass
                                except Exception:
                                    pass
                                st.success(f"Saved to Deals{' · ' + str(att_saved) + ' attachment(s) pulled' if att_saved else ''}")
                            except Exception as e:
                                st.error("Failed to save deal: %s" % (e,))
                            finally:
                                try:
                                    if _owned:
                                        _db.close()
                                except Exception:
                                    pass
                    with c5:
                        # Ask RFP Analyzer (restored)
                        try:
                            _render_ask_rfp_button(row.to_dict())  # expected to open the modal
                        except Exception:
                            if st.button('Ask RFP Analyzer 💬', key=f'ask_rfp_{i}'):
                                try:
                                    _ask_rfp_analyzer_modal(row.to_dict())
                                except Exception as _e:
                                    st.warning(f'Analyzer dialog unavailable: {_e}')
                                        # Push notice to Analyzer tab: create RFP, sync attachments, download, and open Analyzer
                    try:
                        tenant_id = get_current_tenant_id(conn)
                    except Exception:
                        tenant_id = 1
                    if st.button("AI attachments ▶", key=f"ai_att_{i}"):
                        # Normalize the notice dict so downstream helpers have context
                        try:
                            notice = row.to_dict()
                        except Exception:
                            notice = {}
                        notice_id = str(
                            (notice.get("notice_id")
                             or notice.get("Notice ID")
                             or notice.get("NoticeID")
                             or notice.get("id")
                             or "")
                        ).strip()
                        if not notice_id:
                            st.warning("Could not resolve a notice_id for this row; cannot fetch attachments.")
                        else:
                            # 1: fetch attachment metadata from SAM.gov
                            attachments = []
                            try:
                                if "fetch_sam_attachments_for_notice" in globals():
                                    attachments = fetch_sam_attachments_for_notice(notice)
                                else:
                                    # Fallback directly to sam_try_fetch_attachments by notice_id string
                                    tmp = []
                                    if "sam_try_fetch_attachments" in globals() and notice_id:
                                        try:
                                            tmp = list(sam_try_fetch_attachments(notice_id) or [])
                                        except Exception:
                                            tmp = []
                                    # Normalize any fallback tuples into metadata dicts
                                    for it in tmp:
                                        if isinstance(it, dict):
                                            name = it.get("name") or it.get("filename") or it.get("File Name") or "attachment"
                                            url = it.get("url") or it.get("URL") or it.get("link")
                                            mime = it.get("mime") or it.get("MimeType") or it.get("content_type")
                                            updated = (
                                                it.get("last_updated")
                                                or it.get("LastUpdated")
                                                or it.get("lastModified")
                                                or it.get("last_modified")
                                            )
                                            attachments.append(
                                                {
                                                    "file_name": str(name).strip() or "attachment",
                                                    "file_url": str(url).strip() if url else None,
                                                    "file_type": str(mime).strip() if mime else None,
                                                    "last_updated": str(updated) if updated else None,
                                                }
                                            )
                                        else:
                                            try:
                                                name, _blob = it
                                                attachments.append(
                                                    {
                                                        "file_name": str(name).strip() or "attachment",
                                                        "file_url": None,
                                                        "file_type": None,
                                                        "last_updated": None,
                                                    }
                                                )
                                            except Exception:
                                                continue
                            except Exception as _e:
                                st.warning(f"Attachment enumeration failed: {_e}")
                            # 2: sync metadata into sam_attachments
                            if attachments:
                                try:
                                    _sync_fn = globals().get("sync_sam_attachments_metadata")
                                    if _sync_fn is not None:
                                        _sync_fn(conn, tenant_id, notice_id, attachments)
                                    else:
                                        st.warning("Attachment metadata sync helper not available in this build.")
                                except Exception as _e:
                                    st.warning(f"Attachment metadata sync failed: {_e}")
                            # 3: create or get the RFP record
                            rfp_id = None
                            try:
                                rfp_id = get_or_create_rfp_from_notice(conn, tenant_id, notice)
                            except Exception:
                                try:
                                    rfp_id = _ensure_rfp_for_notice(conn, notice)
                                except Exception as _e:
                                    st.error(f"Could not create RFP for this notice: {_e}")
                            # Link this RFP into the deals pipeline
                            try:
                                _ensure_deal_for_notice_and_rfp(conn, tenant_id, notice, rfp_id)
                            except Exception:
                                # Keep SAM Watch resilient even if linking fails
                                pass

                            # 4: download files and link to RFP
                            linked = 0
                            if rfp_id:
                                try:
                                    # Prefer the unified RFP documents pipeline if available
                                    if "rfp_documents_fetch_from_sam" in globals():
                                        try:
                                            linked = int(rfp_documents_fetch_from_sam(conn, int(rfp_id), notice or notice_id) or 0)
                                        except Exception:
                                            linked = 0
                                    # Fallback for older builds: use sam_try_fetch_attachments + save_rfp_file_db
                                    if (not linked) and ("sam_try_fetch_attachments" in globals()):
                                        try:
                                            files = sam_try_fetch_attachments(str(notice_id)) or []
                                        except Exception:
                                            files = []
                                        for fname, fbytes in files:
                                            try:
                                                save_rfp_file_db(conn, int(rfp_id), fname, fbytes)
                                                linked += 1
                                            except Exception:
                                                continue
                                except Exception as _e:
                                    st.warning(f"Attachment download/linking error: {_e}")
                                # Run ingest/analyze so the One-Page view is ready when Analyzer opens.
                                try:
                                    _one_click_analyze(conn, int(rfp_id), (notice or {}).get("sam_url") or (notice or {}).get("url") or None)
                                except Exception as _e2:
                                    try:
                                        st.warning(f"Auto analyze skipped: {_e2}")
                                    except Exception:
                                        pass

                                # Hand off into RFP Analyzer with this notice as context
                                st.session_state["current_rfp_id"] = int(rfp_id)
                                st.session_state["rfp_selected_notice"] = notice
                                st.session_state["nav_target"] = "RFP Analyzer"
                                if linked:
                                    st.success(f"RFP #{rfp_id} ready. Linked {linked} attachment(s) from SAM.gov.")
                                else:
                                    st.info(f"RFP #{rfp_id} ready. No attachments were linked from SAM.gov.")
                                try:
                                    router("RFP Analyzer", conn); st.stop()
                                except Exception:
                                    try:
                                        st.rerun()
                                    except Exception:
                                        st.success("Sent to RFP Analyzer. Switch to that tab to continue.")
                            else:
                                # Could not create RFP; still try to route to Analyzer so user can work manually
                                st.info("Opening RFP Analyzer…")
                                try:
                                    router("RFP Analyzer", conn); st.stop()
                                except Exception:
                                    try:
                                        st.rerun()
                                    except Exception:
                                        st.info("Switch to RFP Analyzer tab to continue.")

            # Selected details panel


            # Bottom pager
            bp1, bp2, bp3 = st.columns([1, 3, 1])
            with bp1:
                if st.button("◀ Prev", key="sam_prev_btn_bottom", disabled=(cur_page <= 1)):
                    _sam_go(-1)
            with bp2:
                st.caption(f"Page {cur_page} of {total_pages} — showing {min(page_size, total - (cur_page - 1) * page_size)} of {total} results")
            with bp3:
                if st.button("Next ▶", key="sam_next_btn_bottom", disabled=(cur_page >= total_pages)):
                    _sam_go(1)
            st.divider()
            sel_idx = st.session_state.get("sam_selected_idx")
            if isinstance(sel_idx, int) and 0 <= sel_idx < len(results_df):
                row = results_df.iloc[sel_idx]
                with st.expander("Opportunity Details", expanded=True):
                    c1, c2 = st.columns([3,2])
                    with c1:
                        st.write(f"**Title:** {row.get('Title') or ''}")
                        st.write(f"**Solicitation:** {row.get('Solicitation') or '—'}")
                        st.write(f"**Type:** {row.get('Type') or '—'}")
                        st.write(f"**Set-Aside:** {row.get('Set-Aside') or '—'} ({row.get('Set-Aside Code') or '—'})")
                        st.write(f"**NAICS:** {row.get('NAICS') or '—'}  **PSC:** {row.get('PSC') or '—'}")
                        st.write(f"**Agency Path:** {row.get('Agency Path') or '—'}")
                    with c2:
                        st.write(f"**Posted:** {row.get('Posted') or '—'}")
                        st.write(f"**Response Due:** {row.get('Response Due') or '—'}")
                        st.write(f"**Notice ID:** {row.get('Notice ID') or '—'}")
                        if row.get('SAM Link'):
                            st.markdown(f"[Open in SAM]({row['SAM Link']})")

                        st.markdown("---")
                        st.subheader("CO / POC contact builder")
                        with st.form(f"co_contact_builder_{sel_idx}"):
                            co_col1, co_col2 = st.columns([2, 2])
                            with co_col1:
                                co_name = st.text_input("Name", value="", key=f"sam_co_name_{sel_idx}")
                                co_email = st.text_input("Email", value="", key=f"sam_co_email_{sel_idx}")
                            with co_col2:
                                default_org = row.get("Agency Path") or ""
                                co_phone = st.text_input("Phone", value="", key=f"sam_co_phone_{sel_idx}")
                                co_title = st.text_input("Title", value="", key=f"sam_co_title_{sel_idx}")
                            co_org = st.text_input("Organization", value=default_org, key=f"sam_co_org_{sel_idx}")
                            co_role = st.selectbox(
                                "Role",
                                ["Contracting Officer", "Contract Specialist", "Technical POC", "Other"],
                                key=f"sam_co_role_{sel_idx}",
                            )
                            submit_co = st.form_submit_button("Save CO/POC to contacts")
                            if submit_co:
                                from contextlib import closing as _closing
                                try:
                                    name_val = (co_name or "").strip()
                                    email_val = (co_email or "").strip()
                                    org_val = (co_org or default_org or "").strip()
                                    if not (name_val or email_val):
                                        st.error("Please provide at least a name or email.")
                                    else:
                                        owner = get_current_user_name()
                                        with _closing(conn.cursor()) as cur:
                                            contact_id = None
                                            if email_val:
                                                row_c = cur.execute(
                                                    "SELECT id FROM contacts WHERE email = ? AND (owner_user = ? OR owner_user IS NULL OR owner_user = '') LIMIT 1;",
                                                    (email_val, owner),
                                                ).fetchone()
                                                if row_c:
                                                    contact_id = row_c[0]
                                            if contact_id is None:
                                                cur.execute(
                                                    "INSERT INTO contacts(name, email, org, public_source, owner_user) VALUES (?, ?, ?, ?, ?);",
                                                    (name_val or None, email_val or None, org_val or None, 'sam_notice', owner),
                                                )
                                                contact_id = cur.lastrowid
                                            notice_id_val = (row.get('Notice ID') or '').strip()
                                            if notice_id_val and contact_id:
                                                cur.execute(
                                                    "INSERT OR IGNORE INTO notice_contacts(notice_id, contact_id, role, owner_user, created_at) VALUES (?, ?, ?, ?, datetime('now'));",
                                                    (notice_id_val, contact_id, co_role, owner),
                                                )
                                            conn.commit()
                                        st.success("Saved CO/POC contact and linked to this notice.")
                                except Exception as e:
                                    st.error(f"Failed to save CO/POC contact: {e}")

    # ---------- ALERTS TAB ----------



# === Cached SQL helpers for read-heavy analytics ===
try:
    import streamlit as _st_cache_sql  # type: ignore
    @_st_cache_sql.cache_data(show_spinner=False, ttl=600)
    def _cached_read_sql(db_path: str, sql: str, params: tuple):
        """Cached wrapper around pd.read_sql_query for read-heavy, read-only pages.

        This keeps the analytics pages (Top Buyers / Top Vendors / Knowledge Hub)
        responsive by reusing results for identical queries within a short window.
        """
        import sqlite3 as _sql3  # type: ignore
        import pandas as _pd3    # type: ignore
        try:
            conn2 = _sql3.connect(db_path, check_same_thread=False)
        except Exception:
            conn2 = _sql3.connect(db_path)
        try:
            return _pd3.read_sql_query(sql, conn2, params=params)
        finally:
            try:
                conn2.close()
            except Exception:
                pass
except Exception:
    def _cached_read_sql(db_path: str, sql: str, params: tuple):
        import sqlite3 as _sql3  # type: ignore
        import pandas as _pd3    # type: ignore
        try:
            conn2 = _sql3.connect(db_path, check_same_thread=False)
        except Exception:
            conn2 = _sql3.connect(db_path)
        try:
            return _pd3.read_sql_query(sql, conn2, params=params)
        finally:
            try:
                conn2.close()
            except Exception:
                pass

def run_top_buyers(conn: "sqlite3.Connection") -> None:
    """Public data analytics: Top buyers by agency and NAICS based on awards."""
    import pandas as pd

    st.header("Top Buyers")
    st.caption("Use this page to see which agencies are actually awarding work in your NAICS so you can focus outreach on real buyers.")

    # Ensure the analytics view exists even if migrations have not been run yet.
    try:
        with conn:
            conn.execute("""
                CREATE VIEW IF NOT EXISTS buyer_stats AS
                SELECT
                    n.tenant_id AS tenant_id,
                    COALESCE(n.agency, '') AS agency,
                    COALESCE(n.naics, '') AS naics,
                    COUNT(*) AS total_awards,
                    SUM(COALESCE(n.award_amount, 0.0)) AS total_value,
                    MAX(CASE WHEN n.award_date IS NOT NULL AND n.award_date <> '' THEN n.award_date ELSE NULL END) AS last_award_date
                FROM notices n
                JOIN current_tenant ct ON n.tenant_id = ct.ctid
                WHERE (n.award_date IS NOT NULL AND n.award_date <> '')
                   OR (n.award_amount IS NOT NULL AND n.award_amount <> 0)
                GROUP BY n.tenant_id, agency, naics;
            """)
    except Exception:
        # View creation problems should not block the page.
        pass

    filter_col1, filter_col2 = st.columns([2, 1])
    with filter_col1:
        naics_filter = st.text_input(
            "NAICS contains (optional)",
            key="buyer_stats_naics",
            help="Filter to a specific NAICS code or family, for example 561 or 561720.",
        )
    with filter_col2:
        sort_by = st.selectbox(
            "Sort by",
            ["Total value", "Total awards", "Last award date"],
            index=0,
            key="buyer_stats_sort",
        )

    base_sql = """
        SELECT agency,
               naics,
               total_awards,
               total_value,
               last_award_date
        FROM buyer_stats
    """
    where_clauses = []
    params: list[str] = []

    if naics_filter:
        where_clauses.append("naics LIKE ?")
        params.append(f"%{naics_filter.strip()}%")

    if where_clauses:
        base_sql += " WHERE " + " AND ".join(where_clauses)

    if sort_by == "Total awards":
        base_sql += " ORDER BY total_awards DESC, total_value DESC"
    elif sort_by == "Last award date":
        base_sql += " ORDER BY last_award_date DESC"
    else:
        base_sql += " ORDER BY total_value DESC"

    try:
        db_path = _db_path_from_conn(conn)
    except Exception:
        try:
            db_path = DB_PATH  # type: ignore[name-defined]
        except Exception:
            db_path = "./data/app.db"
    df = _cached_read_sql(db_path, base_sql, tuple(params))

    if df.empty:
        st.info("No awards found yet for this tenant. Try syncing SAM notices and ensuring award data is populated.")
        return

    # Summary metrics at the top
    total_awards = int(df["total_awards"].sum())
    total_value = float(df["total_value"].sum())
    distinct_buyers = int(df["agency"].nunique())

    m1, m2, m3 = st.columns(3)
    with m1:
        st.metric("Total awards (rows)", f"{total_awards:,}")
    with m2:
        st.metric("Total award value", f"${total_value:,.0f}")
    with m3:
        st.metric("Distinct buyer agencies", f"{distinct_buyers:,}")

    st.subheader("Buyer table")

    df_display = df.copy()
    df_display["total_value"] = df_display["total_value"].round(2)

    st.dataframe(
        df_display,
        use_container_width=True,
    )

    with st.expander("How to use this for DIY lead generation"):
        st.markdown(
            "- Start with the agencies with the highest awarded value in your target NAICS.\n"
            "- Use SAM Watch and your Outreach engine to build a focused contact list for those offices.\n"
            "- Track which buyers respond and convert them into deals in your CRM."
        )




def run_top_vendors(conn: "sqlite3.Connection") -> None:
    """Public data analytics: Top awardee vendors by NAICS based on awards."""
    import pandas as pd

    st.header("Top Vendors / Awardees")
    st.caption("Use this page to see which vendors are winning awards in your NAICS so you can analyze competitors and identify teaming partners.")

    # Ensure the analytics view exists even if migrations have not been run yet.
    try:
        with conn:
            conn.execute("""
                CREATE VIEW IF NOT EXISTS vendor_award_stats AS
                SELECT
                    n.tenant_id AS tenant_id,
                    COALESCE(n.awardee, '') AS vendor_name,
                    '' AS uei_duns,
                    COALESCE(n.naics, '') AS naics,
                    COUNT(*) AS total_awards,
                    SUM(COALESCE(n.award_amount, 0.0)) AS total_value,
                    MAX(CASE WHEN n.award_date IS NOT NULL AND n.award_date <> '' THEN n.award_date ELSE NULL END) AS last_award_date
                FROM notices n
                JOIN current_tenant ct ON n.tenant_id = ct.ctid
                WHERE (n.award_date IS NOT NULL AND n.award_date <> '')
                   OR (n.award_amount IS NOT NULL AND n.award_amount <> 0)
                GROUP BY n.tenant_id, vendor_name, uei_duns, naics;
            """)
    except Exception:
        pass

    naics_filter = st.text_input("NAICS contains (optional)", key="top_vendor_naics")
    name_filter = st.text_input("Vendor name contains (optional)", key="top_vendor_name")
    sort_by = st.selectbox(
        "Sort by",
        ["Total value", "Total awards", "Last award date", "Vendor name"],
        index=0,
        key="top_vendor_sort",
    )

    base_sql = """
        SELECT vendor_name,
               uei_duns,
               naics,
               total_awards,
               total_value,
               last_award_date
        FROM vendor_award_stats
    """
    where_clauses = []
    params: list[str] = []

    if naics_filter:
        where_clauses.append("naics LIKE ?")
        params.append(f"%{naics_filter.strip()}%")

    if name_filter:
        where_clauses.append("vendor_name LIKE ?")
        params.append(f"%{name_filter.strip()}%")

    if where_clauses:
        base_sql += " WHERE " + " AND ".join(where_clauses)

    if sort_by == "Total awards":
        base_sql += " ORDER BY total_awards DESC, total_value DESC"
    elif sort_by == "Last award date":
        base_sql += " ORDER BY last_award_date DESC"
    elif sort_by == "Vendor name":
        base_sql += " ORDER BY vendor_name ASC"
    else:
        base_sql += " ORDER BY total_value DESC"

    try:
        db_path = _db_path_from_conn(conn)
    except Exception:
        try:
            db_path = DB_PATH  # type: ignore[name-defined]
        except Exception:
            db_path = "./data/app.db"
    df = _cached_read_sql(db_path, base_sql, tuple(params))

    if df is None or df.empty:
        st.info("No awarded vendors found yet for this tenant. Try syncing SAM notices and ensuring award data is populated.")
        return

    total_awards = int(df["total_awards"].sum())
    total_value = float(df["total_value"].sum())
    distinct_vendors = int(df["vendor_name"].nunique())

    m1, m2, m3 = st.columns(3)
    with m1:
        st.metric("Total awards (rows)", f"{total_awards:,}")
    with m2:
        st.metric("Total award value", f"${total_value:,.0f}")
    with m3:
        st.metric("Distinct vendors", f"{distinct_vendors:,}")

    st.dataframe(df, use_container_width=True)

    with st.expander("How to use this for competitor and partner analysis", expanded=False):
        st.markdown(
            "- Focus on vendors with the highest total awarded value in your core NAICS.\n"
            "- Treat them as competitors to study and potential primes to team under.\n"
            "- Use Subcontractor Finder and Outreach to pull contact info and begin conversations."
        )


def run_research_tab(conn: "sqlite3.Connection") -> None:
    st.header("Research (FAR/DFARS/Wage/NAICS)")
    url = st.text_input("URL", placeholder="https://www.acquisition.gov/")
    ttl = st.number_input("Cache TTL (hours)", min_value=1, max_value=168, value=24, step=1)
    q = st.text_input("Highlight phrase (optional)")
    if st.button("Fetch", type="primary", key="research_fetch_btn"):
        with st.spinner("Fetching"):
            rec = research_fetch(url.strip(), ttl_hours=int(ttl))
        if rec.get("status", 0) != 200 and not rec.get("cached"):
            st.error(f"Fetch failed or not cached. Status {rec.get('status')} — {rec.get('error','')}")
        else:
            st.success(("Loaded from cache" if rec.get("cached") else "Fetched") + f" — status {rec.get('status')}")
            txt = rec.get("text","")
            ex = research_extract_excerpt(txt, q or "")
            st.text_area("Excerpt", value=ex, height=240)
            if rec.get("path"):
                st.markdown(f"[Open cached text]({rec['path']})")
    st.caption("Shortcuts: FAR | DFARS | Wage Determinations | NAICS | SBA Size Standards")

# === Phase 3 helpers (injected) ===
import datetime as _dt, hashlib, re as _re

def _p3_insert_or_skip_file(conn, rfp_id: int, filename: str, blob: bytes, mime: str | None = None):
    """
    Lightweight helper used by One-Page RFP ingest jobs.

    It delegates to save_rfp_file_db so we reuse the standard
    deduplication + metadata + text-extraction pipeline and avoid
    SQL placeholder mismatches.
    """
    try:
        # save_rfp_file_db performs sha256-based deduplication and stores
        # the bytes and page count in rfp_files, updating existing rows
        # when appropriate.
        save_rfp_file_db(conn, int(rfp_id), filename, blob)
    except Exception:
        try:
            logger.exception("_p3_insert_or_skip_file failed")
        except Exception:
            # If logger is unavailable for any reason, fail silently so
            # the ingest job can continue processing other files.
            pass


def _p3_rfp_meta_get(conn, rfp_id: int, key: str, default: str = "") -> str:
    try:
        import pandas as _pd
        df = _pd.__p_read_sql_query("SELECT value FROM rfp_meta WHERE rfp_id=? AND key=?", conn, params=(int(rfp_id), str(key)))
        if not df.empty:
            return str(df.iloc[0]["value"] or "")
    except Exception:
        pass
    return default

def _p3_rfp_meta_set(conn, rfp_id: int, key: str, value: str):
    from contextlib import closing as _closing
    with _closing(conn.cursor()) as cur:
        try:
            cur.execute("DELETE FROM rfp_meta WHERE rfp_id=? AND key=?", (int(rfp_id), str(key)))
        except Exception:
            pass
        try:
            cur.execute("INSERT INTO rfp_meta(rfp_id, key, value) VALUES(?,?,?)", (int(rfp_id), str(key), str(value)))
        except Exception:
            pass
        conn.commit()

def _p3_parse_notice_id(s: str) -> str:
    if not s: return ""
    m = _re.search(r"NoticeId=([A-Za-z0-9\-]+)", s)
    if m: return m.group(1)
    if _re.fullmatch(r"[A-Za-z0-9\-]{8,}", s):
        return s
    return ""

def _p3_check_sam_updates(conn, rfp_id: int) -> dict:
    results = {"new_files": 0, "attempted": 0, "errors": []}
    try:
        import pandas as _pd
        nid = _p3_rfp_meta_get(conn, rfp_id, "notice_id", "")
        if not nid:
            df = _pd.__p_read_sql_query("SELECT sam_url FROM rfps WHERE id=?", conn, params=(int(rfp_id),))
            if not df.empty:
                nid = _p3_parse_notice_id(str(df.iloc[0]["sam_url"] or ""))
    except Exception as e:
        results["errors"].append(f"lookup error: {e}")
        nid = ""
    fetcher = globals().get("sam_try_fetch_attachments") or globals().get("sam_fetch_attachments") or None
    if not nid:
        results["errors"].append("No SAM Notice ID/URL found.")
        return results
    if not fetcher:
        results["errors"].append("SAM fetcher not available in this build.")
        return results
    try:
        for (fname, data) in fetcher(nid) or []:
            results["attempted"] += 1
            try:
                _p3_insert_or_skip_file(conn, rfp_id, fname, data, None)
                results["new_files"] += 1
            except Exception as e:
                results["errors"].append(f"{fname}: {e}")
    except Exception as e:
        results["errors"].append(f"fetch error: {e}")
    try:
        if "y1_index_rfp" in globals():
            globals()["y1_index_rfp"](conn, int(rfp_id), rebuild=False)
    except Exception:
        pass
    try:
        if "find_section_M" in globals():
            globals()["find_section_M"](conn, int(rfp_id))
    except Exception:
        pass
    _p3_rfp_meta_set(conn, rfp_id, "last_sam_check", _dt.datetime.utcnow().isoformat(timespec="seconds") + "Z")
    return results

def _p3_ensure_deal_and_contacts(conn, rfp_id: int):
    from contextlib import closing as _closing
    import pandas as _pd
    # Deal
    try:
        df = _pd.__p_read_sql_query("SELECT title FROM rfps WHERE id=?", conn, params=(int(rfp_id),))
        title = (df.iloc[0]["title"] if not df.empty else f"RFP #{rfp_id}")
    except Exception:
        title = f"RFP #{rfp_id}"
    try:
        with _closing(conn.cursor()) as cur:
            cur.execute(
                "INSERT OR IGNORE INTO deals(rfp_id, title, stage, created_at) "
                "VALUES(?, ?, COALESCE((SELECT stage, rfp_deadline FROM deals WHERE rfp_id=?), 'No contact made'), datetime('now'))",
                (int(rfp_id), str(title), int(rfp_id))
            )
            # Ensure new or existing deal record for this RFP is owned by the current logical user
            owner_name = get_current_user_name()
            cur.execute(
                "UPDATE deals SET owner = COALESCE(owner, ?), owner_user = COALESCE(owner_user, ?) "
                "WHERE rfp_id=?;",
                (owner_name, owner_name, int(rfp_id)),
            )
            conn.commit()
    except Exception:
        pass
    # Contacts
    try:
        dfp = _pd.__p_read_sql_query("SELECT name, email, phone, title AS job_title, agency FROM pocs WHERE rfp_id=?", conn, params=(int(rfp_id),))
    except Exception:
        dfp = None
    if dfp is not None and not dfp.empty:
        for _, row in dfp.iterrows():
            name = row.get("name") or ""
            email = row.get("email") or ""
            phone = row.get("phone") or ""
            job = row.get("job_title") or ""
            agency = row.get("agency") or ""
            try:
                with _closing(conn.cursor()) as cur:
                    cur.execute(
                        "INSERT OR IGNORE INTO contacts(name, email, phone, title, organization, created_at) "
                        "VALUES (?, ?, ?, ?, ?,?, datetime('now'))",
                        (str(name), str(email), str(phone), str(job), str(agency))
                    )
                    conn.commit()
            except Exception:
                pass

def _p3_due_date_for_rfp(conn, rfp_id: int) -> str:
    try:
        import pandas as _pd
        df = _pd.__p_read_sql_query(
            "SELECT value FROM key_dates WHERE rfp_id=? AND (label LIKE '%Due%' OR '%Close%') "
            "ORDER BY id DESC LIMIT 1;",
            conn, params=(int(rfp_id),)
        )
        if not df.empty:
            return str(df.iloc[0]["value"] or "")
    except Exception:
        pass
    return _p3_rfp_meta_get(conn, rfp_id, "proposal_due", "")

# --- P3 auto wiring helpers: Deals/Contacts/Tasks from RFP ---
def _p3_get_deal_id_for_rfp(conn, rfp_id: int):
    try:
        import pandas as _pd
        df = _pd.read_sql_query("SELECT id, rfp_deadline FROM deals WHERE rfp_id=?", conn, params=(int(rfp_id),))
        if not df.empty:
            return int(df.iloc[0]["id"])
    except Exception:
        pass
    return None

def _p3_auto_stage_for_rfp(conn, rfp_id: int):
    from contextlib import closing as _closing
    import pandas as _pd
    # Simple rule set
    has_clins = False
    has_dates = False
    has_pocs = False
    try:
        has_clins = _pd.read_sql_query("SELECT 1 FROM clin_lines WHERE rfp_id=? LIMIT 1;", conn, params=(int(rfp_id),)).shape[0] > 0
    except Exception:
        pass
    try:
        has_dates = _pd.read_sql_query("SELECT 1 FROM key_dates WHERE rfp_id=? LIMIT 1;", conn, params=(int(rfp_id),)).shape[0] > 0
    except Exception:
        pass
    try:
        has_pocs = _pd.read_sql_query("SELECT 1 FROM pocs WHERE rfp_id=? LIMIT 1;", conn, params=(int(rfp_id),)).shape[0] > 0
    except Exception:
        pass
    new_stage = "No contact made"
    if has_clins or has_dates:
        new_stage = "Quote"
    elif has_pocs:
        new_stage = "co contacted"
    deal_id = _p3_get_deal_id_for_rfp(conn, rfp_id)
    if deal_id is None:
        return
    try:
        # Fetch current stage
        df = _pd.read_sql_query("SELECT stage, status, rfp_deadline FROM deals WHERE id=?", conn, params=(int(deal_id),))
        cur_stage = str(df.iloc[0]["stage"] or "") if not df.empty else ""
    except Exception:
        cur_stage = ""
    if cur_stage != new_stage:
        with _closing(conn.cursor()) as cur:
            cur.execute("UPDATE deals SET stage=?, status=?, updated_at=datetime('now'), last_stage_change_date=datetime('now'), first_entered_stage_date=COALESCE(first_entered_stage_date, datetime('now')) WHERE id=?;", (new_stage, new_stage, int(deal_id)))
            cur.execute("INSERT INTO deal_stage_log(deal_id, stage, changed_at) VALUES(?, ?, datetime('now'));", (int(deal_id), new_stage))
            conn.commit()

    # Create a "Proposal due" task if we have any due date in key_dates but no such open task
    try:
        df_due = _pd.read_sql_query("SELECT COALESCE(date_iso, date_text) AS d FROM key_dates WHERE rfp_id=? AND (LOWER(label) LIKE '%due%' OR LOWER(label) LIKE '%close%') ORDER BY id DESC LIMIT 1;", conn, params=(int(rfp_id),))
        if not df_due.empty:
            deal_id = _p3_get_deal_id_for_rfp(conn, rfp_id)
            if deal_id:
                df_task = _pd.read_sql_query("SELECT 1 FROM tasks WHERE deal_id=? AND title LIKE 'Proposal due%';", conn, params=(int(deal_id),))
                
                if df_task.empty:
                    from contextlib import closing as _closing
                    # Assign proposal due task to the deal owner when possible
                    owner_for_task = "System"
                    try:
                        with _closing(conn.cursor()) as cur2:
                            cur2.execute("SELECT COALESCE(owner_user, owner) FROM deals WHERE id=?;", (int(deal_id),))
                            row_owner = cur2.fetchone()
                        if row_owner and (row_owner[0] or "").strip():
                            owner_for_task = str(row_owner[0]).strip()
                    except Exception:
                        pass
                    with _closing(conn.cursor()) as cur:
                        cur.execute(
                            "INSERT INTO tasks(owner_user, title, due_date, status, priority, deal_id, contact_id, created_at) "
                            "VALUES (?, ?, ?, ?, ?, ?, ?, datetime('now'));",
                            (
                                owner_for_task,
                                "Proposal due",
                                str(df_due.iloc[0]['d'] or ''),
                                "Open",
                                "High",
                                int(deal_id),
                                None,
                            ),
                        )
                        conn.commit()
    except Exception:
        pass

def _p3_auto_wire_crm_from_rfp(conn, rfp_id: int):
    try:
        if _p3_get_deal_id_for_rfp(conn, rfp_id) is None:
            _p3_ensure_deal_and_contacts(conn, int(rfp_id))
        _p3_auto_stage_for_rfp(conn, int(rfp_id))
    except Exception:
        pass
def _p3_make_ics(summary: str, when_str: str) -> bytes:
    from datetime import datetime
    dt = None
    for fmt in ("%Y-%m-%d %H:%M", "%Y-%m-%d", "%m/%d/%Y %H:%M", "%m/%d/%Y", "%b %d, %Y", "%B %d, %Y"):
        try:
            dt = datetime.strptime(when_str.strip(), fmt)
            break
        except Exception:
            continue
    if not dt:
        dt = datetime.utcnow().replace(hour=12, minute=0, second=0, microsecond=0) + _dt.timedelta(days=1)
    dt_utc = dt.strftime("%Y%m%dT%H%M%SZ")
    uid = f"ela-{int(_dt.datetime.utcnow().timestamp())}@local"
    ics = "BEGIN:VCALENDAR\nVERSION:2.0\nPRODID:-//ELA//RFP Due Date//EN\nBEGIN:VEVENT\nUID:" + uid +           "\nDTSTAMP:" + dt_utc + "\nDTSTART:" + dt_utc + "\nSUMMARY:" + summary + "\nEND:VEVENT\nEND:VCALENDAR\n"
    return ics.encode("utf-8")


def _compliance_progress(df_items: pd.DataFrame) -> int:
    if df_items is None or df_items.empty:
        return 0
    done = int((df_items["status"]=="Complete").sum())
    total = int(len(df_items))
    return int(round(done / max(1, total) * 100))

def _load_compliance_matrix(conn: "sqlite3.Connection", rfp_id: int) -> pd.DataFrame:
    """
    Robust loader:
      1) If tenancy views exist (lm_items_t/lm_meta_t), use them.
      2) Else if base tables exist (lm_items/lm_meta), use them.
      3) Else return lm_items-only with blank meta columns.
    """
    # Ensure lm_meta exists (no-op if already there)
    try:
        with closing(conn.cursor()) as c:
            c.execute("CREATE TABLE IF NOT EXISTS lm_meta(\n"
                      " id INTEGER PRIMARY KEY,\n"
                      " lm_id INTEGER REFERENCES lm_items(id) ON DELETE CASCADE,\n"
                      " owner TEXT, ref_page TEXT, ref_para TEXT, evidence TEXT, risk TEXT, notes TEXT\n"
                      ");")
            c.execute("CREATE UNIQUE INDEX IF NOT EXISTS uq_lm_meta_lm ON lm_meta(lm_id);")
            conn.commit()
    except Exception:
        pass

    def _has(name: str) -> bool:
        try:
            q = "SELECT name FROM sqlite_master WHERE name=?;"
            return pd.read_sql_query(q, conn, params=(name,)).shape[0] > 0
        except Exception:
            return False

    use_views = _has("lm_items_t")
    use_meta_view = _has("lm_meta_t")

    if use_views and use_meta_view:
        q = """
            SELECT i.id AS lm_id, i.item_text, i.is_must, i.status,
                   COALESCE(m.owner,'') AS owner,
                   COALESCE(m.ref_page,'') AS ref_page,
                   COALESCE(m.ref_para,'') AS ref_para,
                   COALESCE(m.evidence,'') AS evidence,
                   COALESCE(m.risk,'Green') AS risk,
                   COALESCE(m.notes,'') AS notes
            FROM lm_items_t i
            LEFT JOIN lm_meta_t m ON m.lm_id = i.id
            WHERE i.rfp_id = ?
            ORDER BY i.id;
        """
        try:
            return pd.read_sql_query(q, conn, params=(rfp_id,))
        except Exception:
            pass  # fall through

    # Base tables path (works even if lm_meta is empty)
    if _has("lm_items"):
        # Only join lm_meta if it truly exists (older DBs may lack it)
        if _has("lm_meta"):
            q = """
                SELECT i.id AS lm_id, i.item_text, i.is_must, i.status,
                       COALESCE(m.owner,'') AS owner,
                       COALESCE(m.ref_page,'') AS ref_page,
                       COALESCE(m.ref_para,'') AS ref_para,
                       COALESCE(m.evidence,'') AS evidence,
                       COALESCE(m.risk,'Green') AS risk,
                       COALESCE(m.notes,'') AS notes
                FROM lm_items i
                LEFT JOIN lm_meta m ON m.lm_id = i.id
                WHERE i.rfp_id = ?
                ORDER BY i.id;
            """
        else:
            q = """
                SELECT i.id AS lm_id, i.item_text, i.is_must, i.status,
                       '' AS owner, '' AS ref_page, '' AS ref_para, '' AS evidence, 'Green' AS risk, '' AS notes
                FROM lm_items i
                WHERE i.rfp_id = ?
                ORDER BY i.id;
            """
        try:
            return pd.read_sql_query(q, conn, params=(rfp_id,))
        except Exception:
            pass

    # Final fallback: empty frame with expected columns
    cols = ["lm_id","item_text","is_must","status","owner","ref_page","ref_para","evidence","risk","notes"]
    return pd.DataFrame(columns=cols)

def _compliance_flags(ctx: dict, df_items: pd.DataFrame) -> pd.DataFrame:
    rows = []
    sections = ctx.get("sections", pd.DataFrame())
    text_all = " ".join((sections["content"].tolist() if isinstance(sections, pd.DataFrame) and not sections.empty else []))
    tl = text_all.lower()

    m = re.search(r'(?:page\s+limit|not\s+exceed)\s+(?:of\s+)?(\d{1,3})\s+pages?', tl)
    if m: rows.append({"Rule":"Page Limit","Detail":f"Limit {m.group(1)} pages detected","Severity":"Amber"})
    if re.search(r'(font|typeface).{0,20}(size|pt).{0,5}(10|11)', tl):
        rows.append({"Rule":"Font size","Detail":"Minimum font size 10/11pt likely required","Severity":"Amber"})
    if re.search(r'margin[s]?\s+(?:of|at\s+least)\s+\d', tl):
        rows.append({"Rule":"Margins","Detail":"Specific margin requirements detected","Severity":"Amber"})
    if re.search(r'volume[s]?\s+(i{1,3}|iv|v|technical|price)', tl):
        rows.append({"Rule":"Volumes","Detail":"Multiple volumes required","Severity":"Amber"})
    if re.search(r'(sam\.gov|piee|wawf|email submission|portal)', tl):
        rows.append({"Rule":"Submission portal","Detail":"Specific portal/email submission detected","Severity":"Amber"})

    dates = ctx.get("dates", pd.DataFrame())
    if isinstance(dates, pd.DataFrame) and not dates.empty:
        due = dates[dates["label"].str.contains("due", case=False, na=False)]
        if not due.empty:
            dt = pd.to_datetime(due.iloc[0]["date_text"], errors="coerce")
            if pd.notnull(dt):
                days = (pd.Timestamp(dt) - pd.Timestamp.utcnow()).days
                if days <= 3: rows.append({"Rule":"Timeline","Detail":f"Proposals due in {days} day(s)","Severity":"Red"})
                elif days <= 7: rows.append({"Rule":"Timeline","Detail":f"Proposals due in {days} days","Severity":"Amber"})

    if isinstance(df_items, pd.DataFrame) and not df_items.empty:
        open_musts = df_items[(df_items["is_must"]==1) & (df_items["status"]!="Complete")]
        if not open_musts.empty:
            rows.append({"Rule":"Open MUST items","Detail":f"{len(open_musts)} mandatory items still open","Severity":"Red"})

    return pd.DataFrame(rows)

def _load_rfp_context_struct(conn: "sqlite3.Connection", rfp_id: int) -> dict:
    try:
        rf = pd.read_sql_query("SELECT id, title, solnum, sam_url, created_at FROM rfps WHERE id=?;", conn, params=(int(rfp_id),))
    except Exception:
        rf = pd.DataFrame()
    try:
        df_items = safe_read_sql(conn, "SELECT id, item_text, is_must, status FROM lm_items WHERE rfp_id=? ORDER BY id;", (int(rfp_id),))
    except Exception:
        df_items = pd.DataFrame(columns=["id","item_text","is_must","status"])
    joined = "\n".join(df_items["item_text"].astype(str).tolist()) if not df_items.empty else ""
    sections = pd.DataFrame([{"name":"Checklist Items","content": joined}])
    meta = rf.iloc[0].to_dict() if not rf.empty else {}
    return {"rfp": rf, "sections": sections, "items": df_items, "meta": meta}

def run_lm_checklist(conn: "sqlite3.Connection") -> None:

    st.header("L and M Checklist")
    rfp_id = st.session_state.get('current_rfp_id')
    if not rfp_id:
        try:
            df_rf = safe_read_sql(conn, "SELECT id, title, solnum, created_at FROM rfps_t ORDER BY id DESC;", ())
        except Exception as e:
            st.error(f"Failed to load RFPs: {e}")
            return
        if df_rf.empty:
            st.info("No saved RFP extractions yet. Use RFP Analyzer to parse and save.")
            return
        opt = st.selectbox("Select an RFP context", options=df_rf['id'].tolist(),
                           format_func=lambda rid: f"#{rid} — {df_rf.loc[df_rf['id']==rid,'title'].values[0] or 'Untitled'}")
        rfp_id = opt
        st.session_state['current_rfp_id'] = rfp_id

    st.caption(f"Working RFP ID: {rfp_id}")
    try:
        df_items = safe_read_sql(conn, "SELECT id, item_text, is_must, status FROM lm_items WHERE rfp_id=?;", (rfp_id,))
    except Exception as e:
        st.error(f"Failed to load items: {e}")
        return
    if df_items.empty:
        st.info("No L/M items found for this RFP.")
        return

    pct = _compliance_progress(df_items)
    st.progress(pct/100.0, text=f"{pct}% complete")

    c1, c2, c3 = st.columns([2,2,2])
    with c1:
        if st.button("Mark all Complete"):
            try:
                with closing(conn.cursor()) as cur:
                    cur.execute("UPDATE lm_items SET status='Complete' WHERE rfp_id=?;", (rfp_id,))
                    conn.commit()
                st.success("All items marked Complete")
            except Exception as e:
                st.error(f"Update failed: {e}")
    with c2:
        if st.button("Reset all to Open"):
            try:
                with closing(conn.cursor()) as cur:
                    cur.execute("UPDATE lm_items SET status='Open' WHERE rfp_id=?;", (rfp_id,))
                    conn.commit()
                st.success("All items reset")
            except Exception as e:
                st.error(f"Update failed: {e}")
    with c3:
        if st.button("Mark all MUST to Open"):
            try:
                with closing(conn.cursor()) as cur:
                    cur.execute("UPDATE lm_items SET status='Open' WHERE rfp_id=? AND is_must=1;", (rfp_id,))
                    conn.commit()
                st.success("All MUST items set to Open")
            except Exception as e:
                st.error(f"Update failed: {e}")

    st.subheader("Checklist")
    for _, row in df_items.iterrows():
        key = f"lm_{row['id']}"
        label = ("[MUST] " if row['is_must']==1 else "") + row['item_text']
        checked = st.checkbox(label, value=(row['status']=='Complete'), key=key)
        new_status = 'Complete' if checked else 'Open'
        if new_status != row['status']:
            try:
                with closing(conn.cursor()) as cur:
                    cur.execute("UPDATE lm_items SET status=? WHERE id=?;", (new_status, int(row['id'])))
                    conn.commit()
            except Exception as e:
                st.error(f"Failed to update item {row['id']}: {e}")

    st.divider()

    st.subheader("Compliance Matrix")
    df_mx = _load_compliance_matrix(conn, int(rfp_id))
    if df_mx.empty:
        st.info("No items to show.")
        return

    view = df_mx.rename(columns={
        "item_text":"Requirement","is_must":"Must?","status":"Status",
        "owner":"Owner","ref_page":"Page","ref_para":"Para",
        "evidence":"Evidence/Link","risk":"Risk","notes":"Notes"
    })
    _styled_dataframe(view[["Requirement","Must?","Status","Owner","Page","Para","Evidence/Link","Risk","Notes"]],
                 use_container_width=True, hide_index=True)

    st.markdown("**Edit selected requirement**")
    pick = st.selectbox("Requirement", options=df_mx["lm_id"].tolist(),
                        format_func=lambda lid: f"#{lid} — {df_mx.loc[df_mx['lm_id']==lid,'item_text'].values[0][:80]}")

    rec = df_mx[df_mx["lm_id"]==pick].iloc[0].to_dict()
    e1, e2, e3, e4 = st.columns([2,1,1,1])
    with e1:
        owner = st.text_input("Owner", value=rec.get("owner",""), key=f"mx_owner_{pick}")
        notes = st.text_area("Notes", value=rec.get("notes",""), key=f"mx_notes_{pick}", height=90)
    with e2:
        page = st.text_input("Page", value=rec.get("ref_page",""), key=f"mx_page_{pick}")
        para = st.text_input("Paragraph", value=rec.get("ref_para",""), key=f"mx_para_{pick}")
    with e3:
        risk = st.selectbox("Risk", ["Green","Yellow","Red"],
                            index=["Green","Yellow","Red"].index(rec.get("risk","Green")), key=f"mx_risk_{pick}")
    with e4:
        evidence = st.text_input("Evidence/Link", value=rec.get("evidence",""), key=f"mx_evid_{pick}")

    csave, cexp = st.columns([2,2])
    with csave:
        if st.button("Save Matrix Row", key=f"mx_save_{pick}"):
            try:
                with closing(conn.cursor()) as cur:
                    cur.execute("""
                        INSERT INTO lm_meta(lm_id, owner, ref_page, ref_para, evidence, risk, notes)
                        VALUES (?, ?, ?, ?, ?,?,?,?)
                        ON CONFLICT(lm_id) DO UPDATE SET
                            owner=excluded.owner, ref_page=excluded.ref_page, ref_para=excluded.ref_para,
                            evidence=excluded.evidence, risk=excluded.risk, notes=excluded.notes;
                    """, (int(pick), owner.strip(), page.strip(), para.strip(), evidence.strip(), risk, notes.strip()))
                    conn.commit()
                st.success("Saved"); st.rerun()
            except Exception as e2:
                st.error(f"Save failed: {e2}")
    with cexp:
        if st.button("Export Matrix CSV", key="mx_export"):
            out = view.copy()
            path = str(Path(DATA_DIR) / f"compliance_matrix_rfp_{int(rfp_id)}.csv")
            out.to_csv(path, index=False)
            try:
                with open(path, "rb") as _f:
                    st.success("Exported")
                    st.download_button(
                        "Download CSV",
                        data=_f.read(),
                        file_name=Path(path).name,
                        mime="text/csv",
                        key="mx_export_download",
                    )
            except Exception:
                st.info(f"CSV saved at {path}")

    st.subheader("Red-Flag Finder")
    ctx = _load_rfp_context_struct(conn, int(rfp_id))
    flags = _compliance_flags(ctx, df_items)
    if flags is None or flags.empty:
        st.write("No obvious flags detected.")
    else:
        _styled_dataframe(flags, use_container_width=True, hide_index=True)

    try:
        _rid = locals().get('rfp_id') or locals().get('rid') or st.session_state.get('current_rfp_id')
        y6_render_co_box(conn if 'conn' in locals() else None, _rid, key_prefix="run_lm_checklist_y6", title="Ask the CO about L&M")
    except Exception:
        pass

def _estimate_pages(total_words: int, spacing: str = "1.15", words_per_page: Optional[int] = None) -> float:
    """Rough page estimate at common spacings for 11pt fonts."""
    if words_per_page is None:
        s = (spacing or "1.15").strip().lower()
        if s in {"1", "1.0", "single"}:
            wpp = 500
        elif s in {"1.15", "1,15"}:
            wpp = 400
        elif s in {"1.5", "1,5"}:
            wpp = 300
        elif s in {"double", "2", "2.0"}:
            wpp = 250
        else:
            wpp = 400
    else:
        wpp = max(50, int(words_per_page))
    return round((total_words or 0) / float(wpp), 2)

def _export_docx(
    path: str,
    doc_title: str | None = None,
    sections=None,
    clins=None,
    checklist=None,
    metadata=None,
    font_name: str = "Calibri",
    font_size_pt: int = 11,
    spacing: str | float | int = "1.15",
    **kwargs,
) -> str | None:
    """
    Build a proposal DOCX that looks clean and professional:
    - Comfortable line spacing (default ~1.15)
    - Small space between paragraphs
    - Clear spacing before/after headings
    - Real bullets and tables
    """

    try:
        from docx import Document  # type: ignore
        from docx.shared import Pt, Inches, RGBColor  # type: ignore
        from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING  # type: ignore
        from docx.oxml import OxmlElement  # type: ignore
        from docx.oxml.ns import qn  # type: ignore
    except Exception as e:
        try:
            st.error(f"DOCX export unavailable: {e}")
        except Exception:
            pass
        return None

    # Normalize inputs
    def _as_rows(obj):
        if obj is None:
            return []
        if hasattr(obj, "to_dict"):
            try:
                return list(obj.to_dict("records"))
            except Exception:
                pass
        if isinstance(obj, dict):
            return [obj]
        try:
            return list(obj)
        except Exception:
            return [obj]

    sections = _as_rows(sections)
    clins = _as_rows(clins)
    checklist = _as_rows(checklist)
    metadata = dict(metadata or {})

    # Robust spacing handling to support values like "Single", "1.15", "Double"
    def _coerce_spacing(spacing_val) -> float:
        default_ls = 1.15
        if spacing_val is None:
            return default_ls
        if isinstance(spacing_val, (int, float)):
            v = float(spacing_val)
            return v if v > 0 else default_ls
        s = str(spacing_val).strip()
        if not s:
            return default_ls
        # try direct float
        try:
            return float(s.replace(",", "."))
        except Exception:
            pass
        s_low = s.lower()
        if s_low in ("single", "1", "1.0", "1,0"):
            return 1.0
        if s_low in ("1.15", "1,15"):
            return 1.15
        if s_low in ("1.5", "1,5"):
            return 1.5
        if s_low in ("double", "2", "2.0", "2,0"):
            return 2.0
        return default_ls

    line_spacing = _coerce_spacing(spacing)
    if line_spacing <= 0:
        line_spacing = 1.15

    font_name = font_name or "Calibri"
    font_size_pt = int(font_size_pt or 11)

    import re as _re

    # Paragraph styling for normal text and bullets
    def _style_paragraph(p, *, is_heading: bool = False, is_list: bool = False):
        fmt = p.paragraph_format
        # Vertical spacing
        if is_heading:
            fmt.space_before = Pt(12)
            fmt.space_after = Pt(6)
        else:
            fmt.space_before = Pt(0)
            fmt.space_after = Pt(6)  # small gap between paragraphs for readability

        # Line spacing
        if line_spacing <= 1.05:
            fmt.line_spacing_rule = WD_LINE_SPACING.SINGLE
            fmt.line_spacing = None
        else:
            fmt.line_spacing_rule = WD_LINE_SPACING.MULTIPLE
            fmt.line_spacing = line_spacing

        # Indentation: indent bullets slightly, normal paragraphs flush left
        if is_list:
            fmt.left_indent = Inches(0.25)
            fmt.first_line_indent = Inches(0)
        else:
            fmt.left_indent = Inches(0)
            fmt.first_line_indent = Inches(0)

        p.alignment = WD_ALIGN_PARAGRAPH.LEFT

    def _likely_json_line(txt: str) -> bool:
        s = (txt or "").strip()
        if not s:
            return False
        if not (s.startswith("{") or s.startswith("[")):
            return False
        if len(s) < 4:
            return False
        if ":" not in s:
            return False
        if any(ch in s for ch in ("analysis", "tool_calls", "choices")):
            return True
        return False

    def _add_inline_runs(doc, text: str):
        """Add a normal paragraph or bullet, honoring simple markdown (**bold**, _italics_)."""

        raw = text or ""

        # Detect bullet line
        m_bullet = _re.match(r"^\s*[-*\u2022]\s+(.*)", raw)
        is_bullet = bool(m_bullet)
        if is_bullet:
            raw = m_bullet.group(1)

        try:
            if is_bullet:
                p = doc.add_paragraph(style="List Bullet")
            else:
                p = doc.add_paragraph()
        except Exception:
            p = doc.add_paragraph()

        _style_paragraph(p, is_heading=False, is_list=is_bullet)

        # Split into markdown style segments
        pattern = r"(\*\*[^*]+\*\*|_[^_]+_|`[^`]+`|\[[^\]]+\]\([^)]+\))"
        parts = _re.split(pattern, raw)

        for seg in parts:
            if not seg:
                continue

            if seg.startswith("**") and seg.endswith("**"):
                txt = seg[2:-2]
                run = p.add_run(txt)
                run.bold = True
            elif seg.startswith("_") and seg.endswith("_"):
                txt = seg[1:-1]
                run = p.add_run(txt)
                run.italic = True
            elif seg.startswith("`") and seg.endswith("`"):
                txt = seg[1:-1]
                run = p.add_run(txt)
                run.font.name = "Consolas"
                run.font.size = Pt(font_size_pt)
            elif seg.startswith("[") and "](" in seg and seg.endswith(")"):
                label = seg[1 : seg.index("](")]
                run = p.add_run(label)
                run.underline = True
            else:
                run = p.add_run(seg)

            # Enforce font and color
            f = run.font
            f.name = font_name
            f.size = Pt(font_size_pt)
            try:
                f.color.rgb = RGBColor(0, 0, 0)
            except Exception:
                pass

        return p

    def _para(doc, text: str, bold: bool = False):
        p = doc.add_paragraph()
        _style_paragraph(p)
        run = p.add_run(text or "")
        run.bold = bool(bold)
        f = run.font
        f.name = font_name
        f.size = Pt(font_size_pt)
        try:
            f.color.rgb = RGBColor(0, 0, 0)
        except Exception:
            pass
        return p

    def _looks_table_header(line: str) -> bool:
        s = (line or "").strip()
        if not s.startswith("|") or "|" not in s[1:]:
            return False
        parts = [c.strip() for c in s.strip().strip("|").split("|")]
        parts = [p for p in parts if p]
        if len(parts) < 2:
            return False
        has_non_numeric = any(not p.replace(".", "", 1).isdigit() for p in parts)
        return has_non_numeric

    def _parse_table_block(lines, start_idx: int):
        header_line = lines[start_idx]
        headers = [c.strip() for c in header_line.strip().strip("|").split("|")]
        headers = [h for h in headers if h]
        rows = []
        i = start_idx + 1

        # Optional delimiter row like |---|---|
        if i < len(lines):
            test = lines[i].strip().strip("|").replace("-", "").replace(" ", "")
            if test == "":
                i += 1

        while i < len(lines):
            ln = lines[i]
            if not ln.strip().startswith("|"):
                break
            cols = [c.strip() for c in ln.strip().strip("|").split("|")]
            while len(cols) < len(headers):
                cols.append("")
            row = {headers[j]: cols[j] for j in range(len(headers))}
            rows.append(row)
            i += 1

        return rows, i

    def _add_table(doc, maybe_title: str, rows):
        if not rows:
            return
        if maybe_title:
            _para(doc, maybe_title, bold=True)

        cols = list(rows[0].keys())
        tbl = doc.add_table(rows=len(rows) + 1, cols=len(cols))
        try:
            tbl.style = "Table Grid"
        except Exception:
            pass

        # Header row
        for j, col in enumerate(cols):
            cell = tbl.rows[0].cells[j]
            cell.text = str(col or "")
            for par in cell.paragraphs:
                for run in par.runs:
                    run.bold = True
                    f = run.font
                    f.name = font_name
                    f.size = Pt(font_size_pt)
                    try:
                        f.color.rgb = RGBColor(0, 0, 0)
                    except Exception:
                        pass

        # Data rows
        for i, row in enumerate(rows, start=1):
            for j, col in enumerate(cols):
                cell = tbl.rows[i].cells[j]
                cell.text = str(row.get(col, "") or "")
                for par in cell.paragraphs:
                    # table paragraphs: no extra space, but match line spacing
                    fmt = par.paragraph_format
                    fmt.space_before = Pt(0)
                    fmt.space_after = Pt(0)
                    if line_spacing <= 1.05:
                        fmt.line_spacing_rule = WD_LINE_SPACING.SINGLE
                        fmt.line_spacing = None
                    else:
                        fmt.line_spacing_rule = WD_LINE_SPACING.MULTIPLE
                        fmt.line_spacing = line_spacing
                    for run in par.runs:
                        f = run.font
                        f.name = font_name
                        f.size = Pt(font_size_pt)
                        try:
                            f.color.rgb = RGBColor(0, 0, 0)
                        except Exception:
                            pass

    def _write_body_markdown(doc, text: str):
        """Very small markdown subset: paragraphs, bullets, and pipe tables."""
        if not text:
            return

        lines = text.splitlines()
        idx = 0
        n = len(lines)

        while idx < n:
            raw = lines[idx].rstrip("\n")
            if not raw.strip():
                idx += 1
                continue

            # Skip obvious JSON blobs from AI output
            if _likely_json_line(raw):
                idx += 1
                continue

            # Table block starting with a header line
            if raw.lstrip().startswith("|") and _looks_table_header(raw):
                rows, idx2 = _parse_table_block(lines, idx)
                _add_table(doc, None, rows)
                idx = idx2
                continue

            # Normal or bullet paragraph
            _add_inline_runs(doc, raw)
            idx += 1

    # Build document
    doc = Document()

    # Base style: black, Calibri (or chosen)
    base = doc.styles["Normal"]
    base.font.name = font_name
    base.font.size = Pt(font_size_pt)
    try:
        base.font.color.rgb = RGBColor(0, 0, 0)
    except Exception:
        pass

    # Make headings black as well and adjust heading spacing
    for style_name in ("Title", "Heading 1", "Heading 2", "Heading 3"):
        try:
            s = doc.styles[style_name]
            s.font.name = font_name
            s.font.size = Pt(font_size_pt + (4 if style_name == "Title" else 2))
            s.font.color.rgb = RGBColor(0, 0, 0)
            # tweak heading spacing to avoid crowding
            for p in s.paragraph_format.__dict__.keys():
                pass
        except Exception:
            continue

    # Title
    title_text = doc_title or "Proposal"
    h = doc.add_heading(title_text, level=0)
    _style_paragraph(h, is_heading=True)

    # Meta summary (acts as intro on title page)
    if metadata:
        _para(doc, "Summary", bold=True)
        for k, v in metadata.items():
            _para(doc, f"{k}: {v}")

    # New page for Table of Contents (second page)
    doc.add_page_break()

    # Table of Contents (Word will populate when you update fields)
    toc_heading = doc.add_paragraph()
    _style_paragraph(toc_heading, is_heading=True)
    toc_run = toc_heading.add_run("Table of Contents")
    toc_run.bold = True
    toc_run.font.name = font_name
    toc_run.font.size = Pt(font_size_pt + 1)

    toc_para = doc.add_paragraph()
    _style_paragraph(toc_para)
    fld = OxmlElement("w:fldSimple")
    fld.set(qn("w:instr"), 'TOC \\o "1-3" \\h \\z \\u')
    toc_para._p.append(fld)

    # Page break after TOC into main content
    doc.add_page_break()

    # CLIN table
    if clins:
        rows = []
        for row in clins:
            try:
                r = dict(row)
            except Exception:
                r = {"clin": str(row)}
            clin = r.get("clin") or r.get("code") or r.get("CLIN") or ""
            desc = r.get("description") or r.get("desc") or r.get("Description") or ""
            qty = r.get("qty") or r.get("quantity") or ""
            unit = r.get("unit") or r.get("uom") or ""
            price = r.get("unit_price") or r.get("price") or r.get("Unit Price") or ""
            ext = r.get("extended_price") or r.get("ext_price") or r.get("Extended Price") or ""
            rows.append(
                {
                    "CLIN": str(clin or ""),
                    "Description": str(desc or ""),
                    "Qty": str(qty or ""),
                    "Unit": str(unit or ""),
                    "Unit Price": str(price or ""),
                    "Extended Price": str(ext or ""),
                }
            )
        if rows:
            _para(doc, "CLIN Summary", bold=True)
            _add_table(doc, None, rows)

    # Checklist
    if checklist:
        _para(doc, "Checklist", bold=True)
        for row in checklist:
            try:
                r = dict(row)
            except Exception:
                r = {"text": str(row)}
            txt = r.get("item_text") or r.get("text") or r.get("item") or ""
            must = bool(r.get("is_must") or r.get("must"))
            label = "[MUST] " if must else ""
            _para(doc, f"• {label}{txt}")

    # Sections in the order user selected
    # Insert page breaks before major sections (but not before the very first one)
    major_break_titles = [
        "Executive Summary",
        "Technical Approach",
        "Management Plan",
        "Management & Staffing Plan",
        "Staffing Plan",
        "Past Performance",
        "Pricing",
        "Price Volume",
        "Appendix",
        "Annex",
    ]

    for idx, sec in enumerate(sections):
        try:
            title = sec.get("title") or "Section"
            body = sec.get("body") or ""
        except Exception:
            title = "Section"
            body = str(sec)

        # Smart page break: only for later sections with "major" titles
        normalized = str(title or "").strip()
        if idx > 0 and any(normalized.startswith(t) for t in major_break_titles):
            doc.add_page_break()

        heading = doc.add_heading(title, level=1)
        _style_paragraph(heading, is_heading=True)
        _write_body_markdown(doc, body)

    try:
        doc.save(path)
        return path
    except Exception as e:
        try:
            st.error(f"Save DOCX failed: {e}")
        except Exception:
            pass
        return None
def run_proposal_builder(conn: "sqlite3.Connection") -> None:
    # Ensure proposal schema exists; tolerate missing helper in older builds
    try:
        _ensure_x7_schema(conn)  # type: ignore[name-defined]
    except NameError:
        # If helper is not present (older app builds), continue without raising
        pass
    st.header("Proposal Builder")
    st.caption("Use this page to draft compliant proposals and assemble the major sections you need for submission.")
    try:
        db_path = _db_path_from_conn(conn)
    except Exception:
        try:
            db_path = DB_PATH  # type: ignore[name-defined]
        except Exception:
            db_path = "./data/app.db"
    df_rf = _cached_read_sql(db_path, "SELECT id, title, solnum, notice_id FROM rfps_t ORDER BY id DESC;", ())
    if df_rf.empty:
        st.info("No RFP context found. Use RFP Analyzer first to parse and save.")
        return
    rfp_id = st.selectbox(
        "RFP context",
        options=df_rf["id"].tolist(),
        format_func=lambda rid: f"#{rid} — {df_rf.loc[df_rf['id']==rid,'title'].values[0] or 'Untitled'}",
        index=0,
    )
    st.session_state["current_rfp_id"] = rfp_id
    ctx = _load_rfp_context_struct(conn, rfp_id)
    rfp_naics = ""
    try:
        _rfp_df = ctx.get("rfp")
        if isinstance(_rfp_df, pd.DataFrame) and "naics" in _rfp_df.columns and not _rfp_df.empty:
            rfp_naics = str(_rfp_df["naics"].iloc[0] or "")
    except Exception:
        rfp_naics = ""
    # Load active proposal templates and show library controls
    try:
        tpl_df = pd.read_sql_query(
            "SELECT id, name, template_type, default_section, is_active, created_at FROM proposal_templates ORDER BY name;",
            conn,
            params=(),
        )
    except Exception:
        tpl_df = pd.DataFrame(columns=["id", "name", "template_type", "default_section", "is_active", "created_at"])

    with st.expander("Template library (proposal templates)", expanded=False):
        x7_template_library_ui(conn)

    with st.expander("Snippet library (reusable snippets)", expanded=False):
        x7_snippet_library_ui(conn)

    left, right = st.columns([3, 2])
    with left:
        st.subheader("Sections")
        default_sections = [
            "Cover Letter",
            "Executive Summary",
            "Understanding of Requirements",
            "Technical Approach",
            "Management Approach",
            "Staffing and Key Personnel",
            "Quality Assurance / QC",
            "Risks and Mitigations",
            "Past Performance",
            "Pricing Narrative (non-cost)",
            "Compliance Crosswalk",
            "Appendices"
        ]
        selected = st.multiselect("Include sections", default_sections, default=default_sections, key="pb_include_sections")
        gcol1, gcol2 = st.columns([1,1])
        with gcol1:
            global_maxw = st.number_input(
                "Max words per section (Draft All)",
                min_value=0,
                value=750,
                step=50,
                key="pb_global_maxw",
            )
        with gcol2:
            st.caption("Used when per-section max is unset.")

        # Draft all Proposal Builder sections via background job
        if st.button("Draft All Sections ▶", key="pb_draft_all"):
            if not selected:
                st.warning("Select at least one section to draft.")
            else:
                conn_jobs = get_db()
                # Build a lightweight context from current PB state
                notes_map = {}
                for sec in selected:
                    notes_map[str(sec)] = st.session_state.get(f"y3_notes_{sec}", "")
                payload = {
                    "scope": "proposal_builder_draft",
                    "rfp_id": int(rfp_id) if rfp_id else None,
                    "sections": list(selected),
                    "notes": notes_map,
                    "global_max_words": int(st.session_state.get("pb_global_maxw", 750)),
                }
                job_id = jobs_enqueue(conn_jobs, job_type="pb_draft_all", payload=payload)
                st.session_state["pb_draft_all_last_job_id_main"] = job_id
                st.info("Draft job submitted. It will run in the background; see status below.")

        # Hydrate PB sections from the last completed background draft job (if any)
        try:
            last_job_id_main = st.session_state.get("pb_draft_all_last_job_id_main")
            if last_job_id_main:
                import pandas as _pd
                import json as _json
                conn_jobs = get_db()
                ensure_jobs_schema(conn_jobs)
                _df_pb = _pd.read_sql_query(
                    "SELECT status, result_json FROM jobs WHERE id = ?",
                    conn_jobs,
                    params=(int(last_job_id_main),),
                )
                if not _df_pb.empty:
                    _status_pb = str(_df_pb.iloc[0].get("status") or "").lower()
                    _result_json_pb = _df_pb.iloc[0].get("result_json") or ""
                    if _status_pb == "done" and _result_json_pb:
                        try:
                            _data_pb = _json.loads(_result_json_pb) if isinstance(_result_json_pb, str) else {}
                        except Exception:
                            _data_pb = {}
                        _draft_pb = _data_pb.get("draft") or {}
                        for sec, body in _draft_pb.items():
                            if not body:
                                continue
                            final = _finalize_section(sec, body)
                            norm = _pb_normalize_text(final)
                            st.session_state[f"pb_section_{sec}"] = norm
                            st.session_state[f"pb_ta_{sec}"] = norm
        except Exception:
            pass
        # Optional: auto-fill all selected sections from templates.
        if isinstance(tpl_df, pd.DataFrame) and not tpl_df.empty and selected:
            with st.expander("Auto-fill sections from templates", expanded=False):
                st.caption(
                    "For each section, this will use the first active template whose default_section "
                    "matches the section name. If no exact match is found, it will fall back to any "
                    "active 'section' template."
                )
                if st.button("Apply templates to all sections", key="pb_apply_templates_all"):
                    for sec in selected:
                        try:
                            _sec_label = str(sec).strip().lower()
                            _df = tpl_df.copy()
                            if "is_active" in _df.columns:
                                _df = _df[_df["is_active"] == 1]
                            if "default_section" in _df.columns:
                                _match = _df["default_section"].fillna("").str.strip().str.lower() == _sec_label
                                _candidates = _df[_match]
                            else:
                                _candidates = _df
                            if _candidates is None or _candidates.empty:
                                if "template_type" in _df.columns:
                                    _candidates = _df[_df["template_type"].fillna("section") == "section"]
                                else:
                                    _candidates = _df
                            if _candidates is None or _candidates.empty:
                                continue
                            _tid = int(_candidates["id"].iloc[0])
                            _rendered = x7_render_template_text(
                                conn,
                                _tid,
                                rfp_id=int(rfp_id) if rfp_id is not None else None,
                            )
                            if _rendered:
                                _norm = _pb_normalize_text(_rendered)
                                st.session_state[f"pb_section_{sec}"] = _norm
                                st.session_state[f"pb_ta_{sec}"] = _norm
                        except Exception:
                            # Do not break the whole loop if one section fails
                            continue
                    st.success("Applied templates to selected sections.")
                    st.rerun()
        content_map: Dict[str, str] = {}
        for sec in selected:
            default_val = st.session_state.get(f"pb_section_{sec}", "")
            st.markdown(f"**{sec}**")
            notes = st.text_input(f"Notes for {sec}", key=f"y3_notes_{sec}")
            cA, cB, cC = st.columns([1,1,1])
            with cA:
                k = y_auto_k(f"{sec} {notes}")
            with cB:
                maxw = st.number_input(f"Max words — {sec}", min_value=0, value=int(st.session_state.get("pb_global_maxw", 750)), step=10, key=f"y3_maxw_{sec}")
            with cC:
                if st.button(f"Draft {sec}", key=f"y3_draft_{sec}"):
                    ph = st.empty(); acc = []
                    for tok in y3_stream_draft(conn, int(rfp_id), section_title=sec, notes=notes or "", k=int(k), max_words=int(maxw) if maxw>0 else None):
                        acc.append(tok); ph.markdown("".join(acc))
                    drafted = "".join(acc).strip()
                    if drafted:
                        drafted = _strip_citations(drafted)
                        drafted = _y3_top_off_precise(conn, int(rfp_id), sec, notes or "", drafted, int(maxw) if maxw>0 else None)
                        final = _finalize_section(sec, drafted)
                        norm = _pb_normalize_text(final)
                        st.session_state[f"pb_section_{sec}"] = norm
                        st.session_state[f"pb_ta_{sec}"] = norm
                        st.rerun()

            ta_key = f"pb_ta_{sec}"

            if ta_key not in st.session_state:
                st.session_state[ta_key] = st.session_state.get(f"pb_section_{sec}", "")

            # Optionally load this section from a saved template
            _tpl_df_local = None
            try:
                _tpl_df_local = tpl_df
            except Exception:
                _tpl_df_local = None

            if _tpl_df_local is not None and not _tpl_df_local.empty:
                _match = _tpl_df_local["default_section"].fillna("").str.strip().str.lower() == str(sec).strip().lower()
                _candidates = _tpl_df_local[_match]
                if _candidates.empty:
                    _candidates = _tpl_df_local
                _template_ids = _candidates["id"].tolist()

                _labels: dict[int, str] = {}
                for _idx, _row in _candidates.iterrows():
                    _tid = int(_row["id"])
                    _tt = (_row.get("template_type") or "section")
                    _sec_label = (_row.get("default_section") or "").strip()
                    _nm = _row["name"] or f"Template {_tid}"
                    if _sec_label:
                        _labels[_tid] = f"{_nm} — {_tt}, {_sec_label}"
                    else:
                        _labels[_tid] = f"{_nm} — {_tt}"

                def _x7_fmt_tpl_id(_tid: int | None) -> str:
                    if _tid is None:
                        return "Choose..."
                    return _labels.get(int(_tid), f"Template {_tid}")

                _selected_tpl_id = st.selectbox(
                    f"Load from template for {sec}",
                    options=[None] + _template_ids,
                    format_func=_x7_fmt_tpl_id,
                    key=f"x7_tpl_sel_{sec}",
                )
                if _selected_tpl_id is not None:
                    if st.button("Apply template", key=f"x7_tpl_apply_{sec}"):
                        _rendered = x7_render_template_text(conn, int(_selected_tpl_id), rfp_id=int(rfp_id))
                        if _rendered:
                            _norm = _pb_normalize_text(_rendered)
                            st.session_state[f"pb_section_{sec}"] = _norm
                            st.session_state[ta_key] = _norm
                            st.success("Template applied.")
                            st.rerun()

            # Optional: insert reusable snippets into this section
            with st.expander(f"Snippet library for {sec}", expanded=False):
                categories = ["technical", "management", "past_performance", "risks"]
                sec_lower = str(sec).lower()
                default_cat = None
                if "technical" in sec_lower:
                    default_cat = "technical"
                elif "management" in sec_lower:
                    default_cat = "management"
                elif "past performance" in sec_lower:
                    default_cat = "past_performance"
                elif "risk" in sec_lower:
                    default_cat = "risks"

                cat_index = 0
                if default_cat in categories:
                    try:
                        cat_index = categories.index(default_cat)
                    except Exception:
                        cat_index = 0

                snip_cat = st.selectbox(
                    "Snippet category",
                    options=categories,
                    index=cat_index,
                    key=f"pb_snip_cat_{sec}",
                )

                naics_default = rfp_naics or ""
                snip_naics = st.text_input("NAICS filter", value=naics_default, key=f"pb_snip_naics_{sec}")
                snip_work = st.text_input("Work type filter", value="", key=f"pb_snip_work_{sec}")

                rows = []
                try:
                    from contextlib import closing as _closing
                    with _closing(conn.cursor()) as cur:
                        cur.execute(
                            """
                            SELECT id, name, category, naics, work_type, body
                            FROM proposal_snippets
                            WHERE is_active = 1
                              AND category = ?
                              AND (? = '' OR naics = ? OR naics LIKE '%' || ? || '%')
                              AND (? = '' OR work_type = ? OR work_type LIKE '%' || ? || '%')
                            ORDER BY created_at DESC
                            LIMIT 100;
                            """,
                            (snip_cat, snip_naics, snip_naics, snip_naics, snip_work, snip_work, snip_work),
                        )
                        rows = cur.fetchall()
                except Exception:
                    rows = []

                if not rows:
                    st.caption("No matching snippets yet. Use the Snippet library controls above to add some.")
                else:
                    labels = {int(r[0]): f"#{int(r[0])} — {str(r[1] or '')[:80]}" for r in rows}
                    ids = [int(r[0]) for r in rows]
                    choice = st.selectbox(
                        "Snippet to insert",
                        options=[None] + ids,
                        format_func=lambda sid: "Choose..." if sid is None else labels.get(int(sid), f"Snippet {int(sid)}"),
                        key=f"pb_snip_sel_{sec}",
                    )
                    if choice is not None:
                        body = ""
                        for r in rows:
                            if int(r[0]) == int(choice):
                                body = r[5] or ""
                                break
                        if body:
                            current = st.session_state.get(ta_key, "") or ""
                            if current and not current.endswith("\n"):
                                current = current + "\n\n"
                            st.session_state[ta_key] = current + body
                            st.session_state[f"pb_section_{sec}"] = st.session_state[ta_key]
                            st.success("Snippet inserted.")
                            st.rerun()

            content_map[sec] = st.text_area(sec, height=200, key=ta_key)
            with st.expander(f"Preview — {sec}", expanded=False):
                st.markdown(st.session_state.get(ta_key, ""))
    # Ensure text areas reflect latest session values
    for sec in selected:
        src = st.session_state.get(f"pb_section_{sec}")
        if src is not None and st.session_state.get(f"pb_ta_{sec}", None) != src:
            st.session_state[f"pb_ta_{sec}"] = src
            st.rerun()
    with right:
        st.subheader("Guidance and limits")
        # Surface AI attachments for the selected RFP so you do not need to re-upload files here.
        try:
            with st.expander("AI attachments for this RFP", expanded=False):
                render_ai_attachments_panel(conn, int(rfp_id))
        except Exception:
            # Keep Proposal Builder usable even if the AI attachments panel is unavailable.
            pass
        spacing = st.selectbox("Line spacing", ["Single", "1.15", "Double"], index=1)
        font_name = st.selectbox("Font", ["Times New Roman", "Calibri", "Arial"], index=0)
        font_size = st.number_input("Font size", min_value=10, max_value=12, value=11)
        page_limit = st.number_input("Page limit for narrative", min_value=1, max_value=200, value=10)

        st.markdown("**Must address items from L and M**")
        items = _ctxd(ctx, "items") if isinstance(ctx.get("items"), pd.DataFrame) else pd.DataFrame()
        if not items.empty:
            _styled_dataframe(items.rename(columns={"item_text": "Item", "status": "Status"}), use_container_width=True, hide_index=True, height=240)
        else:
            st.caption("No checklist items found for this RFP")

        total_words = sum(len((content_map.get(k) or "").split()) for k in selected)
        est_pages = _estimate_pages(total_words, spacing)
        st.info(f"Current word count {total_words}  Estimated pages {est_pages}")
        if est_pages > page_limit:
            st.error("Content likely exceeds page limit. Consider trimming or tighter formatting")

        out_name = f"Proposal_RFP_{int(rfp_id)}.docx"
        out_path = os.path.join(DATA_DIR, out_name)

        

        if st.button("Export DOCX", type="primary"):
            # Track this heavy export as a background job in the jobs table.
            try:
                ensure_jobs_schema(conn)
            except Exception:
                pass

            try:
                try:
                    _user_name = get_current_user_name()
                except Exception:
                    _user_name = ""
                payload = {
                    "scope": "proposal_export_docx",
                    "rfp_id": int(rfp_id) if rfp_id is not None else None,
                    "section_keys": [k for k in selected],
                    "out_name": out_name,
                }
                job_id = jobs_enqueue(
                    conn,
                    job_type="proposal_export_docx",
                    payload=payload,
                    created_by=_user_name or None,
                )
            except Exception:
                job_id = None

            # Existing synchronous export logic (still runs inline for now).
            sections = [{"title": k, "body": content_map.get(k, "")} for k in selected]
            try:
                if job_id:
                    jobs_update_status(
                        conn,
                        job_id,
                        status="running",
                        progress=0.1,
                        mark_started=True,
                    )
            except Exception:
                pass

            try:
                exported = _export_docx(
                    out_path,
                    doc_title=_first_row_value(_ctxd(ctx, "rfp"), "title", "Proposal"),
                    sections=sections,
                    clins=_ctxd(ctx, "clins"),
                    checklist=_ctxd(ctx, "items"),
                    metadata={
                        "rfp_id": int(rfp_id) if rfp_id is not None else None,
                        "notice_id": _first_row_value(_ctxd(ctx, "rfp"), "notice_id", None),
                    },
                    font_name=font_name,
                    font_size_pt=int(font_size),
                    spacing=spacing,
                )
            except Exception as e:
                try:
                    st.error(f"Export DOCX failed: {e}")
                except Exception:
                    pass
                exported = None
            if exported:
                st.success(f"Exported to {exported}")
                try:
                    if job_id:
                        jobs_update_status(
                            conn,
                            job_id,
                            status="done",
                            progress=1.0,
                            mark_finished=True,
                            result={"path": str(exported)},
                        )
                except Exception:
                    pass

                try:
                    with open(exported, "rb") as _f:
                        _data = _f.read()
                    st.download_button(
                        "Download DOCX",
                        data=_data,
                        file_name=out_name,
                        mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                        key=f"dl_exported_docx"
                    )
                except Exception as _e:
                    st.error(f"Download unavailable: {_e}")
                    try:
                        if job_id:
                            jobs_update_status(
                                conn,
                                job_id,
                                status="failed",
                                error_message=str(_e),
                                mark_finished=True,
                            )
                    except Exception:
                        pass
        # [removed] Legacy Snippets/Citations panel hidden
# ---------- Subcontractor Finder (Phase D) ----------
    try:
        _rid = locals().get('rfp_id') or locals().get('rid') or st.session_state.get('current_rfp_id')
        y6_render_co_box(conn if 'conn' in locals() else None, _rid, key_prefix="run_proposal_builder_y6", title="Ask the CO while drafting")
    except Exception:
        pass

def _s1d_paginate(df, page_size: int, page_key: str = "s1d_page"):
    import math
    import streamlit as st

    n = 0 if df is None else len(df)
    page_size = max(5, int(page_size or 25))
    pages = max(1, math.ceil((n or 0) / page_size))
    page = int(st.session_state.get(page_key, 1))
    if page < 1: page = 1
    if page > pages: page = pages
    start = (page - 1) * page_size
    end = start + page_size
    view = df.iloc[start:end].copy() if df is not None else df
    st.session_state[page_key] = page
    return view, page, pages


def _vendor_recompute_score(conn: "sqlite3.Connection", vendor_id: int) -> None:
    """Recompute rolled-up vendor_score for a vendor from capacity snapshots and job history."""
    from contextlib import closing as _closing
    try:
        vid = int(vendor_id)
    except Exception:
        try:
            vid = int(str(vendor_id or "0").strip() or "0")
        except Exception:
            return
    try:
        with _closing(conn.cursor()) as cur:
            # Average of all recorded job overall scores
            try:
                cur.execute(
                    "SELECT AVG(overall_score) FROM vendor_job WHERE vendor_id=? AND overall_score IS NOT NULL;",
                    (vid,),
                )
                row = cur.fetchone()
                job_avg = float(row[0]) if row and row[0] is not None else None
            except Exception:
                job_avg = None

            # Latest capacity snapshot overall score
            try:
                cur.execute(
                    """
                    SELECT overall_score
                    FROM vendor_capacity
                    WHERE vendor_id=?
                    ORDER BY last_updated DESC, id DESC
                    LIMIT 1;
                    """,
                    (vid,),
                )
                row = cur.fetchone()
                cap_score = float(row[0]) if row and row[0] is not None else None
            except Exception:
                cap_score = None

            scores = []
            if cap_score is not None:
                scores.append(cap_score)
            if job_avg is not None:
                scores.append(job_avg)

            vscore = None
            if scores:
                try:
                    vscore = sum(scores) / float(len(scores))
                except Exception:
                    try:
                        vscore = float(scores[-1])
                    except Exception:
                        vscore = None

            try:
                if vscore is not None:
                    cur.execute(
                        "UPDATE vendors SET vendor_score=? WHERE id=?;",
                        (float(vscore), vid),
                    )
                else:
                    cur.execute(
                        "UPDATE vendors SET vendor_score=NULL WHERE id=?;",
                        (vid,),
                    )
            except Exception:
                # Column may not exist on very old schemas; safe to ignore.
                pass
        conn.commit()
    except Exception:
        try:
            conn.rollback()
        except Exception:
            pass


def _get_suggested_subs(conn: "sqlite3.Connection", required_naics: str = "", work_type_tags: str = "", limit: int = 10):
    """
    Compute a ranked list of suggested subcontractors based on required NAICS and work_type tags.

    - required_naics: single NAICS code (or comma-separated list); we use the first non-empty token.
    - work_type_tags: comma/semicolon separated capability labels, e.g. "janitorial, grounds, HVAC".
    """
    try:
        import pandas as _pd
    except Exception:
        return None

    def _norm_naics(s):
        vals = []
        for part in str(s or "").replace(";", ",").split(","):
            p = part.strip()
            if not p:
                continue
            # Keep only leading digits (NAICS codes are numeric)
            digits = "".join(ch for ch in p if ch.isdigit())
            if digits:
                vals.append(digits)
        return set(vals)

    def _norm_tags(s):
        tags = set()
        raw = str(s or "")
        for sep in [";", ",", "|", "/"]:
            raw = raw.replace(sep, ",")
        for part in raw.split(","):
            t = part.strip().lower()
            if t:
                tags.add(t)
        return tags

    req_naics = ""
    for part in str(required_naics or "").replace(";", ",").split(","):
        p = part.strip()
        if p:
            # normalize to digits only for comparison, but keep raw for display
            req_naics = "".join(ch for ch in p if ch.isdigit()) or p
            break

    req_tags = _norm_tags(work_type_tags)

    if not req_naics and not req_tags:
        return _pd.DataFrame(columns=["id", "name", "city", "state", "naics", "primary_naics",
                                      "other_naics", "capability_tags", "vendor_score",
                                      "naics_match", "tag_match_count", "rank_score"])

    try:
        tid = _current_tenant(conn)
    except Exception:
        tid = 1

    try:
        df_v = _pd.read_sql_query(
            """
            SELECT id, name, city, state,
                   COALESCE(primary_naics, '') AS primary_naics,
                   COALESCE(other_naics, '') AS other_naics,
                   COALESCE(naics, '') AS naics,
                   COALESCE(capability_tags, '') AS capability_tags,
                   COALESCE(vendor_score, 0.0) AS vendor_score
            FROM vendors
            WHERE (tenant_id = ? OR tenant_id IS NULL)
            """,
            conn,
            params=(int(tid),),
        )
    except Exception:
        # Fall back to base table without tenant scoping if needed
        try:
            df_v = _pd.read_sql_query(
                """
                SELECT id, name, city, state,
                       COALESCE(primary_naics, '') AS primary_naics,
                       COALESCE(other_naics, '') AS other_naics,
                       COALESCE(naics, '') AS naics,
                       COALESCE(capability_tags, '') AS capability_tags,
                       COALESCE(vendor_score, 0.0) AS vendor_score
                FROM vendors
                """,
                conn,
                params=(),
            )
        except Exception:
            return None

    if df_v is None or df_v.empty:
        return df_v

    scores = []
    naics_matches = []
    tag_counts = []
    for _, r in df_v.iterrows():
        v_primary = _norm_naics(r.get("primary_naics"))
        v_other = _norm_naics(r.get("other_naics"))
        v_naics = _norm_naics(r.get("naics"))
        v_tags = _norm_tags(r.get("capability_tags"))

        base = 0.0
        match_label = ""
        all_naics = v_primary | v_other | v_naics

        if req_naics:
            if req_naics in v_primary:
                base += 3.0
                match_label = "primary"
            elif req_naics in all_naics:
                base += 2.0
                match_label = "other"
            else:
                # prefix match (e.g., 561 for 561720)
                if any(x.startswith(req_naics) or req_naics.startswith(x) for x in all_naics):
                    base += 1.5
                    match_label = "prefix"
                else:
                    match_label = ""

        tag_match = len(req_tags & v_tags) if req_tags else 0
        if tag_match:
            base += 1.0 + 0.5 * tag_match  # reward multiple tag hits

        vscore = 0.0
        try:
            vscore = float(r.get("vendor_score") or 0.0)
        except Exception:
            vscore = 0.0

        # Blend vendor_score into ranking, normalized to ~0-1 range
        rank = base + (vscore / 5.0)

        scores.append(rank)
        naics_matches.append(match_label)
        tag_counts.append(tag_match)

    df_v = df_v.copy()
    df_v["naics_match"] = naics_matches
    df_v["tag_match_count"] = tag_counts
    df_v["rank_score"] = scores

    try:
        df_v = df_v.sort_values(by=["rank_score", "vendor_score", "name"], ascending=[False, False, True])
    except Exception:
        pass

    if limit and limit > 0:
        df_v = df_v.head(int(limit))

    return df_v

def run_subcontractor_finder(conn: "sqlite3.Connection") -> None:
    st.header("Subcontractor Finder")
    st.caption("Use this page to search for qualified subcontractors by location and service and add them to your pipeline.")
    st.caption("Seed and manage vendors by NAICS/PSC/state; handoff selected vendors to Outreach.")

    ctx = st.session_state.get("rfp_selected_notice", {})
    default_naics = ctx.get("NAICS") or st.session_state.get("sub_default_naics", "")
    default_state = st.session_state.get("sub_default_state", "")

    # Default Place of Performance from selected notice if available
    default_pop = (ctx.get("Place of Performance") or ctx.get("place_of_performance") or ctx.get("POP") or "").strip()

    # Ensure vendor capability columns exist and detect if they are available
    has_vendor_caps = False
    try:
        with closing(conn.cursor()) as cur:
            for _col, _ctype in [
                ("primary_naics", "TEXT"),
                ("other_naics", "TEXT"),
                ("coverage_locations", "TEXT"),
                ("capability_tags", "TEXT"),
                ("small_business_flag", "INTEGER"),
                ("set_aside_flags", "TEXT"),
                ("vendor_score", "REAL"),
            ]:
                try:
                    cur.execute("SELECT " + _col + " FROM vendors LIMIT 1;")
                except Exception:
                    try:
                        cur.execute("ALTER TABLE vendors ADD COLUMN " + _col + " " + _ctype + ";")
                    except Exception:
                        # If this fails we still try to continue with base columns
                        pass
            conn.commit()
            cur.execute("PRAGMA table_info(vendors);")
            cols = [r[1] for r in cur.fetchall()]
            _needed = {"primary_naics", "other_naics", "coverage_locations", "capability_tags", "small_business_flag", "set_aside_flags"}
            has_vendor_caps = _needed.issubset(set(cols))
    except Exception:
        has_vendor_caps = False

    with st.expander("Filters", expanded=True):
        c1, c2, c3, c4 = st.columns([2, 2, 2, 2])
        with c1:
            f_naics = st.text_input("NAICS", value=default_naics, key="filter_naics")
        with c2:
            f_state = st.text_input("State (for example TX)", value=default_state, key="filter_state")
        with c3:
            f_city = st.text_input("City contains", key="filter_city")
        with c4:
            f_kw = st.text_input("Keyword in name or notes", key="filter_kw")
        c5, _ = st.columns([2, 2])
        with c5:
            f_capability = st.text_input("Capability keyword (tags)", key="filter_capability") if has_vendor_caps else ""
        st.caption("Use CSV import or add vendors manually. Internet seeding can be added later.")

    with st.expander("Import Vendors (CSV)", expanded=False):
        st.caption("Headers: name, email, phone, city, state, naics, cage, uei, website, notes")
        up = st.file_uploader("Upload vendor CSV", type=["csv"], key="vendor_csv")
        if up and st.button("Import CSV"):
            try:
                df = pd.read_csv(up)
                if "name" not in {c.lower() for c in df.columns}:
                    st.error("CSV must include a 'name' column")
                else:
                    df.columns = [c.lower() for c in df.columns]
                    n = 0
                    with closing(conn.cursor()) as cur:
                        for _, r in df.iterrows():
                            cur.execute(
                                """
                                INSERT INTO vendors(name, cage, uei, naics, city, state, phone, email, website, notes)
                                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                                ;
                                """,
                                (
                                    str(r.get("name", ""))[:200],
                                    str(r.get("cage", ""))[:20],
                                    str(r.get("uei", ""))[:40],
                                    str(r.get("naics", ""))[:20],
                                    str(r.get("city", ""))[:100],
                                    str(r.get("state", ""))[:10],
                                    str(r.get("phone", ""))[:40],
                                    str(r.get("email", ""))[:120],
                                    str(r.get("website", ""))[:200],
                                    str(r.get("notes", ""))[:500],
                                ),
                            )
                            n += 1
                    conn.commit()
                    st.success(f"Imported {n} vendors")
            except Exception as e:
                st.error(f"Import failed: {e}")

    with st.expander("Add Vendor", expanded=False):
        c1, c2, c3 = st.columns([2, 2, 2])
        with c1:
            v_name = st.text_input("Company name", key="add_name")
            v_email = st.text_input("Email", key="add_email")
            v_phone = st.text_input("Phone", key="add_phone")
        with c2:
            v_city = st.text_input("City", key="add_city")
            v_state = st.text_input("State", key="add_state")
            v_naics = st.text_input("NAICS", key="add_naics")
            v_primary_naics = st.text_input("Primary NAICS", key="add_primary_naics", value=v_naics) if has_vendor_caps else v_naics
        with c3:
            v_other_naics = st.text_input("Other NAICS codes", key="add_other_naics") if has_vendor_caps else ""
            v_coverage_locations = st.text_input("Coverage locations (states or regions)", key="add_coverage_locations") if has_vendor_caps else ""
            v_cage = st.text_input("CAGE", key="add_cage")
            v_uei = st.text_input("UEI", key="add_uei")
        v_site = st.text_input("Website", key="add_site")
        v_capability_tags = st.text_input(
            "Capability tags (comma separated, for example janitorial, HVAC, grounds)",
            key="add_capability_tags",
        ) if has_vendor_caps else ""
        v_set_aside_flags = st.text_input(
            "Set-aside flags (for example WOSB, SDVOSB)",
            key="add_set_aside_flags",
        ) if has_vendor_caps else ""
        v_small_business_flag = st.checkbox("Small business", key="add_small_business_flag", value=True) if has_vendor_caps else False
        v_notes = st.text_area("Notes", height=80, key="add_notes")
        if st.button("Save Vendor"):
            if not v_name.strip():
                st.error("Name is required")
            else:
                try:
                    with closing(conn.cursor()) as cur:
                        if has_vendor_caps:
                            cur.execute(
                                """
                                INSERT INTO vendors(
                                    name, cage, uei, naics, city, state, phone, email, website, notes,
                                    primary_naics, other_naics, coverage_locations, capability_tags,
                                    small_business_flag, set_aside_flags
                                )
                                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                                ;
                                """,
                                (
                                    v_name.strip(),
                                    v_cage.strip(),
                                    v_uei.strip(),
                                    v_naics.strip(),
                                    v_city.strip(),
                                    v_state.strip(),
                                    v_phone.strip(),
                                    v_email.strip(),
                                    v_site.strip(),
                                    v_notes.strip(),
                                    (v_primary_naics or "").strip(),
                                    (v_other_naics or "").strip(),
                                    (v_coverage_locations or "").strip(),
                                    (v_capability_tags or "").strip(),
                                    1 if v_small_business_flag else 0,
                                    (v_set_aside_flags or "").strip(),
                                ),
                            )
                        else:
                            cur.execute(
                                """
                                INSERT INTO vendors(name, cage, uei, naics, city, state, phone, email, website, notes)
                                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                                ;
                                """,
                                (
                                    v_name.strip(),
                                    v_cage.strip(),
                                    v_uei.strip(),
                                    v_naics.strip(),
                                    v_city.strip(),
                                    v_state.strip(),
                                    v_phone.strip(),
                                    v_email.strip(),
                                    v_site.strip(),
                                    v_notes.strip(),
                                ),
                            )
                        conn.commit()
                    st.success("Vendor saved")
                except Exception as e:
                    st.error(f"Save failed: {e}")

    # Vendor listing
    if has_vendor_caps:
        q = (
            "SELECT id, name, email, phone, city, state, naics, cage, uei, website, notes, "
            "primary_naics, other_naics, coverage_locations, capability_tags, small_business_flag, set_aside_flags, vendor_score "
            "FROM vendors WHERE 1=1"
        )
    else:
        # Fallback to the legacy view if capability columns are not reliably present
        q = "SELECT id, name, email, phone, city, state, naics, cage, uei, website, notes FROM vendors_t WHERE 1=1"
    params: List[Any] = []
    if f_naics:
        if has_vendor_caps:
            q += " AND (naics LIKE ? OR primary_naics LIKE ? OR other_naics LIKE ?)"
            params.extend([f"%{f_naics}%", f"%{f_naics}%", f"%{f_naics}%"])
        else:
            q += " AND (naics LIKE ?)"
            params.append(f"%{f_naics}%")
    if f_state:
        q += " AND (state LIKE ?)"
        params.append(f"%{f_state}%")
    if f_city:
        q += " AND (city LIKE ?)"
        params.append(f"%{f_city}%")
    if f_kw:
        q += " AND (name LIKE ? OR notes LIKE ?)"
        params.extend([f"%{f_kw}%", f"%{f_kw}%"])
    if has_vendor_caps and f_capability:
        q += " AND (capability_tags LIKE ? OR primary_naics LIKE ? OR other_naics LIKE ?)"
        params.extend([f"%{f_capability}%", f"%{f_capability}%", f"%{f_capability}%"])

    try:
        try:
            db_path = _db_path_from_conn(conn)
        except Exception:
            try:
                db_path = DB_PATH  # type: ignore[name-defined]
            except Exception:
                db_path = "./data/app.db"
        df_v = _cached_read_sql(db_path, q + " ORDER BY name ASC;", tuple(params))
    except Exception as e:
        st.error(f"Query failed: {e}")
        df_v = pd.DataFrame()

    st.subheader("Vendors")
    if df_v.empty:
        st.write("No vendors match filters")
    else:
        selected_ids = []
        for _, row in df_v.iterrows():
            label = f"{row['name']}  ({row['email'] or 'no email'})"
            chk = st.checkbox(f"Select — {label}", key=f"vend_{int(row['id'])}")

            # Show capability summary for each vendor if columns are present
            primary_naics = row.get("primary_naics") if "primary_naics" in row else None
            other_naics = row.get("other_naics") if "other_naics" in row else None
            coverage_locations = row.get("coverage_locations") if "coverage_locations" in row else None
            capability_tags = row.get("capability_tags") if "capability_tags" in row else None
            small_business_flag = row.get("small_business_flag") if "small_business_flag" in row else None
            set_aside_flags = row.get("set_aside_flags") if "set_aside_flags" in row else None

            meta_parts: List[str] = []
            if row.get("naics"):
                meta_parts.append(f"NAICS {row.get('naics')}")
            if primary_naics:
                meta_parts.append(f"Primary {primary_naics}")
            if other_naics:
                meta_parts.append(f"Other {other_naics}")
            if coverage_locations:
                meta_parts.append(f"Coverage {coverage_locations}")
            if small_business_flag not in (None, ""):
                try:
                    sb_val = int(small_business_flag or 0)
                except Exception:
                    sb_val = 0
                meta_parts.append("Small business" if sb_val else "Not small business")
            if set_aside_flags:
                meta_parts.append(f"Set-asides {set_aside_flags}")
            # Show rolled-up vendor_score if present
            vendor_score_val = None
            try:
                if "vendor_score" in row and row.get("vendor_score") is not None:
                    vendor_score_val = float(row.get("vendor_score"))
            except Exception:
                vendor_score_val = None
            if vendor_score_val is not None:
                meta_parts.append(f"Score {vendor_score_val:.1f}/5.0")
            if meta_parts:
                st.caption(" • " + "  |  ".join(meta_parts))
            if capability_tags:
                st.caption(f" • Capabilities {capability_tags}")

            # Capacity and past performance UI per vendor
            vid = int(row["id"])
            with st.expander("Capacity and past performance", expanded=False):
                # Latest capacity snapshot
                existing_cap = None
                try:
                    df_cap = safe_read_sql(
                        conn,
                        """
                        SELECT capacity_notes,
                               max_concurrent_jobs,
                               max_annual_value,
                               current_load_notes,
                               quality,
                               schedule,
                               communication,
                               price,
                               overall_score,
                               last_updated
                        FROM vendor_capacity
                        WHERE vendor_id=?
                        ORDER BY last_updated DESC, id DESC
                        LIMIT 1
                        """,
                        (vid,),
                    )
                    if df_cap is not None and not df_cap.empty:
                        existing_cap = df_cap.iloc[0]
                except Exception:
                    df_cap = None
                    existing_cap = None

                if existing_cap is not None:
                    last_ts = existing_cap.get("last_updated") or ""
                    st.caption(f"Last capacity snapshot: {last_ts}")
                    score_parts = []
                    for key_label in [
                        ("quality", "Quality"),
                        ("schedule", "Schedule"),
                        ("communication", "Communication"),
                        ("price", "Price"),
                        ("overall_score", "Overall"),
                    ]:
                        key, label_txt = key_label
                        val = existing_cap.get(key)
                        if val not in (None, ""):
                            try:
                                score_parts.append(f"{label_txt} {float(val):.1f}")
                            except Exception:
                                score_parts.append(f"{label_txt} {val}")
                    if score_parts:
                        st.caption("Scores: " + "  |  ".join(score_parts))
                    if existing_cap.get("capacity_notes"):
                        st.write(f"Capacity notes: {existing_cap.get('capacity_notes')}")
                    if existing_cap.get("current_load_notes"):
                        st.write(f"Current load: {existing_cap.get('current_load_notes')}")
                else:
                    st.caption("No capacity snapshot logged yet.")

                st.markdown("**Update capacity snapshot**")
                vcap_notes = st.text_area(
                    "Capacity notes",
                    value=(existing_cap.get("capacity_notes") if existing_cap is not None else ""),
                    key=f"vcap_notes_{vid}",
                )
                vcap_current = st.text_area(
                    "Current load notes",
                    value=(existing_cap.get("current_load_notes") if existing_cap is not None else ""),
                    key=f"vcap_current_{vid}",
                )
                vcap_max_jobs = st.number_input(
                    "Max concurrent jobs",
                    min_value=0,
                    step=1,
                    value=int(existing_cap.get("max_concurrent_jobs") or 0) if existing_cap is not None else 0,
                    key=f"vcap_max_jobs_{vid}",
                )
                vcap_max_value = st.number_input(
                    "Max annual value (approx)",
                    min_value=0.0,
                    step=1000.0,
                    value=float(existing_cap.get("max_annual_value") or 0.0) if existing_cap is not None else 0.0,
                    key=f"vcap_max_value_{vid}",
                )
                c_scores = st.columns(5)
                with c_scores[0]:
                    vcap_quality = st.number_input(
                        "Quality",
                        min_value=0.0,
                        max_value=5.0,
                        step=0.1,
                        value=float(existing_cap.get("quality") or 0.0) if existing_cap is not None else 0.0,
                        key=f"vcap_quality_{vid}",
                    )
                with c_scores[1]:
                    vcap_schedule = st.number_input(
                        "Schedule",
                        min_value=0.0,
                        max_value=5.0,
                        step=0.1,
                        value=float(existing_cap.get("schedule") or 0.0) if existing_cap is not None else 0.0,
                        key=f"vcap_schedule_{vid}",
                    )
                with c_scores[2]:
                    vcap_comm = st.number_input(
                        "Communication",
                        min_value=0.0,
                        max_value=5.0,
                        step=0.1,
                        value=float(existing_cap.get("communication") or 0.0) if existing_cap is not None else 0.0,
                        key=f"vcap_comm_{vid}",
                    )
                with c_scores[3]:
                    vcap_price = st.number_input(
                        "Price",
                        min_value=0.0,
                        max_value=5.0,
                        step=0.1,
                        value=float(existing_cap.get("price") or 0.0) if existing_cap is not None else 0.0,
                        key=f"vcap_price_{vid}",
                    )
                with c_scores[4]:
                    vcap_overall = st.number_input(
                        "Overall",
                        min_value=0.0,
                        max_value=5.0,
                        step=0.1,
                        value=float(existing_cap.get("overall_score") or 0.0) if existing_cap is not None else 0.0,
                        key=f"vcap_overall_{vid}",
                    )

                if st.button("Save capacity snapshot", key=f"save_vcap_{vid}"):
                    try:
                        with conn:
                            conn.execute(
                                """
                                INSERT INTO vendor_capacity(
                                    vendor_id,
                                    capacity_notes,
                                    max_concurrent_jobs,
                                    max_annual_value,
                                    current_load_notes,
                                    quality,
                                    schedule,
                                    communication,
                                    price,
                                    overall_score
                                )
                                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                                """,
                                (
                                    vid,
                                    vcap_notes.strip(),
                                    int(vcap_max_jobs),
                                    float(vcap_max_value),
                                    vcap_current.strip(),
                                    float(vcap_quality),
                                    float(vcap_schedule),
                                    float(vcap_comm),
                                    float(vcap_price),
                                    float(vcap_overall),
                                ),
                            )
                        st.success("Capacity snapshot saved")
                        try:
                            _vendor_recompute_score(conn, vid)
                        except Exception:
                            pass
                    except Exception as e:
                        st.error(f"Save failed: {e}")

                st.markdown("**Log past performance job**")
                vj_name = st.text_input("Job name", key=f"vj_name_{vid}")
                vj_agency = st.text_input("Agency", key=f"vj_agency_{vid}")
                vj_role = st.text_input("Role (for example prime or sub)", key=f"vj_role_{vid}")
                vj_contract = st.text_input("Contract number", key=f"vj_contract_{vid}")
                vj_naics = st.text_input("NAICS", key=f"vj_naics_{vid}")
                col_dates = st.columns(2)
                with col_dates[0]:
                    vj_start = st.text_input("Start date", key=f"vj_start_{vid}")
                with col_dates[1]:
                    vj_end = st.text_input("End date", key=f"vj_end_{vid}")
                vj_value = st.number_input(
                    "Contract value",
                    min_value=0.0,
                    step=1000.0,
                    key=f"vj_value_{vid}",
                )
                score_cols = st.columns(5)
                with score_cols[0]:
                    vj_quality = st.number_input(
                        "Quality (0 to 5)",
                        min_value=0.0,
                        max_value=5.0,
                        step=0.1,
                        key=f"vj_quality_{vid}",
                    )
                with score_cols[1]:
                    vj_schedule = st.number_input(
                        "Schedule (0 to 5)",
                        min_value=0.0,
                        max_value=5.0,
                        step=0.1,
                        key=f"vj_schedule_{vid}",
                    )
                with score_cols[2]:
                    vj_comm = st.number_input(
                        "Communication (0 to 5)",
                        min_value=0.0,
                        max_value=5.0,
                        step=0.1,
                        key=f"vj_comm_{vid}",
                    )
                with score_cols[3]:
                    vj_price = st.number_input(
                        "Price (0 to 5)",
                        min_value=0.0,
                        max_value=5.0,
                        step=0.1,
                        key=f"vj_price_{vid}",
                    )
                with score_cols[4]:
                    vj_overall = st.number_input(
                        "Overall (0 to 5)",
                        min_value=0.0,
                        max_value=5.0,
                        step=0.1,
                        key=f"vj_overall_{vid}",
                    )
                vj_notes = st.text_area("Job notes", key=f"vj_notes_{vid}", height=80)

                if st.button("Add job record", key=f"vj_add_{vid}") and vj_name.strip():
                    try:
                        with conn:
                            conn.execute(
                                """
                                INSERT INTO vendor_job(
                                    vendor_id,
                                    job_name,
                                    agency,
                                    contract_number,
                                    naics,
                                    role,
                                    start_date,
                                    end_date,
                                    contract_value,
                                    quality,
                                    schedule,
                                    communication,
                                    price,
                                    overall_score,
                                    notes
                                )
                                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                                """,
                                (
                                    vid,
                                    vj_name.strip(),
                                    vj_agency.strip(),
                                    vj_contract.strip(),
                                    vj_naics.strip(),
                                    vj_role.strip(),
                                    vj_start.strip(),
                                    vj_end.strip(),
                                    float(vj_value),
                                    float(vj_quality),
                                    float(vj_schedule),
                                    float(vj_comm),
                                    float(vj_price),
                                    float(vj_overall),
                                    vj_notes.strip(),
                                ),
                            )
                        st.success("Job record added")
                        try:
                            _vendor_recompute_score(conn, vid)
                        except Exception:
                            pass
                    except Exception as e:
                        st.error(f"Save failed: {e}")

                # Recent jobs table
                try:
                    df_jobs = safe_read_sql(
                        conn,
                        """
                        SELECT job_name,
                               agency,
                               role,
                               contract_number,
                               naics,
                               start_date,
                               end_date,
                               contract_value,
                               quality,
                               schedule,
                               communication,
                               price,
                               overall_score
                        FROM vendor_job
                        WHERE vendor_id=?
                        ORDER BY COALESCE(end_date, start_date) DESC, id DESC
                        LIMIT 10
                        """,
                        (vid,),
                    )
                except Exception:
                    df_jobs = None

                if df_jobs is not None and not df_jobs.empty:
                    st.dataframe(df_jobs, use_container_width=True)
                else:
                    st.caption("No past performance jobs logged yet.")

            # Delete vendor
            if st.button("Delete vendor", key=f"vendor_del_{vid}"):
                try:
                    with conn:
                        conn.execute("DELETE FROM vendors WHERE id=?", (vid,))
                    st.success("Vendor deleted")
                    st.rerun()
                except Exception as e:
                    st.error(f"Delete failed: {e}")

            if chk:
                selected_ids.append(int(row["id"]))
        c1, c2 = st.columns([2, 2])
        with c1:
            if st.button("Send to Outreach ▶") and selected_ids:
                st.session_state["rfq_vendor_ids"] = selected_ids
                st.success(f"Queued {len(selected_ids)} vendors for Outreach")
        with c2:
            st.caption("Selections are stored in session and available in Outreach tab")
# ---------- Outreach (Phase D) ----------
    try:
        _rid = locals().get('rfp_id') or locals().get('rid') or st.session_state.get('current_rfp_id')
        y6_render_co_box(conn if 'conn' in locals() else None, _rid, key_prefix="run_subcontractor_finder_y6", title="CO guidance for subcontractors")
    except Exception:
        pass

def _smtp_settings() -> Dict[str, Any]:
    out = {"host": None, "port": 587, "username": None, "password": None, "from_email": None, "from_name": "ELA Management", "use_tls": True}
    try:
        cfg = st.secrets.get("smtp", {})
        out.update({k: cfg.get(k, out[k]) for k in out})
    except Exception:
        pass
    for k in list(out.keys()):
        if not out[k]:
            try:
                v = st.secrets.get(k)
                if v:
                    out[k] = v
            except Exception:
                pass
    return out

def send_email_smtp(to_email: str, subject: str, html_body: str, attachments: List[str]) -> Tuple[bool, str]:
    """
    Simple SMTP helper used by some outreach flows.

    This version:
    - Sends a standard text/html body.
    - Attaches files with a guessed MIME type instead of always application/octet-stream.
    - Skips extensions outside a conservative safelist.
    """
    cfg = _smtp_settings()
    if not all([cfg.get("host"), cfg.get("port"), cfg.get("username"), cfg.get("password"), cfg.get("from_email")]):
        return False, "Missing SMTP settings in secrets"

    msg = MIMEMultipart()
    msg["From"] = f"{cfg.get('from_name') or ''} <{cfg['from_email']}>"
    msg["To"] = to_email
    msg["Subject"] = subject or ""

    # Standard HTML body
    msg.attach(MIMEText(html_body or "", "html", "utf-8"))

    # Allow only common, generally safe attachment types
    safe_exts = {
        ".pdf", ".doc", ".docx", ".txt", ".rtf",
        ".xls", ".xlsx", ".csv",
        ".png", ".jpg", ".jpeg", ".gif"
    }

    for path in attachments or []:
        try:
            if not path:
                continue
            ext = (os.path.splitext(path)[1] or "").lower()
            if ext and ext not in safe_exts:
                # Skip potentially unsafe attachment types to reduce provider virus flags
                continue

            ctype, encoding = mimetypes.guess_type(path)
            if not ctype:
                ctype = "application/octet-stream"
            maintype, subtype = ctype.split("/", 1)

            with open(path, "rb") as f:
                file_data = f.read()

            # Choose a MIME class based on the maintype
            if maintype == "text":
                try:
                    part = MIMEText(file_data.decode("utf-8", "ignore"), subtype or "plain", "utf-8")
                except Exception:
                    part = MIMEBase(maintype, subtype)
                    part.set_payload(file_data)
                    encoders.encode_base64(part)
            elif maintype == "image":
                from email.mime.image import MIMEImage
                part = MIMEImage(file_data, _subtype=subtype or None)
            elif maintype == "application":
                from email.mime.application import MIMEApplication
                part = MIMEApplication(file_data, _subtype=subtype or "octet-stream")
            else:
                part = MIMEBase(maintype, subtype)
                part.set_payload(file_data)
                encoders.encode_base64(part)

            part.add_header("Content-Disposition", f'attachment; filename="{os.path.basename(path)}"')
            msg.attach(part)
        except Exception:
            # Ignore per-file issues so one bad attachment does not block the entire send
            pass

    try:
        server = smtplib.SMTP(cfg['host'], int(cfg['port']))
        if cfg.get('use_tls', True):
            server.starttls()
        server.login(cfg['username'], cfg['password'])
        server.sendmail(cfg['from_email'], [to_email], msg.as_string())
        server.quit()
        return True, "sent"
    except Exception as e:
        return False, str(e)


def _merge_text(t: str, vendor: Dict[str, Any], notice: Dict[str, Any]) -> str:
    repl = {
        "company": vendor.get("name", ""),
        "email": vendor.get("email", ""),
        "phone": vendor.get("phone", ""),
        "city": vendor.get("city", ""),
        "state": vendor.get("state", ""),
        "naics": vendor.get("naics", ""),
        "title": notice.get("Title", ""),
        "solicitation": notice.get("Solicitation", ""),
        "due": notice.get("Response Due", ""),
        "notice_id": notice.get("Notice ID", ""),
    }
    out = t
    for k, v in repl.items():
        out = out.replace(f"{{{{{k}}}}}", str(v))
    return out

def _calc_extended(qty: Optional[float], unit_price: Optional[float]) -> Optional[float]:
    try:
        if qty is None or unit_price is None:
            return None
        return float(qty) * float(unit_price)
    except Exception:
        return None

def run_quote_comparison(conn: "sqlite3.Connection") -> None:
    st.header("Quote Comparison")
    st.caption("Use this page to compare vendor quotes side by side so you can pick competitive and profitable pricing.")
    df = pd.read_sql_query("SELECT id, title, solnum FROM rfps_t ORDER BY id DESC;", conn, params=())
    if df.empty:
        st.info("No RFPs in DB. Use RFP Analyzer to create one (Parse → Save).")
        return
    rfp_id = st.selectbox("RFP context", options=df["id"].tolist(), format_func=lambda rid: f"#{rid} — {df.loc[df['id']==rid, 'title'].values[0] or 'Untitled'}")

    st.subheader("Upload / Add Quotes")
    with st.expander("CSV Import", expanded=False):
        st.caption("Columns: vendor, clin, qty, unit_price, description (optional). One row = one CLIN line.")
        up = st.file_uploader("Quotes CSV", type=["csv"], key="quotes_csv")
        if up and st.button("Import Quotes CSV"):
            try:
                df_csv = pd.read_csv(up)
                required = {"vendor", "clin", "qty", "unit_price"}
                if not required.issubset({c.lower() for c in df_csv.columns}):
                    st.error("CSV missing required columns: vendor, clin, qty, unit_price")
                else:
                    df_csv.rename(columns={c: c.lower() for c in df_csv.columns}, inplace=True)
                    with closing(conn.cursor()) as cur:
                        by_vendor = df_csv.groupby("vendor", dropna=False)
                        total_rows = 0
                        for vendor, block in by_vendor:
                            cur.execute(
                                "INSERT INTO quotes(rfp_id, vendor, received_date, notes) VALUES(?,?,?,?);",
                                (int(rfp_id), str(vendor)[:200], datetime.utcnow().isoformat(), "imported")
                            )
                            qid = cur.lastrowid
                            for _, r in block.iterrows():
                                qty = float(r.get("qty", 0) or 0)
                                upx = float(r.get("unit_price", 0) or 0)
                                ext = _calc_extended(qty, upx) or 0.0
                                cur.execute(
                                    """
                                    INSERT INTO quote_lines(
                                        quote_id,
                                        clin,
                                        description,
                                        qty,
                                        unit_price,
                                        extended_price
                                    )
                                    VALUES (?, ?, ?, ?, ?, ?);
                                    """
                                    , (
                                        qid,
                                        str(r.get("clin", ""))[:50],
                                        str(r.get("description", ""))[:300],
                                        qty,
                                        upx,
                                        ext,
                                    ),
                                )
                                total_rows += 1
                        conn.commit()
                    _recompute_quote_totals(conn, int(rfp_id))
                    st.success(f"Imported {len(by_vendor)} quotes / {total_rows} lines.")
            except Exception as e:
                st.error(f"Import failed: {e}")

    with st.expander("Add Quote (manual)", expanded=False):
        vendor = st.text_input("Vendor name")
        date = st.date_input("Received date", value=datetime.utcnow().date())
        notes = st.text_input("Notes", value="")
        add_quote = st.button("Create Quote")
        if add_quote and vendor.strip():
            with closing(conn.cursor()) as cur:
                cur.execute("INSERT INTO quotes(rfp_id, vendor, received_date, notes) VALUES(?,?,?,?);",
                            (int(rfp_id), vendor.strip(), date.isoformat(), notes.strip()))
                qid = cur.lastrowid
                conn.commit()
                st.success(f"Created quote for {vendor}. Now add lines below (Quote ID {qid}).")
                st.session_state["current_quote_id"] = qid

    df_q = pd.read_sql_query("SELECT id, vendor, received_date, notes FROM quotes WHERE rfp_id=? ORDER BY vendor;", conn, params=(rfp_id,))
    if not df_q.empty:
        st.subheader("Quotes")
        _styled_dataframe(df_q, use_container_width=True, hide_index=True)
        qid = st.selectbox("Edit lines for quote", options=df_q["id"].tolist(), format_func=lambda qid: f"#{qid} — {df_q.loc[df_q['id']==qid,'vendor'].values[0]}")
        with st.form("add_quote_line", clear_on_submit=True):
            c1, c2, c3 = st.columns([2, 1, 1])
            with c1:
                clin = st.text_input("CLIN")
                desc = st.text_input("Description")
            with c2:
                qty = st.number_input("Qty", min_value=0.0, step=1.0)
            with c3:
                price = st.number_input("Unit Price", min_value=0.0, step=1.0)
            submitted = st.form_submit_button("Add Line")
        if submitted:
            ext = _calc_extended(qty, price) or 0.0
            with closing(conn.cursor()) as cur:
                cur.execute(
                    """
                    INSERT INTO quote_lines(
                        quote_id,
                        clin,
                        description,
                        qty,
                        unit_price,
                        extended_price
                    )
                    VALUES (?, ?, ?, ?, ?, ?);
                    """
                    , (qid, clin.strip(), desc.strip(), float(qty), float(price), float(ext)),
                )
                conn.commit()
            _recompute_quote_totals(conn, int(rfp_id))
            st.success("Line added.")

    st.subheader("Comparison")
    df_target = pd.read_sql_query("SELECT clin, description FROM clin_lines WHERE rfp_id=? GROUP BY clin, description ORDER BY clin;", conn, params=(rfp_id,))
    df_lines = pd.read_sql_query("""
        SELECT q.vendor, l.clin, l.qty, l.unit_price, l.extended_price
        FROM quote_lines l
        JOIN quotes q ON q.id = l.quote_id
        WHERE q.rfp_id=?
    """, conn, params=(rfp_id,))
    if df_lines.empty:
        st.info("No quote lines yet.")
        return

    mat = df_lines.pivot_table(index="clin", columns="vendor", values="extended_price", aggfunc="sum").fillna(0.0)
    mat = mat.sort_index()
    _styled_dataframe(mat.style.format("{:,.2f}"), use_container_width=True)

    best_vendor_by_clin = mat.replace(0, float("inf")).idxmin(axis=1).to_frame("Best Vendor")
    st.caption("Best vendor per CLIN")
    _styled_dataframe(best_vendor_by_clin, use_container_width=True, hide_index=False)

    totals = df_lines.groupby("vendor")["extended_price"].sum().to_frame("Total").sort_values("Total")
    if not df_target.empty:
        coverage = df_lines.groupby("vendor")["clin"].nunique().to_frame("CLINs Quoted")
        coverage["Required CLINs"] = df_target["clin"].nunique()
        coverage["Coverage %"] = (coverage["CLINs Quoted"] / coverage["Required CLINs"] * 100).round(1)
        totals = totals.join(coverage, how="left")
    st.subheader("Totals & Coverage")
    _styled_dataframe(totals.style.format({"Total": "{:,.2f}", "Coverage %": "{:.1f}"}), use_container_width=True)

    if st.button("Export comparison CSV"):
        path = os.path.join(DATA_DIR, "quote_comparison.csv")
        out = mat.copy()
        out["Best Vendor"] = best_vendor_by_clin["Best Vendor"]
        out.to_csv(path)
        st.success("Exported.")
        try:
            with open(path, "rb") as _f:
                _data = _f.read()
            st.download_button(
                "Download comparison CSV",
                data=_data,
                file_name=os.path.basename(path),
                mime="text/csv",
                key="dl_quote_comparison_csv",
            )
        except Exception as _e:
            st.error(f"Download unavailable: {_e}")

# ---------- Pricing Calculator (Phase E) ----------
def _scenario_summary(conn: "sqlite3.Connection", scenario_id: int) -> Dict[str, float]:
    dl = pd.read_sql_query("SELECT hours, rate, fringe_pct FROM pricing_labor WHERE scenario_id=?;", conn, params=(scenario_id,))
    other = pd.read_sql_query("SELECT cost FROM pricing_other WHERE scenario_id=?;", conn, params=(scenario_id,))
    base = pd.read_sql_query("SELECT overhead_pct, gna_pct, fee_pct, contingency_pct FROM pricing_scenarios WHERE id=?;", conn, params=(scenario_id,))
    if base.empty:
        return {}
    overhead_pct, gna_pct, fee_pct, contingency_pct = base.iloc[0]
    direct_labor = float((dl["hours"] * dl["rate"]).sum()) if not dl.empty else 0.0
    fringe = float((dl["hours"] * dl["rate"] * (dl["fringe_pct"].fillna(0.0) / 100)).sum()) if not dl.empty else 0.0
    other_dir = float(other["cost"].sum()) if not other.empty else 0.0
    overhead = (direct_labor + fringe) * (float(overhead_pct) / 100.0)
    gna = (direct_labor + fringe + overhead + other_dir) * (float(gna_pct) / 100.0)
    subtotal = direct_labor + fringe + overhead + gna + other_dir
    contingency = subtotal * (float(contingency_pct) / 100.0)
    fee = (subtotal + contingency) * (float(fee_pct) / 100.0)
    total = subtotal + contingency + fee
    return {
        "Direct Labor": round(direct_labor, 2),
        "Fringe": round(fringe, 2),
        "Overhead": round(overhead, 2),
        "G&A": round(gna, 2),
        "Other Direct": round(other_dir, 2),
        "Contingency": round(contingency, 2),
        "Fee/Profit": round(fee, 2),
        "Total": round(total, 2),
    }

def run_pricing_calculator(conn: "sqlite3.Connection") -> None:
    st.header("Pricing Calculator")
    st.caption("Use this page to build quick should cost estimates and translate inputs into structured pricing.")
    df = pd.read_sql_query("SELECT id, title FROM rfps_t ORDER BY id DESC;", conn, params=())
    if df.empty:
        st.info("No RFP context. Use RFP Analyzer (parse & save) first.")
        return
    rfp_id = st.selectbox("RFP context", options=df["id"].tolist(), format_func=lambda rid: f"#{rid} — {df.loc[df['id']==rid, 'title'].values[0]}")

    st.subheader("Scenario")
    df_sc = pd.read_sql_query("SELECT id, name FROM pricing_scenarios WHERE rfp_id=? ORDER BY id DESC;", conn, params=(rfp_id,))
    mode = st.radio("Mode", ["Create new", "Edit existing"], horizontal=True)
    if mode == "Create new":
        name = st.text_input("Scenario name", value="Base")
        c1, c2, c3, c4 = st.columns(4)
        with c1:
            overhead = st.number_input("Overhead %", min_value=0.0, value=20.0, step=1.0)
        with c2:
            gna = st.number_input("G&A %", min_value=0.0, value=10.0, step=1.0)
        with c3:
            fee = st.number_input("Fee/Profit %", min_value=0.0, value=7.0, step=0.5)
        with c4:
            contingency = st.number_input("Contingency %", min_value=0.0, value=0.0, step=0.5)
        if st.button("Create scenario", type="primary"):
            with closing(conn.cursor()) as cur:
                cur.execute("""
                    INSERT INTO pricing_scenarios(rfp_id, name, overhead_pct, gna_pct, fee_pct, contingency_pct, created_at)
                    VALUES (?, ?, ?, ?, ?, ?, ?);
                """, (int(rfp_id), name.strip(), float(overhead), float(gna), float(fee), float(contingency), datetime.utcnow().isoformat()))
                conn.commit()
            st.success("Scenario created.")
            st.rerun()
        return
    else:
        if df_sc.empty:
            st.info("No scenarios yet. Switch to 'Create new'.")
            return
        scenario_id = st.selectbox("Pick a scenario", options=df_sc["id"].tolist(), format_func=lambda sid: df_sc.loc[df_sc["id"]==sid, "name"].values[0])

    st.subheader("Labor")
    with st.form("add_labor", clear_on_submit=True):
        c1, c2, c3, c4 = st.columns([2, 1, 1, 1])
        with c1:
            cat = st.text_input("Labor Category")
        with c2:
            hrs = st.number_input("Hours", min_value=0.0, step=1.0)
        with c3:
            rate = st.number_input("Rate", min_value=0.0, step=1.0)
        with c4:
            fringe = st.number_input("Fringe %", min_value=0.0, value=0.0, step=0.5)
        add_lab = st.form_submit_button("Add labor row")
    if add_lab:
        with closing(conn.cursor()) as cur:
            cur.execute("""
                INSERT INTO pricing_labor(scenario_id, labor_cat, hours, rate, fringe_pct) VALUES (?, ?, ?, ?, ?);
            """, (int(scenario_id), cat.strip(), float(hrs), float(rate), float(fringe)))
            conn.commit()
        st.success("Added.")

    df_lab = pd.read_sql_query("""
        SELECT id, labor_cat, hours, rate, fringe_pct, (hours*rate) AS direct, (hours*rate*fringe_pct/100.0) AS fringe
        FROM pricing_labor WHERE scenario_id=?;
    """, conn, params=(scenario_id,))
    _styled_dataframe(df_lab, use_container_width=True, hide_index=True)

    st.subheader("Other Direct Costs")
    with st.form("add_odc", clear_on_submit=True):
        c1, c2 = st.columns([3, 1])
        with c1:
            label = st.text_input("Label")
        with c2:
            cost = st.number_input("Cost", min_value=0.0, step=100.0)
        add_odc = st.form_submit_button("Add ODC")
    if add_odc:
        with closing(conn.cursor()) as cur:
            cur.execute("INSERT INTO pricing_other(scenario_id, label, cost) VALUES(?, ?, ?);", (int(scenario_id), label.strip(), float(cost)))
            conn.commit()
        st.success("Added ODC.")

    df_odc = pd.read_sql_query("SELECT id, label, cost FROM pricing_other WHERE scenario_id=?;", conn, params=(scenario_id,))
    _styled_dataframe(df_odc, use_container_width=True, hide_index=True)

    st.subheader("Summary")
    s = _scenario_summary(conn, int(scenario_id))
    if not s:
        st.info("Add labor/ODCs to see a summary.")
        return
    df_sum = pd.DataFrame(list(s.items()), columns=["Component", "Amount"])
    _styled_dataframe(df_sum.style.format({"Amount": "{:,.2f}"}), use_container_width=True, hide_index=True)

    csv_bytes = df_sum.to_csv(index=False).encode("utf-8")
    st.download_button(
        "Download pricing CSV",
        data=csv_bytes,
        file_name=f"pricing_scenario_{int(scenario_id)}.csv",
        mime="text/csv",
    )

# ---------- Win Probability (Phase E) ----------
def _price_competitiveness(conn: "sqlite3.Connection", rfp_id: int, our_total: Optional[float]) -> Optional[float]:
    df = pd.read_sql_query(""" SELECT vendor, total FROM quote_totals WHERE rfp_id=? ORDER BY total ASC; """, conn, params=(rfp_id,))
    if df.empty or our_total is None:
        return None
    comp_min = float(df["total"].min())
    if our_total <= comp_min:
        return 100.0
    ratio = (our_total - comp_min) / comp_min
    if ratio <= 0.05:
        return 85 + (0.05 - ratio) * (15/0.05)
    if ratio <= 0.10:
        return 70 + (0.10 - ratio) * (15/0.05)
    if ratio <= 0.25:
        return 70 * (0.25 - ratio) / 0.15
    return 0.0

def run_win_probability(conn: "sqlite3.Connection") -> None:
    st.header("Win Probability")
    st.caption("Use this page to score opportunities and estimate win probability based on fit, competition, and strategy.")
    df = pd.read_sql_query("SELECT id, title FROM rfps_t ORDER BY id DESC;", conn, params=())
    if df.empty:
        st.info("No RFP context. Use RFP Analyzer first.")
        return
    rfp_id = st.selectbox("RFP context", options=df["id"].tolist(), format_func=lambda rid: f"#{rid} — {df.loc[df['id']==rid, 'title'].values[0]}")

    df_items = pd.read_sql_query("SELECT status FROM lm_items WHERE rfp_id=?;", conn, params=(rfp_id,))
    if df_items.empty:
        compliance = st.slider("Compliance (est.)", 0, 100, 70)
    else:
        done = (df_items["status"] == "Complete").sum()
        total = len(df_items)
        compliance = int(round(done / max(1, total) * 100))

    tech = st.slider("Technical fit", 0, 100, 75)
    past_perf = st.slider("Past performance relevance", 0, 100, 70)
    team = st.slider("Team strength / subs readiness", 0, 100, 70)
    smallbiz = st.slider("Set-aside / socio-economic alignment", 0, 100, 80)

    df_sc = pd.read_sql_query("SELECT id, name FROM pricing_scenarios WHERE rfp_id=? ORDER BY id DESC;", conn, params=(rfp_id,))
    price_score = None
    our_total = None
    if not df_sc.empty:
        sid = st.selectbox("Use pricing scenario (optional)", options=[None] + df_sc["id"].tolist(),
                           format_func=lambda x: "None" if x is None else df_sc.loc[df_sc["id"]==x, "name"].values[0])
        if sid:
            our_total = _scenario_summary(conn, int(sid)).get("Total")
    if our_total is None:
        our_total = st.number_input("Our total price (if no scenario)", min_value=0.0, value=0.0, step=1000.0)
    price_score = _price_competitiveness(conn, int(rfp_id), our_total)
    if price_score is None:
        price_score = st.slider("Price competitiveness (est.)", 0, 100, 70)

    st.subheader("Weights")
    c1, c2, c3 = st.columns(3)
    with c1:
        w_comp = st.number_input("Weight: Compliance", 0, 100, 20)
        w_tech = st.number_input("Weight: Technical", 0, 100, 25)
    with c2:
        w_past = st.number_input("Weight: Past Perf", 0, 100, 15)
        w_team = st.number_input("Weight: Team", 0, 100, 15)
    with c3:
        w_price = st.number_input("Weight: Price", 0, 100, 25)
        w_small = st.number_input("Weight: Small Biz", 0, 100, 0)
    total_w = w_comp + w_tech + w_past + w_team + w_price + w_small
    if total_w == 0:
        st.error("Weights must sum to > 0")
        return

    comp = {
        "Compliance": compliance,
        "Technical": tech,
        "Past Performance": past_perf,
        "Team": team,
        "Price": int(round(price_score)),
        "Small Business": smallbiz,
    }
    df_scores = pd.DataFrame(list(comp.items()), columns=["Factor", "Score (0-100)"])
    _styled_dataframe(df_scores, use_container_width=True, hide_index=True)

    weighted = (
        compliance * w_comp + tech * w_tech + past_perf * w_past + team * w_team + int(round(price_score)) * w_price + smallbiz * w_small
    ) / total_w
    win_prob = round(float(weighted), 1)
    st.subheader(f"Estimated Win Probability: **{win_prob}%**")

    if st.button("Export assessment CSV"):
        path = os.path.join(DATA_DIR, "win_probability_assessment.csv")
        out = df_scores.copy()
        out.loc[len(out)] = ["Weighted Result", win_prob]
        out.to_csv(path, index=False)
        st.success("Exported.")
        try:
            with open(path, "rb") as _f:
                _data = _f.read()
            st.download_button(
                "Download assessment CSV",
                data=_data,
                file_name=os.path.basename(path),
                mime="text/csv",
                key="dl_win_assessment_csv",
            )
        except Exception as _e:
            st.error(f"Download unavailable: {_e}")

# ---------- Phase F: Chat Assistant (rules-based over DB) ----------
def _kb_search(conn: "sqlite3.Connection", rfp_id: Optional[int], query: str) -> Dict[str, Any]:
    q = query.lower()
    res: Dict[str, Any] = {}
    # RFP sections
    if rfp_id:
        dfL = pd.read_sql_query("SELECT section, content FROM rfp_sections WHERE rfp_id=?;", conn, params=(rfp_id,))
    else:
        dfL = pd.read_sql_query("SELECT section, content FROM rfp_sections;", conn, params=())
    if not dfL.empty:
        dfL["score"] = dfL["content"].str.lower().apply(lambda t: sum(1 for w in q.split() if w in (t or "")))
        res["sections"] = dfL.sort_values("score", ascending=False).head(5)

    # Checklist
    if rfp_id:
        dfCk = pd.read_sql_query("SELECT item_text, status FROM lm_items WHERE rfp_id=?;", conn, params=(rfp_id,))
    else:
        dfCk = pd.read_sql_query("SELECT item_text, status FROM lm_items;", conn, params=())
    if not dfCk.empty:
        dfCk["score"] = dfCk["item_text"].str.lower().apply(lambda t: sum(1 for w in q.split() if w in (t or "")))
        res["checklist"] = dfCk.sort_values("score", ascending=False).head(10)

    # CLINs
    if rfp_id:
        dfCL = pd.read_sql_query("SELECT clin, description, qty, unit FROM clin_lines WHERE rfp_id=?;", conn, params=(rfp_id,))
    else:
        dfCL = pd.read_sql_query("SELECT clin, description, qty, unit FROM clin_lines;", conn, params=())
    if not dfCL.empty:
        dfCL["score"] = (dfCL["clin"].astype(str) + " " + dfCL["description"].astype(str)).str.lower().apply(lambda t: sum(1 for w in q.split() if w in (t or "")))
        res["clins"] = dfCL.sort_values("score", ascending=False).head(10)

    # Dates
    if rfp_id:
        dfDt = pd.read_sql_query("SELECT label, date_text FROM key_dates WHERE rfp_id=?;", conn, params=(rfp_id,))
    else:
        dfDt = pd.read_sql_query("SELECT label, date_text FROM key_dates;", conn, params=())
    if not dfDt.empty:
        dfDt["score"] = (dfDt["label"].astype(str) + " " + dfDt["date_text"].astype(str)).str.lower().apply(lambda t: sum(1 for w in q.split() if w in (t or "")))
        res["dates"] = dfDt.sort_values("score", ascending=False).head(10)

    # POCs
    if rfp_id:
        dfP = pd.read_sql_query("SELECT name, role, email, phone FROM pocs WHERE rfp_id=?;", conn, params=(rfp_id,))
    else:
        dfP = pd.read_sql_query("SELECT name, role, email, phone FROM pocs;", conn, params=())
    if not dfP.empty:
        dfP["score"] = (dfP["name"].astype(str) + " " + dfP["role"].astype(str) + " " + dfP["email"].astype(str)).str.lower().apply(lambda t: sum(1 for w in q.split() if w in (t or "")))
        res["pocs"] = dfP.sort_values("score", ascending=False).head(10)

    # Quotes summary by vendor
    if rfp_id:
        dfQ = pd.read_sql_query("""
            SELECT q.vendor, SUM(l.extended_price) AS total, COUNT(DISTINCT l.clin) AS clins_quoted
            FROM quotes q JOIN quote_lines l ON q.id=l.quote_id
            WHERE q.rfp_id=?
            GROUP BY q.vendor
            ORDER BY total ASC;
        """, conn, params=(rfp_id,))
        res["quotes"] = dfQ

    # Coverage & compliance
    if rfp_id:
        df_target = pd.read_sql_query("SELECT DISTINCT clin FROM clin_lines WHERE rfp_id=?;", conn, params=(rfp_id,))
        total_clins = int(df_target["clin"].nunique()) if not df_target.empty else 0
        df_items = pd.read_sql_query("SELECT status FROM lm_items WHERE rfp_id=?;", conn, params=(rfp_id,))
        compl = 0
        if not df_items.empty:
            compl = int(round(((df_items["status"]=="Complete").sum() / max(1, len(df_items))) * 100))
        res["meta"] = {"total_clins": total_clins, "compliance_pct": compl}

    return res


def _list_rfps(conn) -> list[tuple[str, str]]:
    """Return list of (rfp_id, label). Defensive over possible schemas."""
    try:
        cur = conn.cursor()
        cur.execute("SELECT name FROM sqlite_master WHERE type='table'")
        tables = [r[0] for r in cur.fetchall()]
    except Exception:
        return []
    # candidate tables likely to hold RFP metadata
    candidates = [t for t in tables if re.search(r"rfp", t, re.I)]
    results = []
    for t in candidates:
        # probe common id/title columns
        id_cols = ["id","rfp_id","rid"]
        title_cols = ["title","name","subject","rfp_title"]
        for idc in id_cols:
            for tc in title_cols:
                try:
                    cur.execute(f"SELECT {idc}, {tc} FROM {t} ORDER BY 1 DESC LIMIT 50")
                    for rid, title in cur.fetchall():
                        rid_s = str(rid)
                        title_s = str(title) if title is not None else ""
                        label = f"{rid_s} — {title_s} [{t}]"
                        results.append((rid_s, label))
                    if results:
                        return results
                except Exception:
                    pass
        # fallback: only id
        for idc in id_cols:
            try:
                cur.execute(f"SELECT {idc} FROM {t} ORDER BY 1 DESC LIMIT 50")
                for (rid,) in cur.fetchall():
                    rid_s = str(rid)
                    label = f"{rid_s} [{t}]"
                    results.append((rid_s, label))
                if results:
                    return results
            except Exception:
                pass
    return results

def _load_rfp_context(conn, rfp_id: str, max_chars: int = 200000) -> str:
    """Try multiple table/column patterns to get joined text for a given rfp_id."""
    if not conn or not rfp_id:
        return ""
    try:
        cur = conn.cursor()
        cur.execute("SELECT name FROM sqlite_master WHERE type='table'")
        tables = [r[0] for r in cur.fetchall()]
    except Exception:
        return ""
    candidates = [t for t in tables if re.search(r"rfp", t, re.I)]
    text_cols = ["text","content","full_text","page_text","body"]
    id_cols = ["rfp_id","id","rid"]
    chunks = []
    for t in candidates:
        for idc in id_cols:
            for tc in text_cols:
                try:
                    cur.execute(f"SELECT {tc} FROM {t} WHERE {idc}=? LIMIT 500", (rfp_id,))
                    rows = cur.fetchall()
                    if rows:
                        for (val,) in rows:
                            if isinstance(val, (bytes, bytearray)):
                                try:
                                    val = val.decode("utf-8", errors="ignore")
                                except Exception:
                                    val = val.decode("latin-1", errors="ignore")
                            if val:
                                chunks.append(str(val))
                except Exception:
                    pass
    text = "\n\n---\n\n".join(chunks)
    if not text and candidates:
        # last resort: any rows from 'context' tables
        for t in candidates:
            if re.search(r"context|pages|docs", t, re.I):
                for tc in text_cols:
                    try:
                        cur.execute(f"SELECT {tc} FROM {t} LIMIT 200")
                        rows = cur.fetchall()
                        if rows:
                            for (val,) in rows[:50]:
                                if isinstance(val, (bytes, bytearray)):
                                    try:
                                        val = val.decode("utf-8", errors="ignore")
                                    except Exception:
                                        val = val.decode("latin-1", errors="ignore")
                                if val:
                                    chunks.append(str(val))
                    except Exception:
                        pass
                if chunks:
                    break
    return (text or "\n\n---\n\n".join(chunks))[:max_chars]


def run_chat_assistant(conn: "sqlite3.Connection") -> None:
    import streamlit as st
    st.header("Chat Assistant — Y2")
    st.caption("Use this page to ask questions, summarize documents, and get on demand capture and proposal support without leaving the app.")
    st.caption("CO Chat with memory. Uses One Page Analyzer context. Persists per RFP.")
    try:
        y2_ui_threaded_chat(conn)
    except Exception as _e:
        st.error(f"Chat Assistant failed: {type(_e).__name__}: {_e}")

def _export_capability_docx(path: str, profile: Dict[str, str]) -> Optional[str]:
    try:
        from docx.shared import Pt, Inches  # type: ignore
    except Exception:
        st.error("python-docx is required. pip install python-docx")
        return None

    doc = docx.Document()
    for s in doc.sections:
        s.top_margin = Inches(0.7); s.bottom_margin = Inches(0.7); s.left_margin = Inches(0.7); s.right_margin = Inches(0.7)

    title = profile.get("company_name") or "Capability Statement"
    doc.add_heading(title, level=1)
    if profile.get("tagline"):
        p = doc.add_paragraph(profile["tagline"]); p.runs[0].italic = True

    meta = [
        ("Address", "address"), ("Phone", "phone"), ("Email", "email"), ("Website", "website"),
        ("UEI", "uei"), ("CAGE", "cage")
    ]
    p = doc.add_paragraph()
    for label, key in meta:
        val = profile.get(key, "")
        if val:
            p.add_run(f"{label}: {val}  ")

    def add_bullets(title, key):
        txt = (profile.get(key) or "").strip()
        if not txt:
            return
        doc.add_heading(title, level=2)
        for _line in [x.rstrip() for x in txt.splitlines()]:
            if not _line.strip():
                p = doc.add_paragraph("")
            elif re.match(r"^\d+[\.)]\s+", _line):
                doc.add_paragraph(re.sub(r"^\d+[\.)]\s+", "", _line).strip(), style="List Number")
            elif re.match(r"^[-*•]\s+", _line):
                doc.add_paragraph(re.sub(r"^[-*•]\s+", "", _line).strip(), style="List Bullet")
            else:
                doc.add_paragraph(_line.strip())

    # Content blocks
    add_bullets("Core Competencies", "core_competencies")
    add_bullets("Differentiators", "differentiators")
    add_bullets("Certifications", "certifications")
    add_bullets("Past Performance Highlights", "past_performance")

    naics = (profile.get("naics") or "").replace(",", ", ")
    if naics.strip():
        doc.add_heading("NAICS Codes", level=2)
        doc.add_paragraph(naics)

    contact = profile.get("primary_poc", "")
    if contact.strip():
        doc.add_heading("Primary POC", level=2)
        doc.add_paragraph(contact)

    doc.save(path)
    return path

def _orig_run_capability_statement(conn: "sqlite3.Connection") -> None:
    st.header("Capability Statement")
    st.caption("Store your company profile and export a polished 1-page DOCX capability statement.")

    # Load existing (id=1)
    df = pd.read_sql_query("SELECT * FROM org_profile WHERE id=1;", conn, params=())
    vals = df.iloc[0].to_dict() if not df.empty else {}

    with st.form("org_profile_form"):
        c1, c2 = st.columns([2,2])
        with c1:
            company_name = st.text_input("Company Name", value=vals.get("company_name",""))
            tagline = st.text_input("Tagline (optional)", value=vals.get("tagline",""))
            address = st.text_area("Address", value=vals.get("address",""), height=70)
            phone = st.text_input("Phone", value=vals.get("phone",""))
            email = st.text_input("Email", value=vals.get("email",""))
            website = st.text_input("Website", value=vals.get("website",""))
        with c2:
            uei = st.text_input("UEI", value=vals.get("uei",""))
            cage = st.text_input("CAGE", value=vals.get("cage",""))
            naics = st.text_input("NAICS (comma separated)", value=vals.get("naics",""))
            core_competencies = st.text_area("Core Competencies (one per line)", value=vals.get("core_competencies",""), height=110)
            differentiators = st.text_area("Differentiators (one per line)", value=vals.get("differentiators",""), height=110)
        c3, c4 = st.columns([2,2])
        with c3:
            certifications = st.text_area("Certifications (one per line)", value=vals.get("certifications",""), height=110)
        with c4:
            past_performance = st.text_area("Past Performance Highlights (one per line)", value=vals.get("past_performance",""), height=110)
            primary_poc = st.text_area("Primary POC (name, title, email, phone)", value=vals.get("primary_poc",""), height=70)
        saved = st.form_submit_button("Save Profile", type="primary")

    if saved:
        try:
            with closing(conn.cursor()) as cur:
                cur.execute("DELETE FROM org_profile WHERE id=1;")
                cur.execute("""
                    INSERT INTO org_profile(id, company_name, tagline, address, phone, email, website, uei, cage, naics, core_competencies, differentiators, certifications, past_performance, primary_poc)
                    VALUES(1,?,?,?,?,?,?,?,?,?,?,?,?,?,?);
                """, (company_name, tagline, address, phone, email, website, uei, cage, naics, core_competencies, differentiators, certifications, past_performance, primary_poc))
                conn.commit()
            st.success("Profile saved.")
        except Exception as e:
            st.error(f"Save failed: {e}")

    # Export
    if st.button("Export Capability Statement DOCX"):
        prof = pd.read_sql_query("SELECT * FROM org_profile WHERE id=1;", conn, params=())
        if prof.empty:
            st.error("Save your profile first.")
        else:
            p = prof.iloc[0].to_dict()
            path = os.path.join(DATA_DIR, "Capability_Statement.docx")
            out = _export_capability_docx(path, p)
            if out:
                st.success("Exported.")
                
                try:
                    with open(out, "rb") as _f:
                        _data = _f.read()
                    st.download_button(
                        "Download DOCX",
                        data=_data,
                        file_name=os.path.basename(out) or "export.docx",
                        mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                        key=f"dl_out_docx"
                    )
                except Exception as _e:
                    st.error(f"Download unavailable: {_e}")

# ---------- Phase G: Past Performance Library + Generator ----------
def _pp_score_one(rec: dict, rfp_title: str, rfp_sections: pd.DataFrame) -> int:
    title = (rfp_title or "").lower()
    hay = (title + " " + " ".join((rfp_sections["content"].tolist() if isinstance(rfp_sections, pd.DataFrame) and not rfp_sections.empty else []))).lower()
    score = 0
    # NAICS bonus
    if rec.get("naics") and rec["naics"] in hay:
        score += 40
    # Keywords
    kws = (rec.get("keywords") or "").lower().replace(";", ",").split(",")
    kws = [k.strip() for k in kws if k.strip()]
    for k in kws[:10]:
        if k in hay:
            score += 6
    # Recency via POP end
    try:
        from datetime import datetime
        if rec.get("pop_end"):
            y = int(str(rec["pop_end"]).split("-")[0])
            age = max(0, datetime.now().year - y)
            score += max(0, 20 - (age * 4))  # up to +20, decays 4/yr
    except Exception:
        pass
    # CPARS bonus
    if (rec.get("cpars_rating") or "").strip():
        score += 8
    # Value signal
    try:
        val = float(rec.get("value") or 0)
        if val >= 1000000: score += 6
        elif val >= 250000: score += 3
    except Exception:
        pass
    return min(score, 100)

def _pp_writeup_block(rec: dict) -> str:
    parts = []
    title = rec.get("project_title") or "Project"
    cust = rec.get("customer") or ""
    cn = rec.get("contract_no") or ""
    role = rec.get("role") or ""
    pop = " – ".join([x for x in [rec.get("pop_start") or "", rec.get("pop_end") or ""] if x])
    val = rec.get("value") or ""
    parts.append(f"**{title}** — {cust} {('(' + cn + ')') if cn else ''}")
    meta_bits = [b for b in [f"Role: {role}" if role else "", f"POP: {pop}" if pop else "", f"Value: ${val:,.0f}" if isinstance(val,(int,float)) else (f"Value: {val}" if val else ""), f"NAICS: {rec.get('naics','')}"] if b]
    if meta_bits:
        parts.append("  \n" + " | ".join(meta_bits))
    if rec.get("scope"):
        parts.append(f"**Scope/Work:** {rec['scope']}")
    if rec.get("results"):
        parts.append(f"**Results/Outcome:** {rec['results']}")
    if rec.get("cpars_rating"):
        parts.append(f"**CPARS:** {rec['cpars_rating']}")
    if any([rec.get("contact_name"), rec.get("contact_email"), rec.get("contact_phone")]):
        parts.append("**POC:** " + ", ".join([x for x in [rec.get("contact_name"), rec.get("contact_email"), rec.get("contact_phone")] if x]))
    return "\n\n".join(parts)

def run_past_performance(conn: "sqlite3.Connection") -> None:
    st.header("Past Performance Library")
    st.caption("Store/import projects, score relevance vs an RFP, generate writeups, and push to Proposal Builder.")

    # CSV Import
    with st.expander("Import CSV", expanded=False):
        st.caption("Columns: project_title, customer, contract_no, naics, role, pop_start, pop_end, value, scope, results, cpars_rating, contact_name, contact_email, contact_phone, keywords, notes")
        up = st.file_uploader("Upload CSV", type=["csv"], key="pp_csv")
        if up and st.button("Import", key="pp_do_import"):
            try:
                df = pd.read_csv(up)
                # Normalize headers
                df.columns = [c.strip().lower() for c in df.columns]
                required = {"project_title"}
                if not required.issubset(set(df.columns)):
                    st.error("CSV must include at least 'project_title'")
                else:
                    n=0
                    with closing(conn.cursor()) as cur:
                        for _, r in df.iterrows():
                            cur.execute("""
                                INSERT INTO past_perf(project_title, customer, contract_no, naics, role, pop_start, pop_end, value, scope, results, cpars_rating, contact_name, contact_email, contact_phone, keywords, notes)
                                VALUES (?, ?, ?, ?, ?,?,?,?,?,?,?,?,?,?,?,?);
                            """, (
                                str(r.get("project_title",""))[:200],
                                str(r.get("customer",""))[:200],
                                str(r.get("contract_no",""))[:100],
                                str(r.get("naics",""))[:20],
                                str(r.get("role",""))[:100],
                                str(r.get("pop_start",""))[:20],
                                str(r.get("pop_end",""))[:20],
                                float(r.get("value")) if str(r.get("value","")).strip() not in ("","nan") else None,
                                str(r.get("scope",""))[:2000],
                                str(r.get("results",""))[:2000],
                                str(r.get("cpars_rating",""))[:100],
                                str(r.get("contact_name",""))[:200],
                                str(r.get("contact_email",""))[:200],
                                str(r.get("contact_phone",""))[:100],
                                str(r.get("keywords",""))[:500],
                                str(r.get("notes",""))[:500],
                            ))
                            n+=1
                    conn.commit()
                    st.success(f"Imported {n} projects.")
            except Exception as e:
                st.error(f"Import failed: {e}")

    # Add Project
    with st.expander("Add Project", expanded=False):
        c1, c2, c3 = st.columns([2,2,2])
        with c1:
            project_title = st.text_input("Project Title")
            customer = st.text_input("Customer (Agency/Prime)")
            contract_no = st.text_input("Contract #")
            naics = st.text_input("NAICS")
            role = st.text_input("Role (Prime/Sub)")
        with c2:
            pop_start = st.text_input("POP Start (YYYY-MM)")
            pop_end = st.text_input("POP End (YYYY-MM)")
            value = st.text_input("Value (number)")
            cpars_rating = st.text_input("CPARS Rating (optional)")
            keywords = st.text_input("Keywords (comma-separated)")
        with c3:
            contact_name = st.text_input("POC Name")
            contact_email = st.text_input("POC Email")
            contact_phone = st.text_input("POC Phone")
            scope = st.text_area("Scope/Work", height=100)
            results = st.text_area("Results/Outcome", height=100)
        notes = st.text_area("Notes", height=70)
        if st.button("Save Project", key="pp_save_project"):
            try:
                v = float(value) if value.strip() else None
            except Exception:
                v = None
            try:
                with closing(conn.cursor()) as cur:
                    cur.execute("""
                        INSERT INTO past_perf(project_title, customer, contract_no, naics, role, pop_start, pop_end, value, scope, results, cpars_rating, contact_name, contact_email, contact_phone, keywords, notes)
                        VALUES (?, ?, ?, ?, ?,?,?,?,?,?,?,?,?,?,?,?);
                    """, (project_title.strip(), customer.strip(), contract_no.strip(), naics.strip(), role.strip(), pop_start.strip(), pop_end.strip(), v, scope.strip(), results.strip(), cpars_rating.strip(), contact_name.strip(), contact_email.strip(), contact_phone.strip(), keywords.strip(), notes.strip()))
                    conn.commit()
                st.success("Saved project.")
            except Exception as e:
                st.error(f"Save failed: {e}")

    # Filters
    with st.expander("Filter", expanded=True):
        f1, f2, f3 = st.columns([2,2,2])
        with f1:
            f_kw = st.text_input("Keyword in title/scope/results")
        with f2:
            f_naics = st.text_input("NAICS filter")
        with f3:
            f_role = st.text_input("Role filter")
    q = "SELECT * FROM past_perf WHERE 1=1"
    params = []
    if f_kw:
        q += " AND (project_title LIKE ? OR scope LIKE ? OR results LIKE ?)"
        params.extend([f"%{f_kw}%", f"%{f_kw}%", f"%{f_kw}%"])
    if f_naics:
        q += " AND naics LIKE ?"
        params.append(f"%{f_naics}%")
    if f_role:
        q += " AND role LIKE ?"
        params.append(f"%{f_role}%")
    df = pd.read_sql_query(q + " ORDER BY id DESC;", conn, params=params)
    if df.empty:
        st.info("No projects found.")
        return

    st.subheader("Projects")
    _styled_dataframe(df[["id","project_title","customer","contract_no","naics","role","pop_start","pop_end","value","cpars_rating"]], use_container_width=True, hide_index=True)
    selected_ids = st.multiselect("Select projects for writeup", options=df["id"].tolist(), format_func=lambda i: f"#{i} — {df.loc[df['id']==i, 'project_title'].values[0]}")

    # Relevance scoring vs RFP
    df_rf = pd.read_sql_query("SELECT id, title FROM rfps_t ORDER BY id DESC;", conn, params=())
    rfp_id = None
    if not df_rf.empty:
        rfp_id = st.selectbox("RFP context for relevance scoring (optional)", options=[None] + df_rf["id"].tolist(),
                              format_func=lambda rid: "None" if rid is None else f"#{rid} — {df_rf.loc[df_rf['id']==rid,'title'].values[0]}")
    if rfp_id:
        ctx = _load_rfp_context_struct(conn, int(rfp_id))
        title = (ctx["rfp"].iloc[0]["title"] if _df_nonempty(ctx.get("rfp")) else "")
        secs = ctx.get("sections", pd.DataFrame())
        # Compute scores
        scores = []
        for _, r in df.iterrows():
            scores.append(_pp_score_one(r.to_dict(), title, secs))
        df_sc = df.copy()
        df_sc["Relevance"] = scores
        st.subheader("Relevance vs selected RFP")
        _styled_dataframe(df_sc[["project_title","naics","role","pop_end","value","Relevance"]].sort_values("Relevance", ascending=False),
                     use_container_width=True, hide_index=True)

    # Generate writeups
    st.subheader("Generate Writeups")
    tone = st.selectbox("Template", ["Concise bullets", "Narrative paragraph"])
    max_n = st.slider("How many projects", 1, 7, min(3, len(selected_ids)) if selected_ids else 3)
    do_gen = st.button("Generate", type="primary")
    if do_gen:
        picked = df[df["id"].isin(selected_ids)].head(max_n).to_dict(orient="records")
        if not picked:
            st.error("Select at least one project.")
            return
        # Build markdown text
        blocks = []
        for r in picked:
            blk = _pp_writeup_block(r)
            if tone == "Concise bullets":
                # convert sentences to bullets
                bullets = []
                for line in blk.split("\n"):
                    line = line.strip()
                    if not line:
                        continue
                    if not line.startswith("**"):
                        bullets.append(f"- {line}")
                    else:
                        bullets.append(line)
                blocks.append("\n".join(bullets))
            else:
                blocks.append(blk)
        final_md = "\n\n".join(blocks)
        st.markdown("**Preview**")
        st.write(final_md)

        # Push to Proposal Builder section
        st.session_state["pb_section_Past Performance"] = final_md
        st.success("Pushed to Proposal Builder → Past Performance Summary")

        # Export DOCX
        out_path = str(Path(DATA_DIR) / "Past_Performance_Writeups.docx")
        _export_past_perf_docx(out_path, picked)
def _wp_load_template(conn: "sqlite3.Connection", template_id: int) -> pd.DataFrame:
    """Load sections for a given white paper template ordered by position."""
    return pd.read_sql_query(
        "SELECT id, position, title, body FROM white_template_sections WHERE template_id=? ORDER BY position ASC;",
        conn, params=(template_id,)
    )

def _wp_load_paper(conn: "sqlite3.Connection", paper_id: int) -> pd.DataFrame:
    return pd.read_sql_query(
        "SELECT id, position, title, body, image_path FROM white_paper_sections WHERE paper_id=? ORDER BY position ASC;",
        conn, params=(paper_id,)
    )

def _wp_export_docx(path: str, title: str, subtitle: str, sections: pd.DataFrame) -> Optional[str]:
    try:
        import docx  # type: ignore
    except Exception:
        try:
            from docx import Document  # type: ignore
            import docx  # type: ignore
        except Exception:
            pass

            st.error("python-docx is required. pip install python-docx")
            return None
    try:
        from docx.shared import Inches  # type: ignore
    except Exception:
        pass
    try:
        # Default White Paper styling; kept simple and consistent with Proposal Builder
        font_name = "Calibri"
        font_size_pt = 11
        line_spacing = 1.15

        doc = docx.Document()
        doc.add_heading(title or "Win Plan", 0)
        if subtitle:
            doc.add_paragraph(subtitle)
        if isinstance(sections, pd.DataFrame) and not sections.empty:
            for _, row in sections.iterrows():
                sec = str(row.get('Section') or row.get('section') or row.get('name') or row.get('title') or "Section")
                body = str(row.get('Content') or row.get('content') or row.get('text') or row.get('body') or "")
                doc.add_heading(sec, level=2)
                body = _pb_normalize_text(body or "")
                _pb__write_md(doc, body, font_name, font_size_pt, line_spacing)
        doc.save(path)
        return path
    except Exception as e:
        pass

        st.error(f"DOCX export failed: {e}")
        return None

def run_white_paper_builder(conn: "sqlite3.Connection") -> None:
    st.header("White Paper Builder")
    st.caption("Templates → Drafts → DOCX export. Can include images per section.")

    # --- Templates ---
    with st.expander("Templates", expanded=False):
        df_t = pd.read_sql_query("SELECT id, name, description, created_at FROM white_templates ORDER BY id DESC;", conn, params=())
        t_col1, t_col2 = st.columns([2,2])
        with t_col1:
            st.subheader("Create Template")
            t_name = st.text_input("Template name", key="wp_t_name")
            t_desc = st.text_area("Description", key="wp_t_desc", height=70)
            if st.button("Save Template", key="wp_t_save"):
                if not t_name.strip():
                    st.error("Name required")
                else:
                    with closing(conn.cursor()) as cur:
                        cur.execute("INSERT INTO white_templates(name, description, created_at) VALUES(?,?,datetime('now'));", (t_name.strip(), t_desc.strip()))
                        conn.commit()
                    st.success("Template saved"); st.rerun()
        with t_col2:
            if df_t.empty:
                st.info("No templates yet.")
            else:
                st.subheader("Edit Template Sections")
                t_sel = st.selectbox("Choose template", options=df_t["id"].tolist(), format_func=lambda tid: df_t.loc[df_t["id"]==tid, "name"].values[0], key="wp_t_sel")
                df_ts = _wp_load_template(conn, int(t_sel))
                _styled_dataframe(df_ts, use_container_width=True, hide_index=True)
                st.markdown("**Add section**")
                ts_title = st.text_input("Section title", key="wp_ts_title")
                ts_body = st.text_area("Default body", key="wp_ts_body", height=120)
                if st.button("Add section to template", key="wp_ts_add"):
                    pos = int((df_ts["position"].max() if not df_ts.empty else 0) + 1)
                    with closing(conn.cursor()) as cur:
                        cur.execute("INSERT INTO white_template_sections(template_id, position, title, body) VALUES(?,?,?,?);",
                                    (int(t_sel), pos, ts_title.strip(), ts_body.strip()))
                        conn.commit()
                    st.success("Section added"); st.rerun()
                # Reorder / delete (simple)
                if not df_ts.empty:
                    st.markdown("**Reorder / Delete**")
                    for _, r in df_ts.iterrows():
                        c1, c2, c3 = st.columns([2,1,1])
                        with c1:
                            new_pos = st.number_input(f"#{int(r['id'])} pos", min_value=1, value=int(r['position']), step=1, key=f"wp_ts_pos_{int(r['id'])}")
                        with c2:
                            if st.button("Apply", key=f"wp_ts_pos_apply_{int(r['id'])}"):
                                with closing(conn.cursor()) as cur:
                                    cur.execute("UPDATE white_template_sections SET position=? WHERE id=?;", (int(new_pos), int(r["id"])))
                                    conn.commit()
                                st.success("Updated position"); st.rerun()
                        with c3:
                            if st.button("Delete", key=f"wp_ts_del_{int(r['id'])}"):
                                with closing(conn.cursor()) as cur:
                                    cur.execute("DELETE FROM white_template_sections WHERE id=?;", (int(r["id"]),))
                                    conn.commit()
                                st.success("Deleted"); st.rerun()

    st.divider()

    # --- Drafts ---
    st.subheader("Drafts")
    df_p = pd.read_sql_query("SELECT id, title, subtitle, created_at, updated_at FROM white_papers ORDER BY id DESC;", conn, params=())
    c1, c2 = st.columns([2,2])
    with c1:
        st.markdown("**Create draft from template**")
        df_t = pd.read_sql_query("SELECT id, name FROM white_templates ORDER BY id DESC;", conn, params=())
        d_title = st.text_input("Draft title", key="wp_d_title")
        d_sub = st.text_input("Subtitle (optional)", key="wp_d_sub")
        if df_t.empty:
            st.caption("No templates available")
            t_sel2 = None
        else:
            t_sel2 = st.selectbox("Template", options=[None] + df_t["id"].tolist(),
                                  format_func=lambda x: "Blank" if x is None else df_t.loc[df_t["id"]==x, "name"].values[0],
                                  key="wp_d_template")
        if st.button("Create draft", key="wp_d_create"):
            if not d_title.strip():
                st.error("Title required")
            else:
                with closing(conn.cursor()) as cur:
                    cur.execute("INSERT INTO white_papers(title, subtitle, rfp_id, created_at, updated_at) VALUES (?, ?, ?, ?, ?);",
                                (d_title.strip(), d_sub.strip(), None, datetime.utcnow().isoformat(), datetime.utcnow().isoformat()))
                    pid = cur.lastrowid
                    if t_sel2:
                        df_ts2 = _wp_load_template(conn, int(t_sel2))
                        for _, r in df_ts2.sort_values("position").iterrows():
                            cur.execute("INSERT INTO white_paper_sections(paper_id, position, title, body) VALUES(?,?,?,?);",
                                        (int(pid), int(r["position"]), r.get("title"), r.get("body")))
                    conn.commit()
                st.success("Draft created"); st.rerun()
    with c2:
        if df_p.empty:
            st.info("No drafts yet.")
        else:
            st.markdown("**Open a draft**")
            p_sel = st.selectbox("Draft", options=df_p["id"].tolist(), format_func=lambda pid: df_p.loc[df_p["id"]==pid, "title"].values[0], key="wp_d_sel")

    # Editing panel
    if 'p_sel' in locals() and p_sel:
        st.subheader(f"Editing draft #{int(p_sel)}")
        df_sec = _wp_load_paper(conn, int(p_sel))
        # Add section
        st.markdown("**Add section**")
        ns_title = st.text_input("Section title", key="wp_ns_title")
        ns_body = st.text_area("Body", key="wp_ns_body", height=140)
        ns_img = st.file_uploader("Optional image", type=["png","jpg","jpeg"], key="wp_ns_img")
        if st.button("Add section", key="wp_ns_add"):
            img_path = None
            if ns_img is not None:
                img_path = save_uploaded_file(ns_img, subdir="whitepapers")
            pos = int((df_sec["position"].max() if not df_sec.empty else 0) + 1)
            with closing(conn.cursor()) as cur:
                cur.execute("INSERT INTO white_paper_sections(paper_id, position, title, body, image_path) VALUES (?, ?, ?, ?, ?);",
                            (int(p_sel), pos, ns_title.strip(), ns_body.strip(), img_path))
                cur.execute("UPDATE white_papers SET updated_at=datetime('now') WHERE id=?;", (int(p_sel),))
                conn.commit()
            st.success("Section added"); st.rerun()

        # Section list
        if df_sec.empty:
            st.info("No sections yet.")
        else:
            for _, r in df_sec.iterrows():
                st.markdown(f"**Section #{int(r['position'])}: {r.get('title') or 'Untitled'}**")
                e1, e2, e3, e4 = st.columns([2,1,1,1])
                with e1:
                    new_title = st.text_input("Title", value=r.get("title") or "", key=f"wp_sec_title_{int(r['id'])}")
                    new_body = st.text_area("Body", value=r.get("body") or "", key=f"wp_sec_body_{int(r['id'])}", height=140)
                with e2:
                    new_pos = st.number_input("Pos", value=int(r["position"]), min_value=1, step=1, key=f"wp_sec_pos_{int(r['id'])}")
                    if st.button("Apply", key=f"wp_sec_apply_{int(r['id'])}"):
                        with closing(conn.cursor()) as cur:
                            cur.execute("UPDATE white_paper_sections SET title=?, body=?, position=? WHERE id=?;",
                                        (new_title.strip(), new_body.strip(), int(new_pos), int(r["id"])))
                            cur.execute("UPDATE white_papers SET updated_at=datetime('now') WHERE id=?;", (int(p_sel),))
                            conn.commit()
                        st.success("Updated"); st.rerun()
                with e3:
                    up_img = st.file_uploader("Replace image", type=["png","jpg","jpeg"], key=f"wp_sec_img_{int(r['id'])}")
                    if st.button("Save image", key=f"wp_sec_img_save_{int(r['id'])}"):
                        if up_img is None:
                            st.warning("Choose an image first")
                        else:
                            img_path = save_uploaded_file(up_img, subdir="whitepapers")
                            with closing(conn.cursor()) as cur:
                                cur.execute("UPDATE white_paper_sections SET image_path=? WHERE id=?;", (img_path, int(r["id"])))
                                cur.execute("UPDATE white_papers SET updated_at=datetime('now') WHERE id=?;", (int(p_sel),))
                                conn.commit()
                            st.success("Image saved"); st.rerun()
                with e4:
                    if st.button("Delete", key=f"wp_sec_del_{int(r['id'])}"):
                        with closing(conn.cursor()) as cur:
                            cur.execute("DELETE FROM white_paper_sections WHERE id=?;", (int(r["id"]),))
                            cur.execute("UPDATE white_papers SET updated_at=datetime('now') WHERE id=?;", (int(p_sel),))
                            conn.commit()
                        st.success("Deleted"); st.rerun()
                st.divider()

            # Export & Push
            x1, x2 = st.columns([2,2])
            with x1:
                if st.button("Export DOCX", key="wp_export"):
                    out_path = str(Path(DATA_DIR) / f"White_Paper_{int(p_sel)}.docx")
                    exp = _wp_export_docx(
                        out_path,
                        df_p.loc[df_p["id"] == p_sel, "title"].values[0],
                        df_p.loc[df_p["id"] == p_sel, "subtitle"].values[0] if "subtitle" in df_p.columns else "",
                        _wp_load_paper(conn, int(p_sel)),
                    )
                    if exp:
                        st.success("Exported")
                        try:
                            from pathlib import Path as _Path
                            with open(exp, "rb") as _f:
                                _data = _f.read()
                            _fname = _Path(exp).name or "export.docx"
                            st.download_button(
                                "Download DOCX",
                                data=_data,
                                file_name=_fname,
                                mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                                key="wp_export_dl",
                            )
                        except Exception as _e:
                            st.error(f"Download failed: {_e}")

            with x2:

                if st.button("Push narrative to Proposal Builder", key="wp_push"):
                    # Concatenate sections to markdown
                    secs = _wp_load_paper(conn, int(p_sel))
                    lines = []
                    for _, rr in secs.sort_values("position").iterrows():
                        lines.append(f"## {rr.get('title') or 'Section'}\n\n{rr.get('body') or ''}")
                    md = "\n\n".join(lines)
                    st.session_state["pb_section_White Paper"] = md
                    st.success("Pushed to Proposal Builder → 'White Paper' section")


# ---------- Phase I: CRM (Activities • Tasks • Pipeline) ----------
# Unified stage list used everywhere
STAGES_ORDERED = [
    "No contact made",
    "co contacted",
    "Quote",
    "Multiple quotes",
    "proposal started",
    "proposal finished",
    "submitted",
    "awarded",
    "proposal lost",
]

# Centralized stage probabilities for weighted pipeline
_STAGE_PROB = {
    "No contact made": 5,
    "co contacted": 15,
    "Quote": 40,
    "Multiple quotes": 50,
    "proposal started": 60,
    "proposal finished": 70,
    "submitted": 85,
    "awarded": 100,
    "proposal lost": 0,
}

def _stage_probability(stage: str) -> int:
    try:
        return int(_STAGE_PROB.get(stage or "", 5))
    except Exception:
        return 5

def _stage_next(stage: str) -> str:
    try:
        i = STAGES_ORDERED.index(stage)
        return STAGES_ORDERED[i+1] if i+1 < len(STAGES_ORDERED) else stage
    except Exception:
        return stage

def _stage_prev(stage: str) -> str:
    try:
        i = STAGES_ORDERED.index(stage)
        return STAGES_ORDERED[i-1] if i-1 >= 0 else stage
    except Exception:
        return stage


def _ensure_deal_owner_schema(conn: "sqlite3.Connection") -> None:
    try:
        with conn:
            conn.execute("ALTER TABLE deals ADD COLUMN owner TEXT")
    except Exception:
        pass
    try:
        with conn:
            conn.execute("CREATE INDEX IF NOT EXISTS idx_deals_owner ON deals(owner)")
    except Exception:
        pass


# === Data layer: Deals ========================================================
def data_get_all_deals_for_detail(conn):
    """Return a DataFrame of deals for detail view."""
    import pandas as pd
    try:
        owner_scope = st.session_state.get("deal_owner_ctx", get_current_user_name())
        if owner_scope and owner_scope != "All":
            df = pd.read_sql_query(
                "SELECT d.id, d.title, d.agency, "
                "COALESCE(d.status, '') AS status, "
                "COALESCE(d.value, 0) AS value, "
                "COALESCE(d.rfp_deadline, '') AS rfp_deadline, "
                "COALESCE(d.owner, '') AS owner, "
                "COALESCE(d.engagement_score, 0) AS engagement_score, "
                "COALESCE(d.last_engagement_at, '') AS last_engagement_at, "
                "COALESCE(d.co_contact_id, 0) AS co_contact_id, "
                "COALESCE(c.name, '') AS co_name, "
                "COALESCE(c.email, '') AS co_email "
                "FROM deals_t d "
                "LEFT JOIN contacts_t c ON d.co_contact_id = c.id "
                "WHERE d.owner_user=? ORDER BY d.id DESC;",
                conn,
                params=(owner_scope,),
            )
        else:
            df = pd.read_sql_query(
                "SELECT d.id, d.title, d.agency, "
                "COALESCE(d.status, '') AS status, "
                "COALESCE(d.value, 0) AS value, "
                "COALESCE(d.rfp_deadline, '') AS rfp_deadline, "
                "COALESCE(d.owner, '') AS owner, "
                "COALESCE(d.engagement_score, 0) AS engagement_score, "
                "COALESCE(d.last_engagement_at, '') AS last_engagement_at, "
                "COALESCE(d.co_contact_id, 0) AS co_contact_id, "
                "COALESCE(c.name, '') AS co_name, "
                "COALESCE(c.email, '') AS co_email "
                "FROM deals_t d "
                "LEFT JOIN contacts_t c ON d.co_contact_id = c.id "
                "ORDER BY d.id DESC;",
                conn,
                params=(),
            )
        return df
    except Exception as e:
        logger.exception("data_get_all_deals_for_detail failed")
        return None

# === Service layer: Deals =====================================================
def svc_get_deals_for_detail(conn):
    """Business-friendly wrapper for deal detail view."""
    return data_get_all_deals_for_detail(conn)

# === End Deals data/service helpers ==========================================

def run_crm(conn: "sqlite3.Connection") -> None:
    _ensure_deal_owner_schema(conn)
    st.header("CRM")
    st.caption("Use this page to manage activities, tasks, and your deals pipeline so you always know what is moving and when revenue will land.")

    current_user = get_current_user_name()
    scope = st.radio(
        "Scope",
        ["My items", "All users"],
        index=0,
        horizontal=True,
        help="My items shows only your deals, activities, and tasks. All users shows everything in this workspace.",
    )
    global deal_owner_ctx
    if scope == "My items":
        deal_owner_ctx = current_user
    else:
        deal_owner_ctx = "All"
    st.session_state["deal_owner_ctx"] = deal_owner_ctx
    tabs = st.tabs(["Activities", "Tasks", "Pipeline"])

    # Ensure Deals schema and CRM wiring are in sync
    try:
        _migrate_deals_columns(conn)  # align columns/views like Deals
    except Exception:
        pass
    try:
        rid = int(st.session_state.get("current_rfp_id") or 0)
        if rid:
            _p3_auto_wire_crm_from_rfp(conn, rid)
    except Exception:
        pass

    # --- Activities
    with tabs[0]:
        st.subheader("Log Activity")
        df_deals = pd.read_sql_query("SELECT id, title FROM deals_t ORDER BY id DESC;", conn, params=())
        df_contacts = pd.read_sql_query("SELECT id, name FROM contacts_t ORDER BY name;", conn, params=())
        a_col1, a_col2, a_col3 = st.columns([2,2,2])
        with a_col1:
            a_type = st.selectbox("Type", ["Call","Email","Meeting","Note"], key="act_type")
            a_subject = st.text_input("Subject", value=st.session_state.get("outreach_subject",""))
        with a_col2:
            a_deal = st.selectbox("Related Deal (optional)", options=[None] + df_deals["id"].tolist(),
                                  format_func=lambda x: "None" if x is None else f"#{x} — {df_deals.loc[df_deals['id']==x,'title'].values[0]}",
                                  key="act_deal")
            a_contact = st.selectbox("Related Contact (optional)", options=[None] + df_contacts["id"].tolist(),
                                     format_func=lambda x: "None" if x is None else df_contacts.loc[df_contacts["id"]==x, "name"].values[0],
                                     key="act_contact")
        with a_col3:
            a_notes = st.text_area("Notes", height=100, key="act_notes")
            if st.button("Save Activity", key="act_save"):
                note_id = None
                with closing(conn.cursor()) as cur:
                    # Store activity under the current logical user; tenant_id is handled by triggers.
                    cur.execute(
                        "INSERT INTO activities(ts, type, subject, notes, deal_id, contact_id, owner_user) "
                        "VALUES (datetime('now'), ?, ?, ?, ?, ?, ?);",
                        (
                            a_type,
                            a_subject.strip(),
                            a_notes.strip(),
                            a_deal if a_deal else None,
                            a_contact if a_contact else None,
                            get_current_user_name(),
                        ),
                    )
                    note_id = cur.lastrowid
                    conn.commit()
                # Index this note into the full-text search index for global search.
                if note_id:
                    try:
                        if a_deal:
                            ref_type = "deal_note"
                        elif a_contact:
                            ref_type = "contact_note"
                        else:
                            ref_type = "note"
                        title = (a_subject or "").strip() or f"{ref_type.replace('_', ' ').title()} #{int(note_id)}"
                        body = (a_notes or "").strip()
                        if body:
                            _fts_upsert_doc(conn, ref_type, int(note_id), title, body)
                    except Exception:
                        # FTS is best-effort; do not block saving the activity.
                        pass
                st.success("Saved")

        st.subheader("Activity Log")
        f1, f2, f3 = st.columns([2,2,2])
        with f1:
            f_type = st.multiselect("Type filter", ["Call","Email","Meeting","Note"])
        with f2:
            f_deal = st.selectbox("Deal filter", options=[None] + df_deals["id"].tolist(),
                                  format_func=lambda x: "All" if x is None else f"#{x} — {df_deals.loc[df_deals['id']==x,'title'].values[0]}",
                                  key="act_f_deal")
        with f3:
            f_contact = st.selectbox("Contact filter", options=[None] + df_contacts["id"].tolist(),
                                     format_func=lambda x: "All" if x is None else df_contacts.loc[df_contacts["id"]==x, "name"].values[0],
                                     key="act_f_contact")
        q = "SELECT ts, type, subject, notes, deal_id, contact_id FROM activities_t WHERE 1=1"
        params = []
        # Scope to current owner when in "My items" mode
        owner_scope = st.session_state.get("deal_owner_ctx", get_current_user_name())
        if owner_scope and owner_scope != "All":
            q += " AND owner_user=?"
            params.append(owner_scope)
        if f_type:
            q += " AND type IN (%s)" % ",".join(["?"]*len(f_type))
            params.extend(f_type)
        if f_deal:
            q += " AND deal_id=?"; params.append(f_deal)
        if f_contact:
            q += " AND contact_id=?"; params.append(f_contact)
        q += " ORDER BY ts DESC"
        df_a = pd.read_sql_query(q, conn, params=params)
        if df_a.empty:
            st.write("No activities")
        else:
            _styled_dataframe(df_a, use_container_width=True, hide_index=True)
            if st.button("Export CSV", key="act_export"):
                path = str(Path(DATA_DIR) / "activities.csv")
                df_a.to_csv(path, index=False)
                st.markdown(f"[Download CSV]({path})")

    # --- Tasks
    with tabs[1]:
        
        st.subheader("New Task")
        df_deals = pd.read_sql_query("SELECT id, title FROM deals_t ORDER BY id DESC;", conn, params=())
        df_contacts = pd.read_sql_query("SELECT id, name FROM contacts_t ORDER BY name;", conn, params=())
        t1, t2, t3 = st.columns([2,2,2])
        with t1:
            t_title = st.text_input("Task title", key="task_title")
            t_due = st.date_input("Due date", key="task_due")
        with t2:
            t_priority = st.selectbox("Priority", ["Low","Normal","High"], index=1, key="task_priority")
        with t3:
            t_deal = st.selectbox(
                "Related deal (optional)",
                options=[None] + df_deals["id"].tolist(),
                format_func=lambda i: "None" if i is None else f"#{i} — " + str(df_deals.loc[df_deals['id'] == i, 'title'].values[0]),
                key="task_deal",
            )
            t_contact = st.selectbox(
                "Related contact (optional)",
                options=[None] + df_contacts["id"].tolist(),
                format_func=lambda i: "None" if i is None else str(df_contacts.loc[df_contacts['id'] == i, 'name'].values[0]),
                key="task_contact",
            )
            if st.button("Create task", key="task_create"):
                from contextlib import closing as _closing
                title_clean = (t_title or "").strip()
                if not title_clean:
                    st.error("Task title is required.")
                else:
                    owner_scope = st.session_state.get("deal_owner_ctx", get_current_user_name())
                    with _closing(conn.cursor()) as cur:
                        cur.execute(
                            "INSERT INTO tasks(owner_user, title, due_date, status, priority, deal_id, contact_id, created_at) "
                            "VALUES (?, ?, ?, ?, ?, ?, ?, datetime('now'));",
                            (
                                owner_scope,
                                title_clean,
                                str(t_due) if t_due else None,
                                "Open",
                                t_priority,
                                int(t_deal) if t_deal else None,
                                int(t_contact) if t_contact else None,
                            ),
                        )
                        conn.commit()
                    st.success("Task created.")
                    st.rerun()

        with f1:
            tf_status = st.multiselect("Status", ["Open","In Progress","Done"], default=["Open","In Progress"])
        with f2:
            tf_priority = st.multiselect("Priority", ["Low","Normal","High"], default=[])
        q = "SELECT id, title, due_date, status, priority, deal_id, contact_id FROM tasks_t WHERE 1=1"
        params = []
        owner_scope = st.session_state.get("deal_owner_ctx", get_current_user_name())
        if owner_scope and owner_scope != "All":
            q += " AND owner_user=?"
            params.append(owner_scope)
        if tf_status:
            q += " AND status IN (%s)" % ",".join(["?"]*len(tf_status)); params.extend(tf_status)
        if tf_priority:
            q += " AND priority IN (%s)" % ",".join(["?"]*len(tf_priority)); params.extend(tf_priority)
        q += " ORDER BY COALESCE(due_date,'9999-12-31') ASC"
        try:
            df_t = pd.read_sql_query(q, conn, params=params)
        except Exception:
            import pandas as _pd
            df_t = _pd.DataFrame(columns=["id","title","due_date","status","priority","deal_id","contact_id"])
        if df_t is None or df_t.empty:
            st.write("No tasks")
        else:
            for _, r in df_t.iterrows():
                c1, c2, c3, c4 = st.columns([3,2,2,2])
                with c1:
                    st.write(f"**{r['title']}**  — due {r['due_date'] or '—'}")
                with c2:
                    _opts = ["Open","In Progress","Done"]
                    _cur = str(r.get('status') or 'Open')
                    _idx = _opts.index(_cur) if _cur in _opts else 0
                    new_status = st.selectbox("Status", _opts, index=_idx, key=f"task_status_{int(r['id'])}")
                with c3:
                    _p = ["Low","Normal","High"]
                    _cp = str(r.get('priority') or 'Normal')
                    _pidx = _p.index(_cp) if _cp in _p else 1
                    new_pri = st.selectbox("Priority", _p, index=_pidx, key=f"task_pri_{int(r['id'])}")
                with c4:
                    if st.button("Apply", key=f"task_apply_{int(r['id'])}"):
                        from contextlib import closing as _closing
                        with _closing(conn.cursor()) as cur:
                            cur.execute(
                                "UPDATE tasks SET status=?, priority=?, completed_at=CASE WHEN ?='Done' THEN datetime('now') ELSE completed_at END WHERE id=?;",
                                (new_status, new_pri, new_status, int(r["id"])),
                            )
                            conn.commit()
                        st.success("Updated")
    # --- Pipeline
    with tabs[2]:
        ptab_pipeline, ptab_detail = st.tabs(["Pipeline view", "Deal detail"])
        with ptab_pipeline:
                # Add Deal (moved from Deals)
                try:
                    df_contacts_all = pd.read_sql_query(
                        "SELECT id, name, email FROM contacts_t ORDER BY name;",
                        conn,
                        params=(),
                    )
                except Exception:
                    import pandas as _pd
                    df_contacts_all = _pd.DataFrame(columns=["id", "name", "email"])

                with st.form("add_deal", clear_on_submit=True):
                    c1, c2, c3, c4, c5 = st.columns([3, 2, 2, 2, 2])
                    with c1:
                        title = st.text_input("Title")
                    with c2:
                        agency = st.text_input("Agency")
                    with c3:
                        status = st.selectbox("Status", STAGES_ORDERED)
                    with c4:
                        value = st.number_input("Est Value", min_value=0.0, step=1000.0, format="%.2f")
                    with c5:
                        owner_sel_opts = ["Quincy", "Collin", "Charles"]
                        _ctx_owner = st.session_state.get("deal_owner_ctx")
                        _def_owner = _ctx_owner if _ctx_owner in owner_sel_opts else "Quincy"
                        owner_val = st.selectbox("Owner", owner_sel_opts, index=owner_sel_opts.index(_def_owner))
                    d_due = st.date_input("Due date (optional)", key="deal_due_date")

                    co_contact_id = None
                    if df_contacts_all is not None and not df_contacts_all.empty:
                        co_opts = [None] + df_contacts_all["id"].astype(int).tolist()

                        def _fmt_contact_option(cid):
                            if cid is None:
                                return "None"
                            try:
                                row = df_contacts_all[df_contacts_all["id"] == cid].iloc[0]
                                nm = str(row.get("name") or "").strip()
                                em = str(row.get("email") or "").strip()
                                label = nm or em or f"ID {cid}"
                                if nm and em:
                                    label = f"{nm} — {em}"
                                return label
                            except Exception:
                                return str(cid)

                        co_contact_val = st.selectbox(
                            "Primary CO / contact (optional)",
                            options=co_opts,
                            format_func=_fmt_contact_option,
                            index=0,
                            key="deal_add_co_contact",
                        )
                        if co_contact_val is not None:
                            co_contact_id = int(co_contact_val)

                    submitted = st.form_submit_button("Add Deal")
                if submitted and title:
                    try:
                        from contextlib import closing as _closing
                        with _closing(conn.cursor()) as cur:
        
                                cur.execute(
                                    "INSERT INTO deals(title, agency, status, stage, value, owner, owner_user, co_contact_id, rfp_deadline, created_at) "
                                    "VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, datetime('now'));",
                                    (title.strip(), agency.strip(), STAGES_ORDERED[0], STAGES_ORDERED[0], float(value), owner_val, owner_val, co_contact_id, d_due)
                                )
                                cur.execute("INSERT INTO deal_stage_log(deal_id, stage, changed_at) VALUES(last_insert_rowid(), ?, datetime('now'));", (STAGES_ORDERED[0],))
                                conn.commit()
                    except Exception as e:
                        st.error(f"Error saving deal {e}")
            
                # Weighted Pipeline view
                st.subheader("Weighted Pipeline")
                owner_scope = st.session_state.get("deal_owner_ctx", get_current_user_name())
                if owner_scope and owner_scope != "All":
                    df = pd.read_sql_query(
                        "SELECT id, title, agency, COALESCE(status, stage, '') AS status, "
                        "COALESCE(value, 0) AS value, rfp_deadline "
                        "FROM deals_t WHERE owner_user = ? ORDER BY id DESC;",
                        conn,
                        params=(owner_scope,),
                    )
                else:
                    df = pd.read_sql_query(
                        "SELECT id, title, agency, COALESCE(status, stage, '') AS status, "
                        "COALESCE(value, 0) AS value, rfp_deadline "
                        "FROM deals_t ORDER BY id DESC;",
                        conn,
                        params=(),
                    )
                if df.empty:
                    st.info("No deals")
                else:
                    import datetime as _dt
                    today = _dt.date.today()
                    # Bring in stage date metadata from base deals table
                    try:
                        df_dates = pd.read_sql_query(
                            "SELECT id, created_at, first_entered_stage_date, last_stage_change_date FROM deals;",
                            conn,
                            params=(),
                        )
                    except Exception:
                        df_dates = None
                    if df_dates is not None and not df_dates.empty:
                        try:
                            m_created = {int(r["id"]): r.get("created_at") for _, r in df_dates.iterrows()}
                            m_stage_last = {int(r["id"]): r.get("last_stage_change_date") for _, r in df_dates.iterrows()}
                        except Exception:
                            m_created = {}
                            m_stage_last = {}
                        import pandas as _pd
                        df["created_at_raw"] = _pd.to_datetime(df["id"].map(m_created), errors="coerce")
                        df["stage_last_raw"] = _pd.to_datetime(df["id"].map(m_stage_last), errors="coerce")
                        stage_start = df["stage_last_raw"].where(df["stage_last_raw"].notnull(), df["created_at_raw"])
                        try:
                            df["days_in_stage"] = (today - stage_start.dt.date).dt.days
                        except Exception:
                            df["days_in_stage"] = _pd.Series([None] * len(df))
                    else:
                        df["days_in_stage"] = None
                    # Compute last activity per deal
                    try:
                        df_act = pd.read_sql_query(
                            "SELECT deal_id, MAX(ts) AS last_ts FROM activities_t GROUP BY deal_id;",
                            conn,
                            params=(),
                        )
                    except Exception:
                        df_act = None
                    if df_act is not None and not df_act.empty:
                        import pandas as _pd
                        try:
                            m_last_act = {int(r["deal_id"]): r.get("last_ts") for _, r in df_act.iterrows()}
                        except Exception:
                            m_last_act = {}
                        df["last_activity_at_raw"] = _pd.to_datetime(df["id"].map(m_last_act), errors="coerce")
                        try:
                            df["days_since_last_activity"] = (today - df["last_activity_at_raw"].dt.date).dt.days
                        except Exception:
                            df["days_since_last_activity"] = _pd.Series([None] * len(df))
                    else:
                        df["days_since_last_activity"] = None
                    # SLA-style color coding via emoji labels
                    import pandas as _pd
                    def _sla_label(days, green, yellow):
                        try:
                            if days is None or _pd.isna(days):
                                return "⚪ n/a"
                            d = int(days)
                        except Exception:
                            return "⚪ n/a"
                        if d <= green:
                            return f"🟢 {d}d"
                        elif d <= yellow:
                            return f"🟡 {d}d"
                        else:
                            return f"🔴 {d}d"
                    # Deadline-based SLA (days until deadline; negative means overdue)
                    def _deadline_sla(days):
                        try:
                            if days is None or _pd.isna(days):
                                return "⚪ n/a"
                            d = int(days)
                        except Exception:
                            return "⚪ n/a"
                        # >14 days out = green, 7-14 = yellow, <7 (including overdue) = red
                        if d > 14:
                            return f"🟢 {d}d"
                        elif d >= 7:
                            return f"🟡 {d}d"
                        else:
                            return f"🔴 {d}d"
                    # Compute days until deadline from rfp_deadline if present
                    try:
                        dl_raw = pd.to_datetime(df["rfp_deadline"], errors="coerce")
                        df["days_until_deadline"] = (dl_raw.dt.date - today).dt.days
                    except Exception:
                        df["days_until_deadline"] = None
                    try:
                        df["stage_sla"] = df["days_in_stage"].apply(lambda d: _sla_label(d, 3, 7))
                    except Exception:
                        df["stage_sla"] = "⚪ n/a"
                    try:
                        df["activity_sla"] = df["days_since_last_activity"].apply(lambda d: _sla_label(d, 3, 7))
                    except Exception:
                        df["activity_sla"] = "⚪ n/a"
                    try:
                        df["deadline_sla"] = df["days_until_deadline"].apply(_deadline_sla)
                    except Exception:
                        df["deadline_sla"] = "⚪ n/a"
                    df["prob_%"] = df["status"].apply(_stage_probability)
                    df["weighted_value"] = (df["value"].fillna(0).astype(float) * df["prob_%"] / 100.0).round(2)
                    _styled_dataframe(
                        df[["title","agency","status","value","prob_%","weighted_value","days_in_stage","days_since_last_activity","days_until_deadline","stage_sla","activity_sla","deadline_sla"]],
                        use_container_width=True,
                        hide_index=True,
                    )
                st.subheader("Kanban")
        
                # Bulk delete controls for deals in current view
                try:
                    _df_del = df_k.copy() if 'df_k' in locals() else df.copy()
                    _opts = {str(int(r['id'])): f"#{int(r['id'])} — {str(r.get('title') or '')[:60]}" for _, r in _df_del.iterrows()}
                    _sel = st.multiselect("Delete deals", options=list(_opts.keys()), format_func=lambda k: _opts.get(k, k), key="kanban_delete_sel")
                    if _sel and st.button("Delete selected", key="kanban_delete_btn"):
                        with conn:
                            for _id in _sel:
                                conn.execute("DELETE FROM deals WHERE id=?", (int(_id),))
                        st.success(f"Deleted {len(_sel)} deal(s)")
                        st.rerun()
                except Exception as _e:
                    pass
        
                try:
                    # Prefer base table. Include owner-null when filtering so old deals still show and can be assigned.
                    if deal_owner_ctx != "All":
                        # Filter by logical owner_user while respecting tenant scoping via deals_t
                        df_k = pd.read_sql_query(
                            "SELECT id, title, agency, COALESCE(status, stage, '') AS status, "
                            "COALESCE(value, 0) AS value, rfp_deadline, COALESCE(owner, '') AS owner "
                            "FROM deals_t WHERE owner_user = ? "
                            "ORDER BY id DESC;",
                            conn,
                            params=(deal_owner_ctx,),
                        )
                    else:
                        df_k = pd.read_sql_query(
                            "SELECT id, title, agency, COALESCE(status, stage, '') AS status, "
                            "COALESCE(value, 0) AS value, rfp_deadline, COALESCE(owner, '') AS owner "
                            "FROM deals_t ORDER BY id DESC;",
                            conn,
                            params=(),
                        )
                except Exception:
                    # Fallback to view without owner_user column
                    try:
                        df_k = pd.read_sql_query(
                            "SELECT id, title, agency, status, value, rfp_deadline, '' AS owner FROM deals_t ORDER BY id DESC;",
                            conn, params=()
                        )
                    except Exception:
                        df_k = None
                
                if df_k is None or df_k.empty:
                    st.caption("No deals to display")
                else:
                    cols = st.columns(len(STAGES_ORDERED))
                    for idx, stage in enumerate(STAGES_ORDERED):
                        with cols[idx]:
                            st.markdown(f"**{stage}**")
                            sdf = df_k[df_k["status"] == stage]
                            if sdf.empty:
                                st.caption("—")
                                continue
                            try:
                                sdf = sdf.copy()
                                if "rfp_deadline" in sdf.columns:
                                    sdf["_deadline_sort"] = sdf["rfp_deadline"].fillna("")
                                    sdf["_has_deadline"] = sdf["_deadline_sort"].astype(str).str.len() > 0
                                    sdf = sdf.sort_values(
                                        by=["_has_deadline", "_deadline_sort", "id"],
                                        ascending=[False, True, True],
                                    )
                            except Exception:
                                pass
                            for _, r in sdf.iterrows():
                                with st.container(border=True):
                                    did = int(r["id"])
                                    st.markdown(f"#{did} · **{r.get('title') or ''}**")
                                    st.caption(str(r.get("agency") or ""))
                                    import datetime as _dt
                                    # Editable due date for Kanban cards (calendar picker)
                                    _raw_deadline = r.get("rfp_deadline")
                                    _due_default = None
                                    try:
                                        if isinstance(_raw_deadline, (_dt.date, _dt.datetime)):
                                            _due_default = _raw_deadline.date() if isinstance(_raw_deadline, _dt.datetime) else _raw_deadline
                                        else:
                                            _s = str(_raw_deadline or "").strip()
                                            if _s and _s not in ("NaT", "None"):
                                                _due_default = _dt.datetime.strptime(_s, "%Y-%m-%d").date()
                                    except Exception:
                                        _due_default = None
                                    if _due_default is None:
                                        _due_default = _dt.datetime.now().date()
                                    due_input = st.date_input(
                                        "Due date",
                                        value=_due_default,
                                        key=f"k_due_{did}",
                                    )
                                    # Editable value
                                    v = st.number_input(
                                        "Value",
                                        value=float(r.get("value") or 0.0),
                                        step=1000.0,
                                        format="%.2f",
                                        key=f"k_val_{did}",
                                    )
                                    # Editable owner
                                    owner_opts = ["Quincy", "Collin", "Charles"]
                                    owner_cur = str(r.get("owner") or "").strip()
                                    if not owner_cur or owner_cur not in owner_opts:
                                        owner_cur = deal_owner_ctx if deal_owner_ctx in owner_opts else owner_opts[0]
                                    owner_new = st.selectbox(
                                        "Owner",
                                        owner_opts,
                                        index=owner_opts.index(owner_cur) if owner_cur in owner_opts else 0,
                                        key=f"k_owner_{did}",
                                    )
                                    # Editable stage
                                    ns = st.selectbox(
                                        "Stage",
                                        STAGES_ORDERED,
                                        index=STAGES_ORDERED.index(stage),
                                        key=f"k_stage_{did}",
                                    )

                                    def _parse_due_input(_val):
                                        if isinstance(_val, _dt.datetime):
                                            return _val.date()
                                        if isinstance(_val, _dt.date):
                                            return _val
                                        return None

                                    c1, c2, c3 = st.columns([1, 1, 1])
                                    with c1:
                                        if st.button("◀", key=f"k_prev_{did}"):
                                            ns2 = _stage_prev(stage)
                                            from contextlib import closing as _closing
                                            with _closing(conn.cursor()) as cur:
                                                _due = _parse_due_input(due_input)
                                                cur.execute(
                                                    "UPDATE deals SET status=?, stage=?, rfp_deadline=?, value=?, owner=?, updated_at=datetime('now') WHERE id=?",
                                                    (ns2, ns2, _due, float(v or 0.0), owner_new, did),
                                                )
                                                if ns2 != stage:
                                                    cur.execute(
                                                        "INSERT INTO deal_stage_log(deal_id, stage, changed_at) VALUES(?, ?, datetime('now'));",
                                                        (did, ns2),
                                                    )
                                                conn.commit()
                                            st.rerun()
                                    with c2:
                                        if st.button("Save", key=f"k_save_{did}"):
                                            from contextlib import closing as _closing
                                            with _closing(conn.cursor()) as cur:
                                                _due = _parse_due_input(due_input)
                                                cur.execute(
                                                    "UPDATE deals SET value=?, status=?, stage=?, rfp_deadline=?, owner=?, updated_at=datetime('now') WHERE id=?",
                                                    (float(v or 0.0), ns, ns, _due, owner_new, did),
                                                )
                                                if ns != stage:
                                                    cur.execute(
                                                        "INSERT INTO deal_stage_log(deal_id, stage, changed_at) VALUES(?, ?, datetime('now'));",
                                                        (did, ns),
                                                    )
                                                conn.commit()
                                            st.rerun()
                                    with c3:
                                        if st.button("▶", key=f"k_next_{did}"):
                                            ns2 = _stage_next(stage)
                                            from contextlib import closing as _closing
                                            with _closing(conn.cursor()) as cur:
                                                _due = _parse_due_input(due_input)
                                                cur.execute(
                                                    "UPDATE deals SET status=?, stage=?, rfp_deadline=?, value=?, owner=?, updated_at=datetime('now') WHERE id=?",
                                                    (ns2, ns2, _due, float(v or 0.0), owner_new, did),
                                                )
                                                if ns2 != stage:
                                                    cur.execute(
                                                        "INSERT INTO deal_stage_log(deal_id, stage, changed_at) VALUES(?, ?, datetime('now'));",
                                                        (did, ns2),
                                                    )
                                                conn.commit()
                                            st.rerun()
                    st.subheader("Summary by Stage")
                    summary = df.groupby("status").agg(
                        deals=("id","count"),
                        value=("value","sum"),
                        expected=("weighted_value","sum")
                    ).reset_index().sort_values("expected", ascending=False)
                    _styled_dataframe(summary, use_container_width=True, hide_index=True)
                    if st.button("Export Pipeline CSV", key="pipe_export"):
                        # Track pipeline metrics export / rebuild as a job.
                        try:
                            ensure_jobs_schema(conn)
                        except Exception:
                            pass

                        try:
                            try:
                                _user_name = get_current_user_name()
                            except Exception:
                                _user_name = ""
                            payload = {
                                "scope": "pipeline_metrics_rebuild",
                                "owner_scope": owner_scope,
                            }
                            job_id = jobs_enqueue(
                                conn,
                                job_type="pipeline_metrics_rebuild",
                                payload=payload,
                                created_by=_user_name or None,
                            )
                        except Exception:
                            job_id = None

                        path = str(Path(DATA_DIR) / "pipeline.csv")
                        try:
                            if job_id:
                                jobs_update_status(
                                    conn,
                                    job_id,
                                    status="running",
                                    progress=0.5,
                                    mark_started=True,
                                )
                        except Exception:
                            pass

                        df.to_csv(path, index=False)
                        st.markdown(f"[Download CSV]({path})")

                        try:
                            if job_id:
                                jobs_update_status(
                                    conn,
                                    job_id,
                                    status="done",
                                    progress=1.0,
                                    mark_finished=True,
                                    result={"path": path, "rows": int(len(df) if df is not None else 0)},
                                )
                        except Exception:
                            pass
            
                # Editable pipeline table with stage logging
                st.subheader("Edit Pipeline")
                try:
                    df_edit = pd.read_sql_query(
                        "SELECT id, title, agency, status, value, rfp_deadline, sam_url FROM deals_t ORDER BY id DESC;",
                        conn,
                        params=(),
                    )
                    if df_edit.empty:
                        st.write("No deals yet")
                    else:
                        # Normalize rfp_deadline to real dates so DateColumn works
                        if "rfp_deadline" in df_edit.columns:
                            df_edit["rfp_deadline"] = pd.to_datetime(df_edit["rfp_deadline"], errors="coerce").dt.date
                        df_edit["prob_%"] = df_edit["status"].apply(_stage_probability)
                        df_edit["weighted_value"] = (df_edit["value"].fillna(0).astype(float) * df_edit["prob_%"] / 100.0).round(2)
                        edited = st.data_editor(
                            df_edit,
                            use_container_width=True,
                            hide_index=True,
                            column_config={"status": st.column_config.SelectboxColumn(options=STAGES_ORDERED), "owner": st.column_config.SelectboxColumn(options=["Quincy","Collin","Charles"]),
                                "rfp_deadline": st.column_config.DateColumn(format="YYYY-MM-DD"), "prob_%": st.column_config.NumberColumn(disabled=True),
                                "weighted_value": st.column_config.NumberColumn(disabled=True, help="value × stage probability"),
                            },
                            key="crm_deals_editor",
                        )
                        if st.button("Save Edits", key="crm_deals_save"):
                            try:
                                df_orig = df_edit.set_index("id")
                                from contextlib import closing as _closing
                                import sqlite3 as _sqlite3  # for IntegrityError type
                                with _closing(conn.cursor()) as cur:
                                    # Relax foreign key checks during pipeline updates so legacy
                                    # rows do not block simple field edits from the UI.
                                    try:
                                        cur.execute("PRAGMA foreign_keys = OFF;")
                                    except Exception:
                                        pass
                                    for _, r in edited.iterrows():
                                        rid = int(r["id"])
                                        old_status = str(df_orig.loc[rid, "status"]) if rid in df_orig.index else ""
                                        new_status = str(r.get("status") or "New")
                                        # Normalize deadline to ISO string for storage
                                        r_deadline = r.get("rfp_deadline")
                                        # Accept pandas Timestamps, datetime.date, or anything with strftime
                                        if hasattr(r_deadline, "strftime"):
                                            try:
                                                r_deadline_str = r_deadline.strftime("%Y-%m-%d")
                                            except Exception:
                                                r_deadline_str = None
                                        elif isinstance(r_deadline, str) and r_deadline.strip():
                                            try:
                                                _tmp_dt = pd.to_datetime(r_deadline, errors="coerce")
                                                r_deadline_str = _tmp_dt.date().strftime("%Y-%m-%d") if _tmp_dt is not pd.NaT else None
                                            except Exception:
                                                r_deadline_str = None
                                        else:
                                            r_deadline_str = None
                                        cur.execute(
                                            """
                                            UPDATE deals
                                            SET title = ?,
                                                agency = ?,
                                                status = ?,
                                                stage = ?,
                                                value = ?,
                                                rfp_deadline = ?,
                                                sam_url = ?,
                                                updated_at = datetime('now')
                                            WHERE id = ?;
                                            """,
                                            (
                                                str(r.get("title", "") or "").strip(),
                                                str(r.get("agency", "") or "").strip(),
                                                new_status,
                                                new_status,
                                                float(r.get("value") or 0.0),
                                                r_deadline_str,
                                                str(r.get("sam_url", "") or "").strip(),
                                                rid,
                                            ),
                                        )
                                        if new_status != old_status:
                                            try:
                                                cur.execute(
                                                    """
                                                    INSERT INTO deal_stage_log(deal_id, stage, changed_at)
                                                    VALUES(?, ?, datetime('now'));
                                                    """,
                                                    (rid, new_status),
                                                )
                                                # Track last stage change timestamp on the deal
                                                try:
                                                    cur.execute(
                                                        "UPDATE deals SET last_stage_change_date=datetime('now') WHERE id=?;",
                                                        (rid,),
                                                    )
                                                except Exception:
                                                    pass
                                            except Exception:
                                                # Ignore logging failures so edits still succeed
                                                pass
                                    try:
                                        cur.execute("PRAGMA foreign_keys = ON;")
                                    except Exception:
                                        pass
                                    conn.commit()
                                st.success("Pipeline updated")
                                try:
                                    st.rerun()
                                except Exception:
                                    pass
                            except Exception as e:
                                st.error(f"Save failed: {e}")
                except Exception as e:
                    st.error(f"Pipeline load failed: {e}")

        with ptab_detail:
            # Simple deal detail view to avoid overwhelming the main pipeline
            try:
                df_all = svc_get_deals_for_detail(conn)
            except Exception as e:
                logger.exception("Deal detail lookup failed")
                ui_error("Could not load detailed deal information.", str(e))
                df_all = None
            if df_all is None or df_all.empty:
                ui_info("No deals available to inspect.")
            else:
                sel_id = st.selectbox(
                    "Select a deal",
                    options=df_all["id"].tolist(),
                    format_func=lambda i: f"#{i} — " + str(df_all.loc[df_all["id"] == i, "title"].values[0]),
                    key="deal_detail_select",
                )
                row = df_all[df_all["id"] == sel_id].iloc[0]
                co_label = (row.get("co_name") or "") or (row.get("co_email") or "")
                if not co_label:
                    co_label = "None set"
                # Compute SLA-style age metrics for this deal
                try:
                    import datetime as _dt
                    import pandas as _pd
                    today = _dt.date.today()
                    df_dates = pd.read_sql_query(
                        "SELECT created_at, first_entered_stage_date, last_stage_change_date FROM deals WHERE id=?;",
                        conn,
                        params=(int(sel_id),),
                    )
                    if df_dates is not None and not df_dates.empty:
                        r0 = df_dates.iloc[0]
                        created_at = _pd.to_datetime(r0.get("created_at"), errors="coerce")
                        last_stage_change = _pd.to_datetime(r0.get("last_stage_change_date"), errors="coerce")
                        stage_start = last_stage_change if last_stage_change is not None and not _pd.isna(last_stage_change) else created_at
                        if stage_start is not None and not _pd.isna(stage_start):
                            days_in_stage = (today - stage_start.date()).days
                        else:
                            days_in_stage = None
                    else:
                        days_in_stage = None
                    df_act = pd.read_sql_query(
                        "SELECT MAX(ts) AS last_ts FROM activities_t WHERE deal_id=?;",
                        conn,
                        params=(int(sel_id),),
                    )
                    if df_act is not None and not df_act.empty and df_act.iloc[0].get("last_ts"):
                        last_ts = _pd.to_datetime(df_act.iloc[0].get("last_ts"), errors="coerce")
                        if last_ts is not None and not _pd.isna(last_ts):
                            days_since_last = (today - last_ts.date()).days
                        else:
                            days_since_last = None
                    else:
                        days_since_last = None
                    def _sla_label_single(days, green, yellow):
                        try:
                            if days is None or _pd.isna(days):
                                return "⚪ n/a"
                            d = int(days)
                        except Exception:
                            return "⚪ n/a"
                        if d <= green:
                            return f"🟢 {d}d"
                        elif d <= yellow:
                            return f"🟡 {d}d"
                        else:
                            return f"🔴 {d}d"
                    stage_sla_label = _sla_label_single(days_in_stage, 3, 7)
                    activity_sla_label = _sla_label_single(days_since_last, 3, 7)
                except Exception:
                    stage_sla_label = "⚪ n/a"
                    activity_sla_label = "⚪ n/a"
                # Compute deadline SLA label for this deal
                try:
                    import datetime as _dt2
                    import pandas as _pd2
                    today2 = _dt2.date.today()
                    dl = _pd2.to_datetime(row.get("rfp_deadline"), errors="coerce")
                    if dl is not None and not _pd2.isna(dl):
                        days_until_deadline = (dl.date() - today2).days
                    else:
                        days_until_deadline = None
                    def _deadline_sla_single(days):
                        try:
                            if days is None or _pd2.isna(days):
                                return "⚪ n/a"
                            d = int(days)
                        except Exception:
                            return "⚪ n/a"
                        if d > 14:
                            return f"🟢 {d}d"
                        elif d >= 7:
                            return f"🟡 {d}d"
                        else:
                            return f"🔴 {d}d"
                    deadline_sla_label = _deadline_sla_single(days_until_deadline)
                except Exception:
                    deadline_sla_label = "⚪ n/a"
                body = (
                    f"Agency: {row['agency']}\n\n"
                    f"Status: {row['status']}\n"
                    f"Deadline: {deadline_sla_label}\n"
                    f"Stage age: {stage_sla_label}\n"
                    f"Last activity: {activity_sla_label}\n\n"
                    f"Estimated value: ${float(row['value'] or 0):,.2f}\n\n"
                    f"Engagement score: {float(row.get('engagement_score') or 0):.1f}\n"
                    f"Last engagement: {row.get('last_engagement_at') or '—'}\n\n"
                    f"Primary CO / contact: {co_label}"
                )
                footer = f"Owner: {row.get('owner') or 'Unassigned'} | Due: {row.get('rfp_deadline') or '—'}"
                render_card(f"Deal #{int(row['id'])} — {row['title']}", body=body, footer=footer)
                # RFP Analyzer deep link from deal detail
                _linked_rfp_id = None
                try:
                    from contextlib import closing as _closing
                    with _closing(conn.cursor()) as _cur:
                        _cur.execute("SELECT rfp_id FROM deals WHERE id=?;", (int(sel_id),))
                        _row_rfp = _cur.fetchone()
                    if _row_rfp and _row_rfp[0]:
                        _linked_rfp_id = int(_row_rfp[0])
                except Exception:
                    _linked_rfp_id = None
                if _linked_rfp_id:
                    if st.button("Open in RFP Analyzer", key=f"deal_open_rfp_{int(sel_id)}"):
                        st.session_state["current_rfp_id"] = int(_linked_rfp_id)
                        st.session_state["nav_target"] = "RFP Analyzer"
                        try:
                            router("RFP Analyzer", conn); st.stop()
                        except Exception:
                            try:
                                st.rerun()
                            except Exception:
                                pass
                # Suggested subs / matching vendors for this deal
                try:
                    from contextlib import closing as _closing
                    required_naics = ""
                    work_type_tags = ""
                    notice_id = ""
                    try:
                        with _closing(conn.cursor()) as _cur:
                            _cur.execute(
                                "SELECT notice_id, COALESCE(naics, ''), COALESCE(work_type_tags, '') "
                                "FROM deals WHERE id=?;",
                                (int(sel_id),),
                            )
                            _r = _cur.fetchone()
                        if _r:
                            notice_id = str(_r[0] or "")
                            required_naics = str(_r[1] or "")
                            work_type_tags = str(_r[2] or "")
                        if (not required_naics) and notice_id:
                            with _closing(conn.cursor()) as _cur2:
                                _cur2.execute("SELECT naics FROM notices WHERE notice_id=?;", (notice_id,))
                                _n = _cur2.fetchone()
                            if _n and _n[0]:
                                required_naics = str(_n[0] or "")
                    except Exception:
                        pass

                    with st.expander("Deal tags & suggested subs", expanded=False):
                        c_tag1, c_tag2 = st.columns(2)
                        with c_tag1:
                            st.write(f"Required NAICS: {required_naics or 'n/a'}")
                        with c_tag2:
                            tags_input = st.text_input(
                                "Work type tags (comma-separated)",
                                value=work_type_tags or "",
                                key=f"deal_detail_worktypes_{int(sel_id)}",
                                help="Examples: janitorial, grounds, HVAC, snow removal",
                            )
                            if st.button("Save tags", key=f"deal_detail_save_worktypes_{int(sel_id)}"):
                                try:
                                    with _closing(conn.cursor()) as _cur3:
                                        _cur3.execute(
                                            "UPDATE deals SET work_type_tags=? WHERE id=?;",
                                            (tags_input.strip(), int(sel_id)),
                                        )
                                        conn.commit()
                                    st.success("Updated work type tags for this deal.")
                                    work_type_tags = tags_input
                                except Exception as _e:
                                    st.error(f"Failed to update tags: {_e}")

                        df_suggest = _get_suggested_subs(conn, required_naics, work_type_tags)
                        if (not required_naics) and (not (work_type_tags or '').strip()):
                            st.caption("Add NAICS or work type tags for this deal to see suggested subcontractors.")
                        elif df_suggest is None or df_suggest.empty:
                            st.caption("No matching vendors yet. Seed vendors with matching capabilities in Subcontractor Finder.")
                        else:
                            import pandas as _pd
                            df_view = df_suggest.copy()
                            try:
                                df_view["location"] = (
                                    df_view["city"].fillna("").str.strip()
                                    + _pd.Series([", "] * len(df_view))
                                    + df_view["state"].fillna("").str.strip()
                                )
                            except Exception:
                                df_view["location"] = df_view.get("city", "") + ", " + df_view.get("state", "")
                            df_view = df_view.rename(
                                columns={"vendor_score": "score", "naics_match": "NAICS match", "tag_match_count": "tag matches"}
                            )
                            cols_show = [c for c in ["name", "location", "score", "NAICS match", "tag matches", "capability_tags"] if c in df_view.columns]
                            if cols_show:
                                st.dataframe(df_view[cols_show], use_container_width=True)
                except Exception as _e:
                    logger.exception("Suggested subs panel failed for deal detail")


                # Optional: allow changing primary CO / contact from detail view
                try:
                    df_contacts = pd.read_sql_query(
                        "SELECT id, name, email FROM contacts_t ORDER BY name;",
                        conn,
                        params=(),
                    )
                except Exception:
                    import pandas as _pd
                    df_contacts = _pd.DataFrame(columns=["id", "name", "email"])
                if df_contacts is not None and not df_contacts.empty:
                    ids = df_contacts["id"].astype(int).tolist()
                    current_co = int(row.get("co_contact_id") or 0)
                    options = [None] + ids

                    def _fmt_contact_option(cid):
                        if cid is None:
                            return "None"
                        try:
                            r2 = df_contacts[df_contacts["id"] == cid].iloc[0]
                            nm = str(r2.get("name") or "").strip()
                            em = str(r2.get("email") or "").strip()
                            label = nm or em or f"ID {cid}"
                            if nm and em:
                                label = f"{nm} — {em}"
                            return label
                        except Exception:
                            return str(cid)

                    idx = 0
                    if current_co and current_co in ids:
                        idx = 1 + ids.index(current_co)

                    sel_co = st.selectbox(
                        "Update primary CO / contact",
                        options=options,
                        format_func=_fmt_contact_option,
                        index=idx,
                        key=f"deal_detail_co_{int(row['id'])}",
                    )
                    if st.button("Save primary contact", key=f"deal_detail_co_save_{int(row['id'])}"):
                        new_id = None if sel_co is None else int(sel_co)
                        from contextlib import closing as _closing
                        try:
                            with _closing(conn.cursor()) as cur:
                                cur.execute(
                                    "UPDATE deals SET co_contact_id=? WHERE id=?;",
                                    (new_id, int(row["id"])),
                                )
                                conn.commit()
                            st.success("Updated deal primary contact.")
                            st.rerun()
                        except Exception as e:
                            st.error(f"Failed to update primary contact: {e}")
                # Quick follow-up tasks for this deal
                from datetime import date, timedelta
                from contextlib import closing as _closing

                def _add_followup_task(days: int) -> None:
                    due = date.today() + timedelta(days=days)
                    owner_scope = st.session_state.get("deal_owner_ctx", get_current_user_name())
                    title = f"Follow up: {row['title']}"
                    primary_contact_id = int(row.get("co_contact_id") or 0) or None
                    with _closing(conn.cursor()) as cur:
                        cur.execute(
                            "INSERT INTO tasks(owner_user, title, due_date, status, priority, deal_id, contact_id, created_at) "
                            "VALUES (?, ?, ?, ?, ?, ?, ?, datetime('now'));",
                            (
                                owner_scope,
                                title[:200],
                                due.isoformat(),
                                "Open",
                                "Normal",
                                int(row["id"]),
                                primary_contact_id,
                            ),
                        )
                        conn.commit()
                    st.success(f"Added follow-up task due {due.isoformat()}")

                st.subheader("Follow-up tasks")
                fc1, fc2, fc3, fc4 = st.columns(4)
                with fc1:
                    if st.button("Follow-up in 3 days", key=f"deal_follow_3_{int(row['id'])}"):
                        _add_followup_task(3)
                with fc2:
                    if st.button("Follow-up in 7 days", key=f"deal_follow_7_{int(row['id'])}"):
                        _add_followup_task(7)
                with fc3:
                    if st.button("Follow-up in 14 days", key=f"deal_follow_14_{int(row['id'])}"):
                        _add_followup_task(14)
                with fc4:
                    if st.button("Follow-up in 30 days", key=f"deal_follow_30_{int(row['id'])}"):
                        _add_followup_task(30)

                # Activity timeline for this deal
                try:
                    owner_scope = st.session_state.get("deal_owner_ctx", get_current_user_name())
                    sql = "SELECT ts, type, subject, notes, contact_id FROM activities_t WHERE deal_id = ?"
                    params = [int(row["id"])]
                    if owner_scope and owner_scope != "All":
                        sql += " AND owner_user = ?"
                        params.append(owner_scope)
                    sql += " ORDER BY ts DESC LIMIT 200"
                    df_act = pd.read_sql_query(sql, conn, params=params)
                except Exception as e:
                    st.error(f"Failed to load activities for this deal: {e}")
                    df_act = None

                st.subheader("Activity timeline")
                if df_act is None or df_act.empty:
                    st.caption("No activities logged for this deal yet.")
                else:
                    # Attach contact names when possible
                    try:
                        df_contacts_tl = pd.read_sql_query(
                            "SELECT id, name FROM contacts_t;",
                            conn,
                            params=(),
                        )
                        name_map = {int(r["id"]): str(r.get("name") or "") for _, r in df_contacts_tl.iterrows()}
                        df_act["contact_name"] = df_act["contact_id"].map(name_map).fillna("")
                    except Exception:
                        df_act["contact_name"] = ""

                    df_act = df_act.rename(
                        columns={
                            "ts": "When",
                            "type": "Type",
                            "subject": "Subject",
                            "notes": "Notes",
                            "contact_name": "Contact",
                        }
                    )
                    cols_order = [c for c in ["When", "Type", "Subject", "Notes", "Contact"] if c in df_act.columns]
                    _styled_dataframe(df_act[cols_order], use_container_width=True, hide_index=True)


def _ensure_files_table(conn: "sqlite3.Connection") -> None:
    try:
        with closing(conn.cursor()) as cur:
            cur.execute("""
                CREATE TABLE IF NOT EXISTS files(
                    id INTEGER PRIMARY KEY,
                    owner_type TEXT,
                    owner_id INTEGER,
                    filename TEXT,
                    path TEXT,
                    size INTEGER,
                    mime TEXT,
                    tags TEXT,
                    notes TEXT,
                    uploaded_at TEXT
                );
            """)
            cur.execute("CREATE INDEX IF NOT EXISTS idx_files_owner ON files(owner_type, owner_id);")
            cur.execute("CREATE INDEX IF NOT EXISTS idx_files_tags ON files(tags);")
            conn.commit()
    except Exception:
        pass

# ---------- Phase J: File Manager & Submission Kit ----------
def _detect_mime(name: str) -> str:
    name = (name or "").lower()
    if name.endswith(".pdf"): return "application/pdf"
    if name.endswith(".docx"): return "application/vnd.openxmlformats-officedocument.wordprocessingml.document"
    if name.endswith(".xlsx"): return "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
    if name.endswith(".zip"): return "application/zip"
    if name.endswith(".png"): return "image/png"
    if name.endswith(".jpg") or name.endswith(".jpeg"): return "image/jpeg"
    if name.endswith(".txt"): return "text/plain"
    return "application/octet-stream"

def run_file_manager(conn: "sqlite3.Connection") -> None:
    _ensure_files_table(conn)
    st.header("File Manager")
    st.caption("Attach files to RFPs / Deals / Vendors, tag them, and build a zipped submission kit.")

    # --- Attach uploader ---
    with st.expander("Upload & Attach", expanded=True):
        c1, c2 = st.columns([2,2])
        with c1:
            owner_type = st.selectbox("Attach to", ["RFP", "Deal", "Vendor", "Other"], key="fm_owner_type")
            owner_id = None
            if owner_type == "RFP":
                df_rf = pd.read_sql_query("SELECT id, title FROM rfps_t ORDER BY id DESC;", conn, params=())
                if not df_rf.empty:
                    owner_id = st.selectbox("RFP", options=df_rf["id"].tolist(),
                                            format_func=lambda i: f"#{i} — {df_rf.loc[df_rf['id']==i, 'title'].values[0]}",
                                            key="fm_owner_rfp")
            elif owner_type == "Deal":
                df_deal = pd.read_sql_query("SELECT id, title FROM deals_t ORDER BY id DESC;", conn, params=())
                if not df_deal.empty:
                    owner_id = st.selectbox("Deal", options=df_deal["id"].tolist(),
                                            format_func=lambda i: f"#{i} — {df_deal.loc[df_deal['id']==i, 'title'].values[0]}",
                                            key="fm_owner_deal")
            elif owner_type == "Vendor":
                df_v = pd.read_sql_query("SELECT id, name FROM vendors_t ORDER BY name;", conn, params=())
                if not df_v.empty:
                    owner_id = st.selectbox("Vendor", options=df_v["id"].tolist(),
                                            format_func=lambda i: f"#{i} — {df_v.loc[df_v['id']==i, 'name'].values[0]}",
                                            key="fm_owner_vendor")
            # Owner_id can be None for "Other"
        with c2:
            tags = st.text_input("Tags (comma-separated)", key="fm_tags")
            notes = st.text_area("Notes (optional)", height=70, key="fm_notes")

        ups = st.file_uploader("Select files", type=None, accept_multiple_files=True, key="fm_files")
        if st.button("Upload", key="fm_upload"):
            if not ups:
                st.warning("Pick at least one file")
            else:
                saved = 0
                for f in ups:
                    pth = save_uploaded_file(f, subdir="attachments")
                    if not pth:
                        continue
                    try:
                        with closing(conn.cursor()) as cur:
                            cur.execute("""
                                INSERT INTO files(owner_type, owner_id, filename, path, size, mime, tags, notes, uploaded_at)
                                VALUES (?, ?, ?, ?, ?, ?, ?, ?, datetime('now'));
                            """, (
                                owner_type, int(owner_id) if owner_id else None, f.name, pth, f.size, _detect_mime(f.name),
                                tags.strip(), notes.strip()
                            ))
                            conn.commit()
                            saved += 1
                    except Exception as e:
                        st.error(f"DB save failed: {e}")
                st.success(f"Uploaded {saved} file(s).")

    # --- Library & filters ---
    with st.expander("Library", expanded=True):
        l1, l2, l3 = st.columns([2,2,2])
        with l1:
            f_owner = st.selectbox("Filter by type", ["All", "RFP", "Deal", "Vendor", "Other"], key="fm_f_owner")
        with l2:
            f_tag = st.text_input("Tag contains", key="fm_f_tag")
        with l3:
            f_kw = st.text_input("Filename contains", key="fm_f_kw")

        q = "SELECT id, owner_type, owner_id, filename, path, size, mime, tags, notes, uploaded_at FROM files_t WHERE 1=1"
        params = []
        if f_owner and f_owner != "All":
            q += " AND owner_type=?"; params.append(f_owner)
        if f_tag:
            q += " AND tags LIKE ?"; params.append(f"%{f_tag}%")
        if f_kw:
            q += " AND filename LIKE ?"; params.append(f"%{f_kw}%")
        q += " ORDER BY uploaded_at DESC"
        try:
            df_files = pd.read_sql_query(q, conn, params=params)
        except Exception as e:
            _ensure_files_table(conn)
            try:
                df_files = pd.read_sql_query(q, conn, params=params)
            except Exception as e2:
                st.error(f"Failed to load files: {e2}")
                df_files = pd.DataFrame()
        if df_files.empty:
            st.write("No files yet.")
        else:
            _styled_dataframe(df_files.drop(columns=["path"]), use_container_width=True, hide_index=True)
            # Per-row controls
            for _, r in df_files.iterrows():
                c1, c2, c3, c4 = st.columns([3,2,2,2])
                with c1:
                    st.caption(f"#{int(r['id'])} — {r['filename']} ({r['owner_type']} {int(r['owner_id']) if r['owner_id'] else ''})")
                with c2:
                    new_tags = st.text_input("Tags", value=r.get("tags") or "", key=f"fm_row_tags_{int(r['id'])}")
                with c3:
                    new_notes = st.text_input("Notes", value=r.get("notes") or "", key=f"fm_row_notes_{int(r['id'])}")
                with c4:
                    b1, b2 = st.columns(2)
                    with b1:
                        if st.button("Save", key=f"fm_row_save_{int(r['id'])}"):
                            with closing(conn.cursor()) as cur:
                                cur.execute("UPDATE files SET tags=?, notes=? WHERE id=?;", (new_tags.strip(), new_notes.strip(), int(r["id"])))
                                conn.commit()
                            st.success("Updated")
                    with b2:
                        if st.button("Delete", key=f"fm_row_del_{int(r['id'])}"):
                            with closing(conn.cursor()) as cur:
                                cur.execute("DELETE FROM files WHERE id=?;", (int(r["id"]),))
                                conn.commit()
                            try:
                                if r.get("path") and os.path.exists(r["path"]):
                                    os.remove(r["path"])
                            except Exception:
                                pass
                            st.success("Deleted"); st.rerun()

    # --- Submission Kit (ZIP) ---
    st.subheader("Submission Kit (ZIP)")
    df_rf_all = pd.read_sql_query("SELECT id, title FROM rfps_t ORDER BY id DESC;", conn, params=())
    if df_rf_all.empty:
        st.info("Create an RFP in RFP Analyzer first (Parse → Save).")
        return

    kit_rfp = st.selectbox("RFP", options=df_rf_all["id"].tolist(),
                           format_func=lambda rid: f"#{rid} — {df_rf_all.loc[df_rf_all['id']==rid,'title'].values[0]}",
                           key="fm_kit_rfp")

    # Load files for this RFP
    try:
        df_kit = pd.read_sql_query("SELECT id, filename, path, tags FROM files_t WHERE owner_type='RFP' AND owner_id=? ORDER BY uploaded_at DESC;", conn, params=(int(kit_rfp),))
    except Exception:
        _ensure_files_table(conn)
        df_kit = pd.DataFrame(columns=["id","filename","path","tags"])
    st.caption("Select attachments to include")
    selected = []
    if df_kit.empty:
        st.write("No attachments linked to this RFP yet.")
    else:
        for _, r in df_kit.iterrows():
            if st.checkbox(f"{r['filename']}  {('['+r['tags']+']') if r.get('tags') else ''}", key=f"fm_ck_{int(r['id'])}"):
                selected.append(int(r["id"]))

    # Optional: include generated docs if they exist
    st.markdown("**Optional generated docs to include (if found):**")
    gen_paths = []
    # Proposal doc
    prop_path = str(Path(DATA_DIR) / f"Proposal_RFP_{int(kit_rfp)}.docx")
    if Path(prop_path).exists():
        if st.checkbox("Include Proposal DOCX", key="fm_inc_prop"):
            gen_paths.append(prop_path)
    # Past Performance writeups
    pp_path = str(Path(DATA_DIR) / "Past_Performance_Writeups.docx")
    if Path(pp_path).exists():
        if st.checkbox("Include Past Performance DOCX", key="fm_inc_pp"):
            gen_paths.append(pp_path)
    # White papers (include any)
    white_candidates = sorted(Path(DATA_DIR).glob("White_Paper_*.docx"))
    if white_candidates:
        inc_wp = st.multiselect("Include White Papers", options=[str(p) for p in white_candidates],
                                format_func=lambda p: Path(p).name, key="fm_inc_wp")
        gen_paths.extend(inc_wp)

    if st.button("Build ZIP", type="primary", key="fm_build_zip"):
        if not selected and not gen_paths:
            st.warning("Select at least one attachment or generated document.")
        else:
            # Collect paths
            rows = []
            if selected:
                ph = ",".join(["?"]*len(selected))
                df_sel = pd.read_sql_query(f"SELECT filename, path FROM files_t WHERE id IN ({ph});", conn, params=selected)
                for _, r in df_sel.iterrows():
                    rows.append((r["filename"], r["path"]))
            for p in gen_paths:
                rows.append((Path(p).name, p))

            # Create ZIP
            from zipfile import ZipFile, ZIP_DEFLATED
            ts = pd.Timestamp.utcnow().strftime("%Y%m%d_%H%M%S")
            zip_path = str(Path(DATA_DIR) / f"submission_kit_RFP_{int(kit_rfp)}_{ts}.zip")
            try:
                with ZipFile(zip_path, "w", compression=ZIP_DEFLATED) as z:
                    for fname, p in rows:
                        try:
                            z.write(p, arcname=fname)
                        except Exception:
                            pass
                    # Add a manifest
                    manifest = "Submission Kit Manifest\n"
                    manifest += f"RFP ID: {int(kit_rfp)}\n"
                    manifest += "\nIncluded files:\n" + "\n".join(f"- {fname}" for fname, _ in rows)
                    z.writestr("MANIFEST.txt", manifest)
                st.success("Submission kit created")

                try:
                    with open(zip_path, "rb") as _zf:
                        _zbytes = _zf.read()
                    st.download_button(
                        "Download ZIP",
                        data=_zbytes,
                        file_name=os.path.basename(zip_path),
                        mime="application/zip",
                        key=f"fm_zip_dl_{int(kit_rfp)}",
                    )
                except Exception as _e:
                    st.warning(f"ZIP file ready at {zip_path} but could not open for download: {_e}")
            except Exception as e:
                st.error(f"ZIP failed: {e}")

# ---------- Phase L: RFQ Pack ----------
def _rfq_pack_by_id(conn: "sqlite3.Connection", pid: int) -> dict | None:
    df = pd.read_sql_query("SELECT * FROM rfq_packs_t WHERE id=?;", conn, params=(pid,))
    return None if df.empty else df.iloc[0].to_dict()

def _rfq_lines(conn: "sqlite3.Connection", pid: int) -> pd.DataFrame:
    return pd.read_sql_query("SELECT id, clin_code, description, qty, unit, naics, psc FROM rfq_lines_t WHERE pack_id=? ORDER BY id ASC;", conn, params=(pid,))

def _rfq_vendors(conn: "sqlite3.Connection", pid: int) -> pd.DataFrame:
    q = """
        SELECT rv.id, rv.vendor_id, v.name, v.email, v.phone
        FROM rfq_vendors_t rv
        JOIN vendors v ON v.id = rv.vendor_id
        WHERE rv.pack_id=?
        ORDER BY v.name;
    """
    try:
        return pd.read_sql_query(q, conn, params=(pid,))
    except Exception:
        return pd.DataFrame(columns=["id","vendor_id","name","email","phone"])

def _rfq_attachments(conn: "sqlite3.Connection", pid: int) -> pd.DataFrame:
    return pd.read_sql_query("SELECT id, file_id, name, path FROM rfq_attach WHERE pack_id=? ORDER BY id ASC;", conn, params=(pid,))

def _rfq_build_zip(conn: "sqlite3.Connection", pack_id: int) -> Optional[str]:
    pack = _rfq_pack_by_id(conn, pack_id)
    if not pack:
        st.error("Pack not found"); return None
    title = pack.get("title") or f"RFQ_{pack_id}"
    # Files to include
    df_att = _rfq_attachments(conn, pack_id)
    files = []
    for _, r in df_att.iterrows():
        if r.get("path"):
            files.append((r["name"] or Path(r["path"]).name, r["path"]))
        elif r.get("file_id"):
            # fallback to files table
            try:
                df = pd.read_sql_query("SELECT filename, path FROM files_t WHERE id=?;", conn, params=(int(r["file_id"]),))
                if not df.empty:
                    files.append((df.iloc[0]["filename"], df.iloc[0]["path"]))
            except Exception:
                pass
    # CLINs CSV
    df_lines = _rfq_lines(conn, pack_id)
    clin_csv_path = str(Path(DATA_DIR) / f"rfq_{pack_id}_CLINs.csv")
    df_lines.to_csv(clin_csv_path, index=False)
    files.append((Path(clin_csv_path).name, clin_csv_path))

    # Mail-merge CSV for vendors
    df_v = _rfq_vendors(conn, pack_id)
    mail_csv_path = str(Path(DATA_DIR) / f"rfq_{pack_id}_vendors_mailmerge.csv")
    mm = df_v.rename(columns={"name":"VendorName", "email":"VendorEmail", "phone":"VendorPhone"})[["VendorName","VendorEmail","VendorPhone"]]
    mm["Subject"] = f"Request for Quote – {title}"
    due = pack.get("due_date") or ""
    mm["Body"] = (
        f"Hello {{VendorName}},\n\n"
        f"Please review the attached RFQ package for '{title}'. "
        f"Reply with pricing and availability no later than {due}.\n\n"
        f"Thank you,"
    )
    mm.to_csv(mail_csv_path, index=False)
    files.append((Path(mail_csv_path).name, mail_csv_path))

    # Build zip
    ts = pd.Timestamp.utcnow().strftime("%Y%m%d_%H%M%S")
    zip_path = str(Path(DATA_DIR) / f"RFQ_Pack_{pack_id}_{ts}.zip")
    try:
        with ZipFile(zip_path, "w", compression=ZIP_DEFLATED) as z:
            for fname, pth in files:
                try: z.write(pth, arcname=fname)
                except Exception: pass
            # manifest
            manifest = "RFQ Pack Manifest\n"
            manifest += f"Title: {title}\n"
            manifest += f"Due: {pack.get('due_date') or ''}\n"
            manifest += f"Lines: {len(df_lines)}\n"
            manifest += f"Vendors: {len(df_v)}\n"
            z.writestr("MANIFEST.txt", manifest)
        return zip_path
    except Exception as e:
        st.error(f"ZIP failed: {e}")
        return None


def run_fast_rfq(conn: "sqlite3.Connection") -> None:
    """Fast RFQ mode: lean CLINs and one-page quote export for small opportunities."""
    st.header("Fast RFQ")
    st.caption("Use this page when you just need a lean CLIN table and a one-page quote DOCX for a small RFQ.")

    # Create / open layout
    left, right = st.columns([2, 2])

    # --- Create new fast RFQ quote ---
    with left:
        st.subheader("Create")
        try:
            df_rf = pd.read_sql_query("SELECT id, title FROM rfps_t ORDER BY id DESC;", conn, params=())
        except Exception:
            df_rf = pd.DataFrame(columns=["id", "title"])
        try:
            df_deals = pd.read_sql_query("SELECT id, nickname, stage FROM deals_t ORDER BY id DESC;", conn, params=())
        except Exception:
            df_deals = pd.DataFrame(columns=["id", "nickname", "stage"])

        rfp_opts = [None] + df_rf["id"].tolist() if not df_rf.empty else [None]
        deal_opts = [None] + df_deals["id"].tolist() if not df_deals.empty else [None]

        def _rfp_label(rid: int | None) -> str:
            if not rid:
                return "None"
            try:
                row = df_rf.loc[df_rf["id"] == rid].iloc[0]
                return f"#{int(row['id'])} — {row['title'] or 'Untitled'}"
            except Exception:
                return f"#{rid}"

        def _deal_label(did: int | None) -> str:
            if not did:
                return "None"
            try:
                row = df_deals.loc[df_deals["id"] == did].iloc[0]
                nick = row.get("nickname") or f"Deal {int(row['id'])}"
                stage = row.get("stage") or ""
                if stage:
                    return f"#{int(row['id'])} — {nick} ({stage})"
                return f"#{int(row['id'])} — {nick}"
            except Exception:
                return f"#{did}"

        rfp_id = st.selectbox(
            "RFP context (optional)",
            options=rfp_opts,
            format_func=_rfp_label,
            key="fast_rfq_rfp_sel",
        )
        deal_id = st.selectbox(
            "Deal context (optional)",
            options=deal_opts,
            format_func=_deal_label,
            key="fast_rfq_deal_sel",
        )

        title = st.text_input("Quote title", key="fast_rfq_title")
        customer = st.text_input("Customer / Agency", key="fast_rfq_customer")
        contact_name = st.text_input("Customer POC name", key="fast_rfq_contact_name")
        contact_email = st.text_input("Customer POC email", key="fast_rfq_contact_email")
        contact_phone = st.text_input("Customer POC phone", key="fast_rfq_contact_phone")
        due_date = st.date_input("Quote due date", key="fast_rfq_due_date")
        valid_through = st.text_input("Quote valid through (e.g., 30 days)", key="fast_rfq_valid")
        terms = st.text_input("Key terms (optional)", key="fast_rfq_terms")
        notes = st.text_area("Notes to include in quote body (optional)", key="fast_rfq_notes", height=100)

        if st.button("Create Fast RFQ quote", key="fast_rfq_create"):
            if not title:
                st.error("Title is required.")
            else:
                from contextlib import closing as _closing
                with _closing(conn.cursor()) as cur:
                    cur.execute(
                        """
                        INSERT INTO fast_rfq_quotes(
                            title, customer, customer_ref, solnum, notice_id,
                            deal_id, rfp_id,
                            contact_name, contact_email, contact_phone,
                            due_date, valid_through, terms, notes,
                            created_at, updated_at
                        )
                        VALUES(?,?,?,?,?,?,?,?,?,?,?,?,?,?,CURRENT_TIMESTAMP,CURRENT_TIMESTAMP);
                        """,
                        (
                            title,
                            customer or None,
                            None,
                            None,
                            None,
                            int(deal_id) if deal_id else None,
                            int(rfp_id) if rfp_id else None,
                            contact_name or None,
                            contact_email or None,
                            contact_phone or None,
                            str(due_date) if due_date else None,
                            valid_through or None,
                            terms or None,
                            notes or None,
                        ),
                    )
                conn.commit()
                st.success("Fast RFQ quote created.")
                try:
                    st.rerun()
                except Exception:
                    pass

    # --- Open / edit existing fast RFQ quote ---
    with right:
        st.subheader("Open")
        try:
            df_q = pd.read_sql_query(
                "SELECT id, title, customer, due_date, valid_through, created_at FROM fast_rfq_quotes_t ORDER BY id DESC;",
                conn,
                params=(),
            )
        except Exception:
            df_q = pd.DataFrame(columns=["id", "title", "customer", "due_date", "valid_through", "created_at"])

        if df_q.empty:
            st.info("No Fast RFQ quotes yet. Create one using the form on the left.")
            return

        def _quote_label(qid: int) -> str:
            try:
                row = df_q.loc[df_q["id"] == qid].iloc[0]
                cust = row.get("customer") or ""
                title = row.get("title") or "Untitled"
                prefix = f"#{int(row['id'])}"
                if cust:
                    return f"{prefix} — {cust} — {title}"
                return f"{prefix} — {title}"
            except Exception:
                return f"Quote #{qid}"

        quote_id = st.selectbox(
            "Fast RFQ quote",
            options=df_q["id"].tolist(),
            format_func=_quote_label,
            key="fast_rfq_open_sel",
        )

    if not quote_id:
        return

    # Reload full row for editing
    try:
        df_q_full = pd.read_sql_query(
            "SELECT * FROM fast_rfq_quotes_t WHERE id=?;",
            conn,
            params=(int(quote_id),),
        )
    except Exception:
        df_q_full = pd.DataFrame()

    if df_q_full.empty:
        st.error("Selected Fast RFQ quote not found.")
        return

    qrow = df_q_full.iloc[0]

    st.subheader("Quote details")
    c1, c2 = st.columns([2, 2])
    with c1:
        title_e = st.text_input("Quote title", value=qrow.get("title") or "", key=f"fast_rfq_title_e_{int(quote_id)}")
        customer_e = st.text_input("Customer / Agency", value=qrow.get("customer") or "", key=f"fast_rfq_customer_e_{int(quote_id)}")
        valid_through_e = st.text_input(
            "Quote valid through",
            value=qrow.get("valid_through") or "",
            key=f"fast_rfq_valid_e_{int(quote_id)}",
        )
    with c2:
        contact_name_e = st.text_input(
            "Customer POC name",
            value=qrow.get("contact_name") or "",
            key=f"fast_rfq_contact_name_e_{int(quote_id)}",
        )
        contact_email_e = st.text_input(
            "Customer POC email",
            value=qrow.get("contact_email") or "",
            key=f"fast_rfq_contact_email_e_{int(quote_id)}",
        )
        contact_phone_e = st.text_input(
            "Customer POC phone",
            value=qrow.get("contact_phone") or "",
            key=f"fast_rfq_contact_phone_e_{int(quote_id)}",
        )

    notes_e = st.text_area(
        "Notes to include in quote body",
        value=qrow.get("notes") or "",
        key=f"fast_rfq_notes_e_{int(quote_id)}",
        height=80,
    )
    terms_e = st.text_input(
        "Key terms",
        value=qrow.get("terms") or "",
        key=f"fast_rfq_terms_e_{int(quote_id)}",
    )

    if st.button("Save quote details", key=f"fast_rfq_save_hdr_{int(quote_id)}"):
        from contextlib import closing as _closing
        with _closing(conn.cursor()) as cur:
            cur.execute(
                """
                UPDATE fast_rfq_quotes
                SET title=?, customer=?, contact_name=?, contact_email=?, contact_phone=?,
                    valid_through=?, terms=?, notes=?, updated_at=CURRENT_TIMESTAMP
                WHERE id=?;
                """,
                (
                    title_e or None,
                    customer_e or None,
                    contact_name_e or None,
                    contact_email_e or None,
                    contact_phone_e or None,
                    valid_through_e or None,
                    terms_e or None,
                    notes_e or None,
                    int(quote_id),
                ),
            )
        conn.commit()
        st.success("Quote details saved.")

    
    st.subheader("CLINs / line items (lean table)")
    try:
        df_lines = pd.read_sql_query(
            "SELECT id, clin_label, description, qty, unit, unit_price, extended_price FROM fast_rfq_lines WHERE quote_id=? ORDER BY id;",
            conn,
            params=(int(quote_id),),
        )
    except Exception:
        df_lines = pd.DataFrame(columns=["id", "clin_label", "description", "qty", "unit", "unit_price", "extended_price"])

    if df_lines.empty:
        base = pd.DataFrame(
            [
                {
                    "clin_label": "",
                    "description": "",
                    "qty": 1.0,
                    "unit": "EA",
                    "unit_price": 0.0,
                    "extended_price": 0.0,
                }
            ]
        )
    else:
        base = df_lines[["clin_label", "description", "qty", "unit", "unit_price", "extended_price"]].copy()

    edited = st.data_editor(
        base,
        num_rows="dynamic",
        use_container_width=True,
        hide_index=True,
        key=f"fast_rfq_lines_editor_{int(quote_id)}",
    )

    if st.button("Save CLIN table", key=f"fast_rfq_save_clins_{int(quote_id)}"):
        df_e = pd.DataFrame(edited).copy()

        def _to_float(val):
            try:
                if val in (None, ""):
                    return None
                return float(val)
            except Exception:
                return None

        from contextlib import closing as _closing
        with _closing(conn.cursor()) as cur:
            cur.execute("DELETE FROM fast_rfq_lines WHERE quote_id=?;", (int(quote_id),))
            for _, row in df_e.iterrows():
                if not (str(row.get("clin_label") or "").strip() or str(row.get("description") or "").strip()):
                    continue
                qty_v = _to_float(row.get("qty"))
                up_v = _to_float(row.get("unit_price"))
                ext_v = _to_float(row.get("extended_price"))
                if ext_v is None and qty_v is not None and up_v is not None:
                    ext_v = qty_v * up_v
                cur.execute(
                    """
                    INSERT INTO fast_rfq_lines(
                        quote_id, clin_label, description, qty, unit, unit_price, extended_price
                    )
                    VALUES(?,?,?,?,?,?,?);
                    """,
                    (
                        int(quote_id),
                        (row.get("clin_label") or "").strip() or None,
                        (row.get("description") or "").strip() or None,
                        qty_v,
                        (row.get("unit") or "").strip() or None,
                        up_v,
                        ext_v,
                    ),
                )
        conn.commit()
        st.success("CLIN table saved.")

    try:
        df_lines_saved = pd.read_sql_query(
            "SELECT extended_price FROM fast_rfq_lines WHERE quote_id=?;",
            conn,
            params=(int(quote_id),),
        )
    except Exception:
        df_lines_saved = pd.DataFrame(columns=["extended_price"])

    total_value = float(df_lines_saved["extended_price"].fillna(0).sum()) if not df_lines_saved.empty else 0.0
    st.caption(f"Estimated total value: ${total_value:,.2f}")
    st.subheader("Export one-page quote")
    

    if st.button("Export Fast RFQ DOCX", type="primary", key=f"fast_rfq_export_{int(quote_id)}"):
        # Ensure jobs schema exists
        try:
            ensure_jobs_schema(conn)
        except Exception:
            pass

        # Build CLIN rows in the format expected by _export_docx
        try:
            df_clins = pd.read_sql_query(
                "SELECT clin_label, description, qty, unit, unit_price, extended_price FROM fast_rfq_lines WHERE quote_id=? ORDER BY id;",
                conn,
                params=(int(quote_id),),
            )
        except Exception:
            df_clins = pd.DataFrame(columns=["clin_label", "description", "qty", "unit", "unit_price", "extended_price"])

        clins_payload = []
        if not df_clins.empty:
            for _, row in df_clins.iterrows():
                clins_payload.append(
                    {
                        "clin": row.get("clin_label"),
                        "description": row.get("description"),
                        "qty": row.get("qty"),
                        "unit": row.get("unit"),
                        "unit_price": row.get("unit_price"),
                        "extended_price": row.get("extended_price"),
                    }
                )

        # Track this export as a job row (even though work runs inline for now)
        try:
            try:
                _user_name = get_current_user_name()
            except Exception:
                _user_name = ""
            payload = {
                "scope": "fast_rfq_export_docx",
                "quote_id": int(quote_id),
                "has_clins": bool(clins_payload),
            }
            job_id = jobs_enqueue(
                conn,
                job_type="fast_rfq_export_docx",
                payload=payload,
                created_by=_user_name or None,
            )
        except Exception:
            job_id = None

        out_name = f"Fast_RFQ_{int(quote_id)}.docx"
        out_path = os.path.join(DATA_DIR, out_name)

        # Compose simple one-page style body
        body_lines = []
        cust_label = customer_e or qrow.get("customer") or ""
        if cust_label:
            body_lines.append(cust_label)
        poc_line = ""
        if contact_name_e:
            poc_line += contact_name_e
        if contact_email_e:
            poc_line += f" <{contact_email_e}>"
        if contact_phone_e:
            if poc_line:
                poc_line += " | "
            poc_line += contact_phone_e
        if poc_line:
            body_lines.append(poc_line)

        body_lines.append("")
        subj = title_e or qrow.get("title") or "Quote"
        body_lines.append(f"Subject: {subj}")
        body_lines.append("")
        body_lines.append("Thank you for the opportunity to provide pricing. Below is our proposed pricing for the requested work.")
        if notes_e:
            body_lines.append("")
            body_lines.append(notes_e)

        if terms_e:
            body_lines.append("")
            body_lines.append(f"Terms: {terms_e}")

        sections = [
            {
                "title": "Quote",
                "body": "\n".join(body_lines),
            }
        ]

        try:
            if job_id:
                try:
                    jobs_update_status(
                        conn,
                        job_id,
                        status="running",
                        progress=0.1,
                        mark_started=True,
                    )
                except Exception:
                    pass

            exported = _export_docx(
                out_path,
                doc_title=subj,
                sections=sections,
                clins=clins_payload,
                checklist=None,
                metadata={
                    "fast_rfq": True,
                    "quote_id": int(quote_id),
                },
            )

            if exported and job_id:
                try:
                    jobs_update_status(
                        conn,
                        job_id,
                        status="done",
                        progress=1.0,
                        result={"path": str(exported)},
                        mark_finished=True,
                    )
                except Exception:
                    pass
        except Exception as e:
            try:
                st.error(f"Fast RFQ DOCX export failed: {e}")
            except Exception:
                pass
            if job_id:
                try:
                    jobs_update_status(
                        conn,
                        job_id,
                        status="failed",
                        error_message=str(e),
                        mark_finished=True,
                    )
                except Exception:
                    pass
            exported = None

        if exported:
            st.success(f"Exported to {exported}")
            try:
                with open(exported, "rb") as _f:
                    _data = _f.read()
                st.download_button(
                    "Download Fast RFQ DOCX",
                    data=_data,
                    file_name=out_name,
                    mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                    key=f"dl_fast_rfq_docx_{int(quote_id)}",
                )
            except Exception:
                st.info("Fast RFQ DOCX is ready. Check the data directory if the download button is not available.")


def run_rfq_pack(conn: "sqlite3.Connection") -> None:
    st.header("RFQ Pack")
    st.caption("Build vendor-ready RFQ packages from your CLINs, attachments, and vendor list.")

    # -- Create / open
    left, right = st.columns([2,2])
    with left:
        st.subheader("Create")
        df_rf = pd.read_sql_query("SELECT id, title FROM rfps_t ORDER BY id DESC;", conn, params=())
        rf_opt = st.selectbox("RFP (optional)", options=[None] + df_rf["id"].tolist(),
                              format_func=lambda x: "None" if x is None else f"#{x} — {df_rf.loc[df_rf['id']==x,'title'].values[0]}",
                              key="rfq_rfp_sel")
        title = st.text_input("Pack title", key="rfq_title")
        due = st.date_input("Quote due date", key="rfq_due")
        instr = st.text_area("Instructions to vendors (email body)", height=100, key="rfq_instr")
        if st.button("Create RFQ Pack", key="rfq_create"):
            if not title.strip():
                st.error("Title required")
            else:
                with closing(conn.cursor()) as cur:
                    cur.execute("""
                        INSERT INTO rfq_packs(rfp_id, deal_id, title, instructions, due_date, created_at, updated_at)
                        VALUES (?, ?, ?, ?, ?, datetime('now'), datetime('now'));
                    """, (rf_opt if rf_opt else None, None, title.strip(), instr.strip(), str(due)))
                    conn.commit()
                st.success("Created"); st.rerun()
    with right:
        st.subheader("Open")
        df_pk = pd.read_sql_query("SELECT id, title, due_date, created_at FROM rfq_packs_t ORDER BY id DESC;", conn, params=())
        if df_pk.empty:
            st.info("No RFQ packs yet")
            return
        pk_sel = st.selectbox("RFQ Pack", options=df_pk["id"].tolist(),
                              format_func=lambda pid: f"#{pid} — {df_pk.loc[df_pk['id']==pid,'title'].values[0]} (due {df_pk.loc[df_pk['id']==pid,'due_date'].values[0] or '—'})",
                              key="rfq_open_sel")

    st.divider()
    st.subheader(f"Editing pack #{int(pk_sel)}")

    # ---- CLINs / Lines ----
    st.markdown("### CLINs / Lines")
    df_lines = _rfq_lines(conn, int(pk_sel))
    _styled_dataframe(df_lines, use_container_width=True, hide_index=True)
    c1, c2, c3, c4, c5, c6 = st.columns([1.2,3,1,1,1,1])
    with c1:
        l_code = st.text_input("CLIN", key="rfq_line_code")
    with c2:
        l_desc = st.text_input("Description", key="rfq_line_desc")
    with c3:
        l_qty = st.number_input("Qty", min_value=0.0, value=1.0, step=1.0, key="rfq_line_qty")
    with c4:
        l_unit = st.text_input("Unit", value="EA", key="rfq_line_unit")
    with c5:
        l_naics = st.text_input("NAICS", key="rfq_line_naics")
    with c6:
        l_psc = st.text_input("PSC", key="rfq_line_psc")
    if st.button("Add Line", key="rfq_line_add"):
        with closing(conn.cursor()) as cur:
            cur.execute("""
                INSERT INTO rfq_lines(pack_id, clin_code, description, qty, unit, naics, psc)
                VALUES (?, ?, ?, ?, ?, ?, ?);
            """, (int(pk_sel), l_code.strip(), l_desc.strip(), float(l_qty or 0), l_unit.strip(), l_naics.strip(), l_psc.strip()))
            conn.commit()
        st.success("Line added"); st.rerun()

    if not df_lines.empty:
        st.markdown("**Edit existing lines**")
        for _, r in df_lines.iterrows():
            ec1, ec2, ec3, ec4 = st.columns([3,1,1,1])
            with ec1:
                nd = st.text_input("Desc", value=r["description"] or "", key=f"rfq_line_e_desc_{int(r['id'])}")
            with ec2:
                nq = st.number_input("Qty", value=float(r["qty"] or 0), step=1.0, key=f"rfq_line_e_qty_{int(r['id'])}")
            with ec3:
                nu = st.text_input("Unit", value=r["unit"] or "EA", key=f"rfq_line_e_unit_{int(r['id'])}")
            with ec4:
                if st.button("Save", key=f"rfq_line_e_save_{int(r['id'])}"):
                    with closing(conn.cursor()) as cur:
                        cur.execute("UPDATE rfq_lines SET description=?, qty=?, unit=? WHERE id=?;",
                                    (nd.strip(), float(nq or 0), nu.strip(), int(r["id"])))
                        conn.commit()
                    st.success("Updated"); st.rerun()

    st.divider()

    # ---- Attachments ----
    st.markdown("### Attachments")
    pack = _rfq_pack_by_id(conn, int(pk_sel))
    rfp_id = pack.get("rfp_id")
    if rfp_id:
        df_rfp_files = pd.read_sql_query("SELECT id, filename, path, tags FROM files_t WHERE owner_type='RFP' AND owner_id=? ORDER BY uploaded_at DESC;", conn, params=(int(rfp_id),))
    else:
        df_rfp_files = pd.DataFrame(columns=["id","filename","path","tags"])
    df_att = _rfq_attachments(conn, int(pk_sel))
    _styled_dataframe(df_att.drop(columns=[]), use_container_width=True, hide_index=True)

    st.markdown("**Add from File Manager**")
    # allow selecting from all files
    df_all_files = pd.read_sql_query("SELECT id, filename FROM files_t ORDER BY uploaded_at DESC;", conn, params=())
    add_file = st.selectbox("File", options=[None] + df_all_files["id"].astype(int).tolist(),
                            format_func=lambda i: "Choose…" if i is None else f"#{i} — {df_all_files.loc[df_all_files['id']==i,'filename'].values[0]}",
                            key="rfq_att_file")
    if st.button("Add Attachment", key="rfq_att_add"):
        if add_file is None:
            st.warning("Pick a file")
        else:
            df_one = pd.read_sql_query("SELECT filename, path FROM files_t WHERE id=?;", conn, params=(int(add_file),))
            if df_one.empty:
                st.error("File not found")
            else:
                with closing(conn.cursor()) as cur:
                    cur.execute("INSERT INTO rfq_attach(pack_id, file_id, name, path) VALUES(?,?,?,?);",
                                (int(pk_sel), int(add_file), df_one.iloc[0]["filename"], df_one.iloc[0]["path"]))
                    conn.commit()
                st.success("Added"); st.rerun()

    if not df_att.empty:
        for _, r in df_att.iterrows():
            dc1, dc2 = st.columns([3,1])
            with dc1:
                st.caption(f"#{int(r['id'])} — {r['name'] or Path(r['path']).name}")
            with dc2:
                if st.button("Remove", key=f"rfq_att_del_{int(r['id'])}"):
                    with closing(conn.cursor()) as cur:
                        cur.execute("DELETE FROM rfq_attach WHERE id=?;", (int(r["id"]),))
                        conn.commit()
                    st.success("Removed"); st.rerun()

    st.divider()

    # ---- Vendors ----
    st.markdown("### Vendors")
    try:
        df_vendors = pd.read_sql_query("SELECT id, name, email FROM vendors_t ORDER BY name;", conn, params=())
    except Exception as e:
        st.info("No vendors table yet. Use Subcontractor Finder to add vendors.")
        df_vendors = pd.DataFrame(columns=["id","name","email"])
    df_rv = _rfq_vendors(conn, int(pk_sel))
    _styled_dataframe(df_rv[["name","email","phone"]] if not df_rv.empty else pd.DataFrame(), use_container_width=True, hide_index=True)

    add_vs = st.multiselect("Add vendors", options=df_vendors["id"].astype(int).tolist(),
                            format_func=lambda vid: df_vendors.loc[df_vendors["id"]==vid, "name"].values[0],
                            key="rfq_vendor_add")
    if st.button("Add Selected Vendors", key="rfq_vendor_add_btn"):
        with closing(conn.cursor()) as cur:
            for vid in add_vs:
                try:
                    cur.execute("INSERT OR IGNORE INTO rfq_vendors(pack_id, vendor_id) VALUES(?,?);", (int(pk_sel), int(vid)))
                except Exception:
                    pass
            conn.commit()
        st.success("Vendors added"); st.rerun()

    if not df_rv.empty:
        for _, r in df_rv.iterrows():
            vc1, vc2 = st.columns([3,1])
            with vc1:
                st.caption(f"{r['name']} — {r.get('email') or ''}")
            with vc2:
                if st.button("Remove", key=f"rfq_vendor_del_{int(r['id'])}"):
                    with closing(conn.cursor()) as cur:
                        cur.execute("DELETE FROM rfq_vendors WHERE id=?;", (int(r["id"]),))
                        conn.commit()
                    st.success("Removed"); st.rerun()

    st.divider()

    # ---- Build + Exports ----
    st.markdown("### Build & Export")
    czip, cmcsv, cclin = st.columns([2,2,2])
    with czip:

        

        if st.button("Build RFQ ZIP", type="primary", key="rfq_build_zip"):
            # Track RFQ pack ZIP build as a job.
            try:
                ensure_jobs_schema(conn)
            except Exception:
                pass

            try:
                try:
                    _user_name = get_current_user_name()
                except Exception:
                    _user_name = ""
                payload = {
                    "scope": "rfq_pack_build",
                    "pack_id": int(pk_sel) if pk_sel is not None else None,
                }
                job_id = jobs_enqueue(
                    conn,
                    job_type="rfq_pack_build",
                    payload=payload,
                    created_by=_user_name or None,
                )
            except Exception:
                job_id = None

            try:
                if job_id:
                    jobs_update_status(
                        conn,
                        job_id,
                        status="running",
                        progress=0.1,
                        mark_started=True,
                    )
            except Exception:
                pass

            z = _rfq_build_zip(conn, int(pk_sel))
            if z:
                try:
                    with open(z, "rb") as _f:
                        st.success("ZIP ready")
                        st.download_button(
                            "Download ZIP",
                            data=_f.read(),
                            file_name=Path(z).name,
                            mime="application/zip",
                            key="rfq_zip_download",
                        )
                except Exception:
                    st.warning(f"ZIP created at {z} but could not prepare a download button. Check the data directory.")
                try:
                    if job_id:
                        jobs_update_status(
                            conn,
                            job_id,
                            status="done",
                            progress=1.0,
                            mark_finished=True,
                            result={"zip_path": str(z)},
                        )
                except Exception:
                    pass


    with cmcsv:
        if st.button("Export Vendors Mail-Merge CSV", key="rfq_mail_csv"):
            df_v = _rfq_vendors(conn, int(pk_sel))
            if df_v.empty:
                st.warning("No vendors selected")
            else:
                out = df_v.rename(columns={"name":"VendorName","email":"VendorEmail","phone":"VendorPhone"})[["VendorName","VendorEmail","VendorPhone"]]
                out["Subject"] = f"Request for Quote – {_rfq_pack_by_id(conn, int(pk_sel)).get('title')}"
                out["Body"] = _rfq_pack_by_id(conn, int(pk_sel)).get("instructions") or ""
                path = str(Path(DATA_DIR) / f"rfq_{int(pk_sel)}_mailmerge.csv")
                out.to_csv(path, index=False)
                try:
                    with open(path, "rb") as _f:
                        st.success("Exported")
                        st.download_button(
                            "Download CSV",
                            data=_f.read(),
                            file_name=Path(path).name,
                            mime="text/csv",
                            key="rfq_mail_csv_download",
                        )
                except Exception:
                    st.info(f"CSV saved at {path}")

    with cclin:
        if st.button("Export CLINs CSV", key="rfq_clins_csv"):
            df = _rfq_lines(conn, int(pk_sel))
            if df.empty:
                st.warning("No CLINs yet")
            else:
                path = str(Path(DATA_DIR) / f"rfq_{int(pk_sel)}_CLINs.csv")
                df.to_csv(path, index=False)
                try:
                    with open(path, "rb") as _f:
                        st.success("Exported")
                        st.download_button(
                            "Download CSV",
                            data=_f.read(),
                            file_name=Path(path).name,
                            mime="text/csv",
                            key="rfq_clins_csv_download",
                        )
                except Exception:
                    st.info(f"CSV saved at {path}")

def _db_path_from_conn(conn: "sqlite3.Connection") -> str:
    try:
        df = pd.read_sql_query("PRAGMA database_list;", conn, params=())
        p = df[df["name"]=="main"]["file"].values[0]
        return p or str(Path(DATA_DIR) / "app.db")
    except Exception:
        return str(Path(DATA_DIR) / "app.db")

def migrate(conn: "sqlite3.Connection") -> None:
    """Lightweight idempotent migrations and indices."""
    with closing(conn.cursor()) as cur:
        # read current version
        try:
            ver = int(pd.read_sql_query("SELECT ver FROM schema_version WHERE id=1;", conn, params=()).iloc[0]["ver"])
        except Exception:
            ver = 0

        # v1: add common indexes
        if ver < 1:
            try:
                cur.execute("CREATE INDEX IF NOT EXISTS idx_deals_stage ON deals(stage);")
                cur.execute("CREATE INDEX IF NOT EXISTS idx_deals_status ON deals(status);")
                cur.execute("CREATE INDEX IF NOT EXISTS idx_lm_items_rfp ON lm_items(rfp_id);")
                cur.execute("CREATE INDEX IF NOT EXISTS idx_files_owner2 ON files(owner_type, owner_id);")
                cur.execute("CREATE INDEX IF NOT EXISTS idx_tasks_due ON tasks(due_date);")
            except Exception:
                pass
            cur.execute("UPDATE schema_version SET ver=1 WHERE id=1;")
            conn.commit()

        # v2: ensure NOT NULL defaults where safe (no schema changes if exists)
        if ver < 2:
            try:
                cur.execute("PRAGMA foreign_keys=ON;")
            except Exception:
                pass
            cur.execute("UPDATE schema_version SET ver=2 WHERE id=1;")
            conn.commit()

        # v3: WAL checkpoint to ensure clean state
        if ver < 3:
            try:
                cur.execute("PRAGMA wal_checkpoint(FULL);")
            except Exception:
                pass
            cur.execute("UPDATE schema_version SET ver=3 WHERE id=1;")
            conn.commit()

        # v4: buyer_stats analytics view for Top Buyers dashboard
        if ver < 4:
            try:
                cur.execute("""
                    CREATE VIEW IF NOT EXISTS buyer_stats AS
                    SELECT
                        n.tenant_id AS tenant_id,
                        COALESCE(n.agency, '') AS agency,
                        COALESCE(n.naics, '') AS naics,
                        COUNT(*) AS total_awards,
                        SUM(COALESCE(n.award_amount, 0.0)) AS total_value,
                        MAX(CASE WHEN n.award_date IS NOT NULL AND n.award_date <> '' THEN n.award_date ELSE NULL END) AS last_award_date
                    FROM notices n
                    JOIN current_tenant ct ON n.tenant_id = ct.ctid
                    WHERE (n.award_date IS NOT NULL AND n.award_date <> '')
                       OR (n.award_amount IS NOT NULL AND n.award_amount <> 0)
                    GROUP BY n.tenant_id, agency, naics;
                """)
            except Exception:
                pass
            cur.execute("UPDATE schema_version SET ver=4 WHERE id=1;")
            conn.commit()

        # v5: vendor_award_stats analytics view for competitor / partner analysis
        if ver < 5:
            try:
                cur.execute("""
                    CREATE VIEW IF NOT EXISTS vendor_award_stats AS
                    SELECT
                        n.tenant_id AS tenant_id,
                        COALESCE(n.awardee, '') AS vendor_name,
                        '' AS uei_duns,
                        COALESCE(n.naics, '') AS naics,
                        COUNT(*) AS total_awards,
                        SUM(COALESCE(n.award_amount, 0.0)) AS total_value,
                        MAX(CASE WHEN n.award_date IS NOT NULL AND n.award_date <> '' THEN n.award_date ELSE NULL END) AS last_award_date
                    FROM notices n
                    JOIN current_tenant ct ON n.tenant_id = ct.ctid
                    WHERE (n.award_date IS NOT NULL AND n.award_date <> '')
                       OR (n.award_amount IS NOT NULL AND n.award_amount <> 0)
                    GROUP BY n.tenant_id, vendor_name, uei_duns, naics;
                """)
            except Exception:
                pass
            cur.execute("UPDATE schema_version SET ver=5 WHERE id=1;")
            conn.commit()

# ---------- Phase N: Backup & Data ----------
def _current_tenant(conn: "sqlite3.Connection") -> int:
    try:
        return int(pd.read_sql_query("SELECT ctid FROM current_tenant WHERE id=1;", conn, params=()).iloc[0]["ctid"])
    except Exception:
        return 1

def _safe_name(s: str) -> str:
    return re.sub(r"[^A-Za-z0-9_.-]+", "_", s or "")

def _backup_db(conn: "sqlite3.Connection") -> Optional[str]:
    # Prefer VACUUM INTO; fallback to file copy using sqlite3 backup API
    db_path = _db_path_from_conn(conn)
    ts = pd.Timestamp.utcnow().strftime("%Y%m%d_%H%M%S")
    out = Path(DATA_DIR) / f"backup_{ts}.db"
    try:
        with closing(conn.cursor()) as cur:
            cur.execute(f"VACUUM INTO '{str(out)}';")
        return str(out)
    except Exception:
        # fallback: use backup API
        try:
            import sqlite3 as _sq
            src = _sq.connect(db_path)
            dst = _sq.connect(str(out))
            with dst:
                src.backup(dst)
            src.close(); dst.close()
            return str(out)
        except Exception as e:
            st.error(f"Backup failed: {e}")
            return None

def _restore_db_from_upload(conn: "sqlite3.Connection", upload) -> bool:
    import sqlite3
    # Use backup API to copy uploaded DB into main DB file
    db_path = _db_path_from_conn(conn)
    tmp = Path(DATA_DIR) / ("restore_" + _safe_name(upload.name))
    try:
        tmp.write_bytes(upload.getbuffer())
    except Exception as e:
        st.error(f"Could not write uploaded file: {e}")
        return False
    try:
        src = sqlite3.connect(str(tmp))
        dst = sqlite3.connect(db_path)
        with dst:
            src.backup(dst)  # replaces content
        src.close(); dst.close()
        return True
    except Exception as e:
        st.error(f"Restore failed: {e}")
        return False

def _export_table_csv(conn: "sqlite3.Connection", table_or_view: str, scoped: bool = True) -> Optional[str]:
    name = table_or_view
    if scoped and not name.endswith("_t"):
        # if a view exists, prefer it
        name_t = name + "_t"
        try:
            pd.read_sql_query(f"SELECT 1 FROM {name_t} LIMIT 1;", conn)
            name = name_t
        except Exception:
            pass
    try:
        df = pd.read_sql_query(f"SELECT * FROM {name};", conn)
        if df.empty:
            st.info("No rows to export.")
        path = Path(DATA_DIR) / f"export_{name}_{pd.Timestamp.utcnow().strftime('%Y%m%d_%H%M%S')}.csv"
        df.to_csv(path, index=False)
        return str(path)
    except Exception as e:
        st.error(f"Export failed: {e}")
        return None

def _import_csv_into_table(conn: "sqlite3.Connection", csv_file, table: str, scoped_to_current: bool=True) -> int:
    # Read CSV and insert rows. If tenant_id missing and scoped, stamp with current tenant.
    try:
        df = pd.read_csv(io.BytesIO(csv_file.getbuffer()))
    except Exception as e:
        st.error(f"CSV read failed: {e}")
        return 0
    if scoped_to_current and "tenant_id" not in df.columns:
        df["tenant_id"] = _current_tenant(conn)
    # Align columns with destination
    try:
        cols = pd.read_sql_query(f"PRAGMA table_info({table});", conn)["name"].tolist()
    except Exception as e:
        st.error(f"Table info failed: {e}")
        return 0
    present = [c for c in df.columns if c in cols]
    if not present:
        st.error("No matching columns in CSV.")
        return 0
    df2 = df[present].copy()
    # Drop ID if autoincrement
    if "id" in df2.columns:
        try: df2 = df2.drop(columns=["id"])
        except Exception: pass
    # Insert
    try:
        placeholders = ",".join(["?"]*len(df2.columns))
        sql = f"INSERT INTO {table}({','.join(df2.columns)}) VALUES({placeholders});"
        with closing(conn.cursor()) as cur:
            cur.executemany(sql, df2.itertuples(index=False, name=None))
            conn.commit()
        return len(df2)
    except Exception as e:
        st.error(f"Import failed: {e}")
        return 0


# ---- Database schema doc & Postgres migration notes ----
MIGRATION_NOTES = """### When to move from SQLite to Postgres

Move to Postgres when:

- Concurrent users regularly exceed 5–10 at the same time.
- The SQLite database file grows beyond roughly 2–4 GB or you start to see frequent locking or "database is busy" errors.
- You need stricter access control, row-level security, or high-availability features.
- You plan to offer ELA GovCon as a multi-tenant SaaS to other companies and expect heavier load.

### One-time migration plan

1. Schema
   - Create a Postgres database (for example: `ela_govcon`).
   - Mirror the key SQLite tables (rfps, rfp_sections, deals, activities, tasks, contacts, vendors, rfq_packs, rfq_lines,
     rfq_vendors, rfq_attach, proposals, proposal_sections, outreach_log, debug_log, saved_searches, alerts, chat tables, etc.)
     with explicit types: INTEGER / BIGINT, NUMERIC, TEXT, TIMESTAMP WITH TIME ZONE, BOOLEAN.
   - Keep the same primary keys and foreign keys. Preserve `tenant_id` and any `owner` / `user_id` columns.

2. Data migration
   - Put the app into maintenance mode so no new writes happen.
   - Export each core table from SQLite to CSV using the Backup & Data tab or a small export script.
   - For each table, use Postgres `\copy` from psql, for example:

       \copy rfps(id, notice_id, title, status, due_date, agency, naics, set_aside, tenant_id)
       FROM '/path/rfps.csv'
       WITH (FORMAT csv, HEADER true);

   - Spot-check row counts and a few sample records between SQLite and Postgres.

3. Application connection changes
   - Replace the `sqlite3.connect` call in `get_db()` with a Postgres driver (for example `psycopg2` or SQLAlchemy)
     using a URL stored in `st.secrets`.
   - Remove SQLite-only PRAGMAs and replace any `datetime('now')`, `INSERT OR IGNORE`, or `ON CONFLICT` patterns with
     Postgres-friendly equivalents.
   - Recreate the `schema_version` table in Postgres and run the migrations so future schema upgrades stay centralized.

4. Cutover
   - Point a test copy of the app at Postgres and validate SAM Watch, Deals, RFP Analyzer, Outreach, Subcontractor Finder,
     and Proposal Builder end to end.
   - When verified, point production to Postgres and keep the last SQLite file as a read-only backup.
"""

def _schema_markdown(conn: "sqlite3.Connection") -> str:
    """Generate a simple schema document listing tables, columns, and types."""
    from contextlib import closing as _closing
    import pandas as _pd

    lines: list[str] = []
    try:
        with _closing(conn.cursor()) as cur:
            cur.execute(
                "SELECT name, type FROM sqlite_master "
                "WHERE type IN ('table','view') AND name NOT LIKE 'sqlite_%' "
                "ORDER BY type, name;"
            )
            entries = cur.fetchall()
    except Exception:
        return "Schema information not available."

    # Short, human-friendly table descriptions (extend as needed).
    table_desc: dict[str, str] = {
        "rfps": "RFP / notice headers for each opportunity you are tracking.",
        "rfp_sections": "Parsed RFP sections and L&M items for compliance tracking.",
        "rfp_files": "Files attached to RFPs (PDF, DOCX, XLSX, etc.).",
        "rfp_chunks": "Searchable text chunks used by the analyzer and chat tools.",
        "deals": "Pipeline deals used by the CRM and win-probability dashboard.",
        "activities": "CRM activities (calls, emails, tasks) linked to deals.",
        "tasks": "Lightweight task list items linked to deals and contacts.",
        "contacts": "COs, POCs, CORs, and other people in your CRM.",
        "vendors": "Subcontractors / teaming partners and vendor metadata.",
        "vendor_contacts": "Named contacts at each vendor (email, phone, role).",
        "rfq_packs": "RFQ pricing packs created from CLINs or manual entry.",
        "rfq_lines": "Individual RFQ line items with quantities and units.",
        "rfq_vendors": "Vendor-specific RFQ metadata (quotes, status, notes).",
        "rfq_attach": "Attachments linked to RFQ packs (exported worksheets, etc.).",
        "lm_items": "Parsed L&M items and compliance checklist entries.",
        "lm_meta": "Metadata for L&M parsing and checklist runs.",
        "proposal_sections": "Stored proposal sections generated in Proposal Builder.",
        "proposals": "Top-level proposals (name, rfp_id, version, status).",
        "outreach_log": "Email blast log with per-recipient delivery status.",
        "debug_log": "Internal debug log for errors and important events.",
        "y2_threads": "Ask-the-CO / analyzer chat threads per RFP.",
        "y2_messages": "Messages inside each y2 thread.",
        "chat_plus_threads": "Chat+ threads linked to RFPs or generic topics.",
        "chat_plus_messages": "Messages inside each Chat+ thread.",
        "saved_searches": "Saved SAM Watch searches.",
        "alerts": "SAM alert rules and their configuration.",
        "smtp_settings": "Stored outbound SMTP sender accounts.",
        "tenants": "Tenant records (for example: ELA, or future customer orgs).",
        "current_tenant": "Pointer to the active tenant for this workspace.",
    }

    for name, obj_type in entries:
        name = str(name)
        obj_type = str(obj_type)
        lines.append(f"### {name} ({obj_type})")
        desc = table_desc.get(name)
        if desc:
            lines.append(desc)

        try:
            df_cols = _pd.read_sql_query(f"PRAGMA table_info({name});", conn)
        except Exception:
            df_cols = _pd.DataFrame()

        if df_cols is not None and not df_cols.empty:
            for _, row in df_cols.iterrows():
                col_name = str(row.get("name", ""))
                col_type = str(row.get("type") or "TEXT")
                notnull = bool(row.get("notnull"))
                dflt = row.get("dflt_value")
                pk = bool(row.get("pk"))
                meta_bits = [col_type]
                if pk:
                    meta_bits.append("PRIMARY KEY")
                if notnull:
                    meta_bits.append("NOT NULL")
                if dflt not in (None, ""):
                    meta_bits.append(f"DEFAULT {dflt}")
                meta = ", ".join(meta_bits)
                lines.append(f"- **{col_name}** — {meta}")
        else:
            lines.append("- (no column metadata available)")
        lines.append("")

    return "\n".join(lines) + "\n"
def run_backup_and_data(conn: "sqlite3.Connection") -> None:
    st.header("Backup & Data")
    st.caption("WAL on; lightweight migrations; export/import CSV; backup/restore the SQLite DB.")

    st.subheader("Database Info")
    dbp = _db_path_from_conn(conn)
    st.write(f"Path: `{dbp}`")
    try:
        ver = pd.read_sql_query("SELECT ver FROM schema_version WHERE id=1;", conn, params=()).iloc[0]["ver"]
    except Exception:
        ver = "n/a"
    st.write(f"Schema version: **{ver}**")

    c1, c2, c3 = st.columns([2,2,2])
    with c1:
        if st.button("Run Migrations"):
            try:
                migrate(conn); st.success("Migrations done")
            except Exception as e:
                st.error(f"Migrations failed: {e}")
    with c2:
        if st.button("WAL Checkpoint (FULL)"):
            try:
                with closing(conn.cursor()) as cur:
                    cur.execute("PRAGMA wal_checkpoint(FULL);")
                st.success("Checkpoint complete")
            except Exception as e:
                st.error(f"Checkpoint failed: {e}")
    with c3:
        if st.button("Analyze DB"):
            try:
                with closing(conn.cursor()) as cur:
                    cur.execute("ANALYZE;")
                st.success("ANALYZE done")
            except Exception as e:
                st.error(f"ANALYZE failed: {e}")

    st.divider()

    st.subheader("Backup & Restore")
    b1, b2 = st.columns([2, 2])

    # Backup is now queued as a background job (backup_full), not run inline.
    with b1:
        if st.button("Queue Backup (.db)", key="backup_queue_btn"):
            try:
                ensure_jobs_schema(conn)
                try:
                    user_name = get_current_user_name()
                except Exception:
                    user_name = ""
                payload = {
                    "scope": "backup_full",
                    "db_path": _db_path_from_conn(conn),
                    "tenant_id": _current_tenant(conn),
                }
                job_id = jobs_enqueue(
                    conn,
                    job_type="backup_full",
                    payload=payload,
                    created_by=user_name or None,
                )
                st.success(f"Backup job queued as #{job_id}. Track progress in the My Jobs tab.")
            except Exception as e:
                st.error(f"Could not queue backup job: {e}")

    # Restore is also handled via a background job (restore_from_backup).
    with b2:
        up = st.file_uploader(
            "Upload backup .db file to restore from",
            type=["db", "sqlite", "sqlite3"],
            key="backup_restore_upload",
        )
        if up and st.button("Queue Restore", key="backup_restore_btn"):
            try:
                ensure_jobs_schema(conn)
                try:
                    user_name = get_current_user_name()
                except Exception:
                    user_name = ""
                tmp = Path(DATA_DIR) / ("restore_job_" + _safe_name(up.name))
                try:
                    tmp.write_bytes(up.getbuffer())
                except Exception as e:
                    st.error(f"Could not store uploaded backup file: {e}")
                else:
                    payload = {
                        "scope": "restore_from_backup",
                        "upload_name": up.name,
                        "upload_path": str(tmp),
                        "db_path": _db_path_from_conn(conn),
                        "tenant_id": _current_tenant(conn),
                    }
                    job_id = jobs_enqueue(
                        conn,
                        job_type="restore_from_backup",
                        payload=payload,
                        created_by=user_name or None,
                    )
                    st.warning("Restoring a backup will overwrite the current SQLite DB file once the job runs.")
                    st.success(f"Restore job queued as #{job_id}. Track progress in the My Jobs tab.")
            except Exception as e:
                st.error(f"Could not queue restore job: {e}")
    st.divider()
    st.subheader("Export / Import CSV")
    tables = ["rfps","lm_items","lm_meta","deals","activities","tasks","deal_stage_log",
              "vendors","files","rfq_packs","rfq_lines","rfq_vendors","rfq_attach","contacts",
              "proposals","proposal_sections","outreach_log","debug_log"]
    tsel = st.selectbox("Table", options=tables, key="persist_tbl")
    e1, e2 = st.columns([2,2])
    with e1:
        if st.button("Export CSV (current workspace)"):
            p = _export_table_csv(conn, tsel, scoped=True)
            if p: st.success("Exported"); st.markdown(f"[Download CSV]({p})")
    with e2:
        if st.button("Export CSV (all rows)"):
            p = _export_table_csv(conn, tsel, scoped=False)
            if p: st.success("Exported"); st.markdown(f"[Download CSV]({p})")

    upcsv = st.file_uploader("Import into selected table from CSV", type=["csv"], key="persist_upcsv")
    if upcsv and st.button("Import CSV"):
        n = _import_csv_into_table(conn, upcsv, tsel, scoped_to_current=True)
        if n:
            st.success(f"Imported {n} row(s) into {tsel}")
            st.rerun()

    st.divider()
    st.subheader("Schema document")
    st.caption("Live view of the SQLite schema for this workspace (tables, columns, and types).")
    try:
        schema_md = _schema_markdown(conn)
        st.markdown(schema_md)
    except Exception as e:
        st.info(f"Schema doc not available: {e}")

    st.divider()
    st.subheader("SQLite → Postgres migration plan")
    st.caption("High-level checklist for moving this workspace to Postgres when you are ready.")
    try:
        st.markdown(MIGRATION_NOTES)
    except Exception:
        st.write("Set MIGRATION_NOTES for detailed migration guidance.")

# ---------- Phase O: Global Theme & Layout ----------
def _apply_theme_old() -> None:
    css = """
    <style>
    /* Base font and spacing */
    sig_html, body, [class*="css"]  { font-family: Inter, system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial, "Apple Color Emoji","Segoe UI Emoji"; }
    .main .block-container { padding-top: 1rem; padding-bottom: 4rem; max-width: 1400px; }
    /* Headings */
    h1, h2, h3 { letter-spacing: 0.2px; }
    h1 { font-size: 1.8rem; margin-bottom: .25rem; }
    h2 { font-size: 1.2rem; margin-top: 1rem; }
    /* Sidebar */
    section[data-testid="stSidebar"] { width: 320px !important; }
    .sidebar-brand { font-weight: 700; font-size: 1.1rem; margin: .25rem 0 .5rem 0; }
    .sidebar-subtle { color: rgba(0,0,0,.55); font-size: .85rem; margin-bottom: .5rem; }
    /* Cards */
    .card { border: 1px solid rgba(0,0,0,.08); border-radius: 14px; padding: 14px 16px; margin: 8px 0 14px 0; box-shadow: 0 1px 2px rgba(0,0,0,.04); background: #fff; }
    .card h3 { margin: 0 0 6px 0; font-size: 1.05rem; }
    /* Dataframes */
    div[data-testid="stDataFrame"] { border-radius: 12px; overflow: hidden; border: 1px solid rgba(0,0,0,.08); }
    /* Tabs */
    button[data-baseweb="tab"] { padding-top: 6px !important; padding-bottom: 6px !important; font-weight: 600; }
    /* Buttons */
    .stButton>button { border-radius: 10px; padding: 0.4rem 0.8rem; }
    /* Hide Streamlit default footer/menu */
    #MainMenu {visibility: hidden;} footer {visibility: hidden;}
    </style>
    """
    st.markdown(css, unsafe_allow_html=True)

def page_header(title: str, subtitle: str | None = None) -> None:
    st.markdown(f"<div class='card'><h3>{title}</h3>" + (f"<div class='sidebar-subtle'>{subtitle}</div>" if subtitle else "") + "</div>", unsafe_allow_html=True)

# ---------- nav + main ----------
def init_session() -> None:
    if "initialized" not in st.session_state:
        st.session_state.initialized = True


def nav() -> str:
    """Main sidebar navigation with smart defaults but no disappearing sidebar.

    We always render the sidebar + "Go to" selectbox. Internal flags only
    control the *default* selected page, never whether the sidebar shows.
    """
    # Decide which page should be preselected
    default_page = None

    # One-shot "force" flag (used when creating / ingesting RFPs, etc.)
    if st.session_state.pop('_force_rfp_analyzer', False):
        default_page = 'RFP Analyzer'

    # Keep user on RFP Analyzer while working with inline / new RFP uploads
    if st.session_state.get('op_inline_files'):
        default_page = 'RFP Analyzer'
    if st.session_state.get('op_new_files'):
        default_page = 'RFP Analyzer'
    if st.session_state.get('onepage_uploads'):
        default_page = 'RFP Analyzer'

    # Auto-jump when a SAM notice was pushed
    if st.session_state.pop('rfp_selected_notice', None):
        default_page = 'RFP Analyzer'

    # One-shot explicit nav target
    _tgt = st.session_state.pop("nav_target", None)
    if _tgt:
        default_page = _tgt

    # Sidebar chrome
    st.sidebar.title("Workspace")
    st.sidebar.caption(BUILD_LABEL)
    try:
        st.sidebar.caption(f"SHA {_file_hash()}")
    except Exception:
        pass

    # Journeys and grouped pages
    journeys = [
        ("Start here", [
            "Start here",
            "Help & Docs",
        ]),
        ("SAM Watch", [
            "SAM Watch",
            "Top Buyers",
            "Top Vendors",
        ]),
        ("RFP Analyzer", [
            "RFP Analyzer",
            "L and M Checklist",
            "File Manager",
            "Past Performance",
            "White Paper Builder",
            "RFQ Pack",
            "Fast RFQ",
        ]),
        ("Knowledge Hub", [
            "Knowledge Hub",
        ]),
        ("Deals and CRM", [
            "Deals",
            "Contacts",
            "Quote Comparison",
            "Pricing Calculator",
            "Win Probability",
        ]),
        ("Proposal Builder", [
            "Proposal Builder",
            "Capability Statement",
        ]),
        ("Outreach", [
            "Outreach",
        ]),
        ("Subcontractor Finder", [
            "Subcontractor Finder",
        ]),
        ("Settings and Admin", [
            "Backup & Data",
            "My Jobs",
            "Chat Assistant",
        ]),
    ]

    # Flat page list for defaults / compatibility
    all_pages = [p for _, plist in journeys for p in plist]
    if default_page not in all_pages:
        default_page = all_pages[0]

    # Pick default journey based on default_page
    journey_index = 0
    for idx, (_label, plist) in enumerate(journeys):
        if default_page in plist:
            journey_index = idx
            break

    with st.sidebar:
        st.markdown("#### User")
        _known_users = KNOWN_USERS
        _current_default = st.session_state.get("current_user_name") or _known_users[0]
        if _current_default not in _known_users:
            _current_default = _known_users[0]
        st.selectbox(
            "Current user",
            _known_users,
            index=_known_users.index(_current_default),
            key="current_user_name",
        )

        st.markdown("#### Global search")
        st.text_input(
            "Search everything",
            key="global_search_query",
            placeholder="Search RFPs, proposals, notes",
        )

        st.markdown("#### Journeys")
        journey_labels = [j[0] for j in journeys]
        selected_journey = st.radio(
            "Choose a flow",
            journey_labels,
            index=journey_index,
            key="nav_journey",
        )

        pages = dict(journeys)[selected_journey]
        try:
            default_index = pages.index(default_page)
        except ValueError:
            default_index = 0

        choice = st.selectbox(
            "Page",
            pages,
            index=default_index,
            key="nav_page",
        )

    return choice


# --- RFP Analyzer: AI attachments panel using rfp_documents ---


def load_document_text(file_path: str, max_bytes: int = 5_000_000) -> str:
    """Load text from a stored RFP document path.

    This reuses the same PDF/Office extraction helpers used elsewhere in the app.
    Returns a best-effort UTF-8 string; empty string if nothing can be read.
    """
    import os

    try:
        if not file_path or not os.path.exists(file_path):
            return ""
        with open(file_path, "rb") as f:
            data = f.read(max_bytes)
    except Exception:
        return ""

    try:
        mime = _guess_mime_from_name(os.path.basename(file_path))
    except Exception:
        mime = None

    try:
        if mime is not None:
            pages = extract_text_pages(data, mime_type=mime)
        else:
            pages = extract_text_pages(data)
    except Exception:
        pages = []

    text = "\n\n".join(pages or [])
    if not text.strip():
        try:
            text = data.decode("utf-8", errors="ignore")
        except Exception:
            text = ""
    return text


def summarize_rfp_document(doc_text: str) -> str:
    """Summarize a single RFP attachment for capture/proposal use."""
    doc_trimmed = doc_text or ""
    if not doc_trimmed.strip():
        return "No readable text was extracted from this attachment."

    # Keep prompts reasonably sized for the model
    if len(doc_trimmed) > 20000:
        doc_trimmed = doc_trimmed[:20000]

    prompt = (
        "You are a senior federal contracting analyst supporting proposal development.\n"
        "Provide a concise summary of the attached document for the capture/proposal team.\n\n"
        "Focus on:\n"
        "- Scope of work and key tasks\n"
        "- Performance standards or quality measures\n"
        "- Key dates and submission instructions\n"
        "- Any special compliance or certification requirements\n\n"
        "Document text:\n"
        f"{doc_trimmed}\n"
    )

    try:
        messages = [
            {
                "role": "system",
                "content": "You are a senior federal contracting officer and proposal analyst.",
            },
            {
                "role": "user",
                "content": prompt,
            },
        ]
        return _chat_plus_call_openai(messages, temperature=0.2, max_output_tokens=800)
    except Exception as e:
        return f"Summary failed: {e}"


def ask_rfp_doc_question(doc_text: str, question: str) -> str:
    """Answer a user question against a single RFP attachment's text."""
    q_clean = (question or "").strip()
    if not q_clean:
        return "Enter a question first."

    doc_trimmed = doc_text or ""
    if not doc_trimmed.strip():
        return "No readable text was extracted from this attachment."

    if len(doc_trimmed) > 20000:
        doc_trimmed = doc_trimmed[:20000]

    prompt = (
        "You are a federal contracting specialist. Use only the document text to answer.\n"
        "If the answer is not clearly stated, say you cannot find it in the document.\n\n"
        f"Question:\n{q_clean}\n\n"
        "Document text:\n"
        f"{doc_trimmed}\n"
    )

    try:
        messages = [
            {
                "role": "system",
                "content": "You are a careful federal contracting specialist who cites only the provided document.",
            },
            {
                "role": "user",
                "content": prompt,
            },
        ]
        return _chat_plus_call_openai(messages, temperature=0.2, max_output_tokens=800)
    except Exception as e:
        return f"Question answering failed: {e}"


def render_ai_attachments_panel(conn, rfp_id: int) -> None:
    """Streamlit panel to browse and analyze rfp_documents records for a given RFP."""
    import streamlit as st
    from contextlib import closing as _closing

    if not rfp_id:
        return

    st.subheader("AI attachments")

    try:
        with _closing(conn.cursor()) as cur:
            cur.execute(
                """
                SELECT id, file_name, stored_path, status
                  FROM rfp_documents
                 WHERE rfp_id = ?
                 ORDER BY created_at;
                """,
                (int(rfp_id),),
            )
            rows = cur.fetchall()
    except Exception as e:
        st.warning(f"Could not load RFP attachment metadata: {e}")
        return

    if not rows:
        st.info("No attachments linked yet for this RFP.")
        return

    for doc_id, fname, path, status in rows:
        label = str(fname or "(unnamed attachment)")
        with st.expander(label, expanded=False):
            st.text(f"Status: {status or 'unknown'}")
            if not path:
                st.caption("This attachment has not been downloaded or stored locally yet.")
                continue

            if st.button("Summarize attachment", key=f"rfpdoc_sum_{doc_id}"):
                text = load_document_text(path)
                if not text.strip():
                    st.warning("No readable text could be extracted from this attachment.")
                else:
                    summary = summarize_rfp_document(text)
                    st.write(summary)

            user_q = st.text_input("Your question", key=f"rfpdoc_q_{doc_id}", placeholder="Ask a question about this attachment")
            if user_q:
                text = load_document_text(path)
                if not text.strip():
                    st.warning("No readable text could be extracted from this attachment.")
                else:
                    answer = ask_rfp_doc_question(text, user_q)
                    st.write(answer)



def _rfp_load_for_scope(conn, owner_scope: str | None):
    """
    Return a DataFrame of RFPs filtered by tenant and optional owner scope.

    If owner_scope is a non-empty string, rows must either be owned by that
    logical user (rfps.owner_user) or be marked visible to all users
    (visibility = 'public'). Falls back to an unfiltered list if newer
    columns are missing so older databases still work.
    """
    import pandas as _pd

    # Base query
    sql = "SELECT id, title FROM rfps"
    clauses = []
    params: list = []

    # Tenant filter when tenant_id is available
    has_tenant = False
    try:
        with conn:
            conn.execute("SELECT tenant_id FROM rfps LIMIT 1;")
        has_tenant = True
    except Exception:
        has_tenant = False
    if has_tenant:
        clauses.append("tenant_id=(SELECT ctid FROM current_tenant WHERE id=1)")

    # Owner / visibility filter when columns are available
    owner_name = owner_scope or ""
    has_owner_visibility = False
    if owner_name:
        try:
            with conn:
                conn.execute("SELECT owner_user, visibility FROM rfps LIMIT 1;")
            has_owner_visibility = True
        except Exception:
            has_owner_visibility = False
    if owner_name and has_owner_visibility:
        clauses.append("(owner_user = ? OR visibility = 'public')")
        params.append(owner_name)

    if clauses:
        sql += " WHERE " + " AND ".join(clauses)
    sql += " ORDER BY id DESC;"

    try:
        return _pd.__p_read_sql_query(sql, conn, params=tuple(params))
    except Exception:
        # Last-resort fallback for older schemas
        try:
            return _pd.__p_read_sql_query("SELECT id, title FROM rfps ORDER BY id DESC;", conn, params=())
        except Exception:
            return _pd.DataFrame(columns=["id", "title"])

def run_rfp_analyzer(conn) -> None:
    import pandas as _pd
    import streamlit as st

    from contextlib import closing as _closing

    # Scope: per-user vs all users
    owner_scope = None
    try:
        _current_user = get_current_user_name()
    except Exception:
        _current_user = None
    if _current_user:
        _scope_choice = st.radio(
            "RFP scope",
            ["My RFPs", "All users"],
            index=0,
            horizontal=True,
            key="rfp_scope_main",
        )
        if _scope_choice == "My RFPs":
            owner_scope = _current_user

    # Load RFP list (tenant- and optional owner-scoped)
    df_rfps = _rfp_load_for_scope(conn, owner_scope)


    if df_rfps is None or df_rfps.empty:
        st.title("RFP Analyzer — One‑Page")
        st.markdown("**Steps:** 1) Select or create RFP  ›  2) Upload RFP files  ›  3) Run analysis  ›  4) Jump into Proposal Builder.")
        st.caption("Use this page to create RFP records, ingest files, and build the RFP context the rest of the app uses (checklists, CLINs, key dates, POCs, meta, and proposal builder).")
        ui_info("No RFPs yet. Create one below to use the One‑Page Analyzer.")
        render_empty_state("No RFPs yet", "Start by creating an RFP record and ingesting the files you want to analyze.")
        ctx0 = st.session_state.get("rfp_selected_notice") or {}
        t0 = st.text_input("RFP Title", value=str(ctx0.get("Title") or ""), key="op_new_title")
        s0 = st.text_input("Solicitation #", value=str(ctx0.get("Solicitation") or ""), key="op_new_sol")
        u0 = st.text_input("SAM URL", value=str(ctx0.get("SAM Link") or ""), key="op_new_sam", placeholder="https://sam.gov/")
        ups = st.file_uploader("Upload RFP files (PDF/DOCX/TXT/XLSX/ZIP)", type=["pdf","docx","txt","xlsx","zip"],
                               accept_multiple_files=True, key="op_new_files")
        # Keep on RFP Analyzer after selecting files
        if 'op_new_files' in st.session_state and st.session_state.get('op_new_files'):
            st.session_state['_force_rfp_analyzer'] = True
            st.session_state['nav_target'] = 'RFP Analyzer'
        st.markdown("### Primary action: Create RFP record and ingest")
        if st.button("Create RFP record and ingest", key="op_create_ingest"):
            try:
                new_id, saved = svc_create_rfp_and_ingest(conn, t0, s0, u0, ups)
                ui_success("RFP created and files ingested.", f"ID #{new_id} — {saved} file(s) saved.")
                st.session_state['current_rfp_id'] = int(new_id)
                st.rerun()
            except Exception as e:
                logger.exception("Create RFP and ingest failed")
                ui_error("Could not create the RFP and ingest files.", str(e))

        return

    # RFP workflow steps
    st.markdown("**Steps:** 1) Select or create RFP  ›  2) Upload RFP files  ›  3) Run analysis  ›  4) Jump into Proposal Builder.")

    # New RFP inline form (always available)
    with st.expander("➕ Start a new RFP", expanded=False):
        ctx0 = st.session_state.get("rfp_selected_notice") or {}
        t0 = st.text_input("RFP Title", value=str(ctx0.get("Title") or ""), key="op_inline_title_main")
        s0 = st.text_input("Solicitation #", value=str(ctx0.get("Solicitation") or ""), key="op_inline_sol_main")
        u0 = st.text_input("SAM URL", value=str(ctx0.get("SAM Link") or ""), key="op_inline_sam_main", placeholder="https://sam.gov/")
        ups = st.file_uploader("Upload RFP files (PDF/DOCX/TXT/XLSX/ZIP)", type=["pdf","docx","txt","xlsx","zip"],
                               accept_multiple_files=True, key="op_inline_files_main")
        # Keep on RFP Analyzer after selecting files
        if 'op_inline_files_main' in st.session_state and st.session_state.get('op_inline_files_main'):
            st.session_state['_force_rfp_analyzer'] = True
            st.session_state['nav_target'] = 'RFP Analyzer'
        st.markdown("### Primary action: Create RFP record and ingest")
        if st.button("Create RFP record and ingest", key="op_inline_create_main"):
            try:
                new_id, saved = svc_create_rfp_and_ingest(conn, t0, s0, u0, ups)
                ui_success("RFP created and files ingested.", f"ID #{new_id} — {saved} file(s) saved.")
                st.session_state['current_rfp_id'] = int(new_id)
                st.rerun()
            except Exception as e:
                logger.exception("Create RFP and ingest failed")
                ui_error("Could not create the RFP and ingest files.", str(e))

                st.error(f"Create & ingest failed: {e}")


    # Selection
    current_id = st.session_state.get("current_rfp_id")
    try:
        default_idx = int(df_rfps["id"].tolist().index(int(current_id))) if current_id in df_rfps["id"].tolist() else 0
    except Exception:
        default_idx = 0
    rid = st.selectbox("RFP (One‑Page Analyzer)", options=df_rfps["id"].tolist(), index=default_idx,
                       format_func=lambda i: f"#{i} — " + df_rfps.loc[df_rfps['id']==i,'title'].values[0], key="onepage_rfp_default")
    # Keep the session in sync with the current dropdown choice
    try:
        st.session_state["current_rfp_id"] = int(rid)
    except Exception:
        st.session_state["current_rfp_id"] = rid

    # Suggested subs for this RFP
    try:
        rid_int = int(_ensure_selected_rfp_id(conn))
    except Exception:
        rid_int = None
    if rid_int:
        # Resolve NAICS from meta or linked notice
        req_naics = _p3_rfp_meta_get(conn, rid_int, "naics", "")
        notice_id = _p3_rfp_meta_get(conn, rid_int, "notice_id", "")
        from contextlib import closing as _closing
        if not notice_id:
            try:
                with _closing(conn.cursor()) as _cur:
                    _cur.execute("SELECT notice_id FROM rfps WHERE id=?;", (int(rid_int),))
                    _row = _cur.fetchone()
                if _row and _row[0]:
                    notice_id = str(_row[0] or "")
            except Exception:
                notice_id = ""
        if (not req_naics) and notice_id:
            try:
                with _closing(conn.cursor()) as _cur2:
                    _cur2.execute("SELECT naics FROM notices WHERE notice_id=?;", (notice_id,))
                    _n = _cur2.fetchone()
                if _n and _n[0]:
                    req_naics = str(_n[0] or "")
            except Exception:
                pass
        work_tags = _p3_rfp_meta_get(conn, rid_int, "work_type_tags", "")
        with st.expander("Suggested subs for this RFP", expanded=False):
            st.write(f"Required NAICS: {req_naics or 'n/a'}")
            tags_input = st.text_input(
                "Work type tags (comma-separated)",
                value=work_tags or "",
                key=f"rfp_worktypes_{rid_int}",
                help="Examples: janitorial, grounds, HVAC, snow removal",
            )
            if st.button("Save RFP tags", key=f"rfp_worktypes_save_{rid_int}"):
                try:
                    _p3_rfp_meta_set(conn, rid_int, "work_type_tags", tags_input.strip())
                    st.success("Updated RFP work type tags.")
                    work_tags = tags_input
                except Exception as _e:
                    st.error(f"Failed to update tags: {_e}")
            df_suggest_rfp = _get_suggested_subs(conn, req_naics, work_tags)
            if (not req_naics) and (not (work_tags or '').strip()):
                st.caption("Add NAICS or work type tags for this RFP to see suggested subcontractors.")
            elif df_suggest_rfp is None or df_suggest_rfp.empty:
                st.caption("No matching vendors yet. Seed vendors with matching capabilities in Subcontractor Finder.")
            else:
                import pandas as _pd
                df_view_r = df_suggest_rfp.copy()
                try:
                    df_view_r["location"] = (
                        df_view_r["city"].fillna("").str.strip()
                        + _pd.Series([", "] * len(df_view_r))
                        + df_view_r["state"].fillna("").str.strip()
                    )
                except Exception:
                    df_view_r["location"] = df_view_r.get("city", "") + ", " + df_view_r.get("state", "")
                df_view_r = df_view_r.rename(
                    columns={"vendor_score": "score", "naics_match": "NAICS match", "tag_match_count": "tag matches"}
                )
                cols_r = [c for c in ["name", "location", "score", "NAICS match", "tag matches", "capability_tags"] if c in df_view_r.columns]
                if cols_r:
                    st.dataframe(df_view_r[cols_r], use_container_width=True)
    # If suggested subs panel fails, errors will surface in the Streamlit logs.
    # Danger zone: delete current RFP
    with st.expander("Danger zone: Delete this RFP", expanded=False):
        st.write("This will permanently remove this RFP and its related analyzer records.")
        if st.button("Delete this RFP", type="secondary", key="p3_delete_rfp"):
            try:
                _delete_rfp_everywhere(conn, int(rid))
                # Clear selection so we don't point at a deleted id
                st.session_state.pop("current_rfp_id", None)
                # Refresh the page; nav() will keep you on RFP Analyzer because the sidebar selection doesn't change
                st.success(f"Deleted RFP #{int(rid)}.")
                st.rerun()
            except Exception as e:
                logger.exception("Delete RFP failed")
                ui_error("Could not delete this RFP.", str(e))

    # Controls
    c1, c2, c3 = st.columns([1,1,2])
    with c1:
        if st.button("Check for SAM updates ▶", key="p3_check_sam"):
            with st.spinner("Checking SAM.gov for amendments and new attachments…"):
                try:
                    res = _p3_check_sam_updates(conn, int(_ensure_selected_rfp_id(conn)))
                    errs = res.get("errors") or []
                    st.success(f"Attempted: {res.get('attempts',0)} — New/updated files: {res.get('new_files',0)}")
                    if errs: st.warning("Notes/Errors:\n- " + "\n- ".join(errs))
                except Exception as e:
                    logger.exception("SAM update check failed")
                    ui_error("Could not check SAM for updates.", str(e))
    with c2:
        if st.button("Ensure Deal & Contacts", key="p3_crm_wire"):
            try:
                _p3_ensure_deal_and_contacts(conn, int(_ensure_selected_rfp_id(conn)))
                st.success("CRM wiring complete (best-effort).")
            except Exception as e:
                logger.exception("CRM wiring failed")
                ui_error("Could not wire this RFP into the CRM.", str(e))
    with c3:
        
        if st.button("Ingest & Analyze ▶", key="p3_ingest_analyze"):
            # Enqueue RFP ingest/analyze; if no separate worker is running,
            # fall back to running the pipeline inline so the Analyzer still updates.
            try:
                ensure_jobs_schema(conn)
            except Exception:
                pass

            try:
                try:
                    _user_name = get_current_user_name()
                except Exception:
                    _user_name = ""
                payload = {
                    "scope": "rfp_ingest_analyze",
                    "rfp_id": int(_ensure_selected_rfp_id(conn)),
                }
                job_id = jobs_enqueue(
                    conn,
                    job_type="rfp_ingest_analyze",
                    payload=payload,
                    created_by=_user_name or None,
                )
                # Remember the last job id for this session so we can show its status elsewhere.
                try:
                    st.session_state["rfp_ingest_analyze_last_job_id"] = job_id
                except Exception:
                    pass

                # Inline fallback so the RFP actually ingests even if no background worker is running.
                _rfp_id = 0
                try:
                    _rfp_id = int(_ensure_selected_rfp_id(conn))
                except Exception:
                    _rfp_id = 0

                if _rfp_id:
                    try:
                        jobs_update_status(
                            conn,
                            job_id,
                            status="running",
                            mark_started=True,
                            progress=0.0,
                        )
                    except Exception:
                        pass
                    try:
                        with st.spinner("Ingesting and analyzing RFP…"):
                            _one_click_analyze(conn, _rfp_id, None)
                        try:
                            jobs_update_status(
                                conn,
                                job_id,
                                status="done",
                                mark_finished=True,
                                progress=1.0,
                                result={"rfp_id": _rfp_id},
                            )
                        except Exception:
                            pass
                        st.success("RFP ingest & analyze completed.")
                    except Exception as _exc:
                        try:
                            jobs_update_status(
                                conn,
                                job_id,
                                status="failed",
                                error_message=str(_exc),
                                mark_finished=True,
                            )
                        except Exception:
                            pass
                        ui_error("RFP ingest & analyze failed.", str(_exc))
                else:
                    st.warning("No RFP selected to ingest/analyze.")
            except Exception as e:
                logger.exception("Failed to enqueue RFP ingest/analyze job")
                ui_error("Could not enqueue the RFP ingest/analyze job.", str(e))

            except Exception as e:
                logger.exception("Failed to enqueue RFP ingest/analyze job")
                ui_error("Could not enqueue the RFP ingest/analyze job.", str(e))



    # Add files

    # AI attachments panel using rfp_documents linked to this RFP
    try:
        _rfp_id_for_ai = int(_ensure_selected_rfp_id(conn))
    except Exception:
        _rfp_id_for_ai = 0
    if _rfp_id_for_ai:
        render_ai_attachments_panel(conn, _rfp_id_for_ai)

    with st.container(border=True):
        st.subheader("➕ Add files to this RFP")
        uploads = st.file_uploader(
            "Upload RFP documents (PDF/DOCX/TXT/ZIP/XLSX)",
            type=["pdf","doc","docx","txt","rtf","zip","xlsx","xls"],
            accept_multiple_files=True,
            key="onepage_uploads_alt",
        )
        save_clicked = st.button("Save files to this RFP", key="onepage_uploads_alt_save")
        if uploads and save_clicked:
            saved = 0
            for f in uploads:
                try:
                    b = f.getbuffer().tobytes() if hasattr(f, "getbuffer") else f.read()
                    save_rfp_file_db(
                        conn,
                        int(_ensure_selected_rfp_id(conn)),
                        getattr(f, "name", "upload"),
                        b,
                    )
                    saved += 1
                except Exception:
                    pass
            if saved > 0:
                st.success(f"Saved {saved} file(s).")
                try:
                    y1_index_rfp(conn, int(_ensure_selected_rfp_id(conn)), rebuild=False)
                except Exception:
                    pass

    # Build pages and render One-Page
    try:
        df_files = _pd.__p_read_sql_query(
            "SELECT filename, mime, bytes, path FROM rfp_files WHERE rfp_id=? ORDER BY id;",
            conn,
            params=(int(_ensure_selected_rfp_id(conn)),),
        )
    except Exception:
        df_files = None

    # AI attachments panel: summarize each document and ask questions
    with st.expander("AI attachments: summarize each document and ask questions", expanded=False):
        try:
            if df_files is None or getattr(df_files, "empty", True):
                st.caption("No files are linked to this RFP yet. Upload or fetch attachments first.")
            else:
                try:
                    file_options = [str(x or "") for x in df_files["filename"].tolist()]
                except Exception:
                    file_options = []
                if not file_options:
                    st.caption("No filenames are available for this RFP yet.")
                else:
                    try:
                        rid_int = int(_ensure_selected_rfp_id(conn))
                    except Exception:
                        rid_int = 0

                    sel_file = st.selectbox(
                        "Attachment",
                        options=file_options,
                        index=0,
                        key=f"rfp_att_file_{rid_int}",
                        help="Pick a single attachment to summarize or ask questions about.",
                    )

                    st.caption("Use AI to get a quick summary of this attachment or answers about its content.")

                    # Helper to load context for a given attachment
                    def _load_attachment_ctx(rid: int, fname: str) -> str:
                        ctx_local = ""
                        if not fname:
                            return ""
                        # Prefer chunked index if available
                        try:
                            df_ctx = _pd.read_sql_query(
                                """
                                SELECT text
                                  FROM rfp_chunks
                                 WHERE rfp_id = ? AND file_name = ?
                                 ORDER BY page, chunk_idx
                                 LIMIT 80;
                                """,
                                conn,
                                params=(int(rid), fname),
                            )
                        except Exception:
                            df_ctx = _pd.DataFrame()
                        if df_ctx is not None and not getattr(df_ctx, "empty", True):
                            try:
                                ctx_local = "\n".join(df_ctx["text"].fillna("").tolist())[:12000]
                            except Exception:
                                ctx_local = ""

                        # Fallback to rfp_files.text if needed
                        if not ctx_local:
                            try:
                                df_ctx2 = _pd.read_sql_query(
                                    "SELECT COALESCE(text,'') AS text FROM rfp_files WHERE rfp_id = ? AND filename = ? LIMIT 1;",
                                    conn,
                                    params=(int(rid), fname),
                                )
                            except Exception:
                                df_ctx2 = _pd.DataFrame()
                            if df_ctx2 is not None and not getattr(df_ctx2, "empty", True):
                                try:
                                    ctx_local = str(df_ctx2["text"].iloc[0] or "")[:12000]
                                except Exception:
                                    ctx_local = ctx_local
                        return ctx_local

                    # Summary button
                    if st.button("Summarize this attachment", key=f"rfp_att_summarize_{rid_int}"):
                        ctx = _load_attachment_ctx(rid_int, sel_file)
                        if not ctx:
                            st.warning("No extracted text found for this attachment yet. Try running One-Click Analyze first.")
                        else:
                            ph = st.empty()
                            acc: list[str] = []
                            prompt = (
                                f"Attachment filename: {sel_file}\n\n"
                                f"Source text (truncated):\n{ctx}\n\n"
                                "Summarize the key requirements, deliverables, performance standards, and any dates or quantities. "
                                "Use short bullet points suitable for a proposal writer."
                            )
                            try:
                                for tok in ask_ai(
                                    [
                                        {
                                            "role": "system",
                                            "content": (
                                                "You are a federal contracting officer reviewing a single RFP attachment. "
                                                "Write concise, factual bullets with no marketing language or fluff."
                                            ),
                                        },
                                        {"role": "user", "content": prompt},
                                    ]
                                ):
                                    acc.append(tok)
                                    ph.markdown("".join(acc))
                            except Exception as e:
                                st.error(f"AI summary failed: {e}")

                    # Q&A on attachment
                    q_key = f"rfp_att_q_{rid_int}"
                    q = st.text_input(
                        "Question about this attachment",
                        key=q_key,
                        placeholder="Example: What inspection and acceptance criteria does this file specify?",
                    )
                    if st.button("Ask about this attachment", key=f"rfp_att_ask_{rid_int}"):
                        q_clean = (q or "").strip()
                        if not q_clean:
                            st.warning("Enter a question first.")
                        else:
                            ctx = _load_attachment_ctx(rid_int, sel_file)
                            if not ctx:
                                st.warning("No extracted text found for this attachment yet. Try running One-Click Analyze first.")
                            else:
                                ph = st.empty()
                                acc: list[str] = []
                                prompt = (
                                    f"Attachment filename: {sel_file}\n\n"
                                    f"Relevant text (truncated):\n{ctx}\n\n"
                                    f"Question: {q_clean}\n\n"
                                    "Answer as a federal contracting officer, citing only information that appears in this text. "
                                    "Use clear bullets or short paragraphs."
                                )
                                try:
                                    for tok in ask_ai(
                                        [
                                            {
                                                "role": "system",
                                                "content": (
                                                    "You are a federal contracting officer answering questions about one RFP attachment. "
                                                    "Be precise and avoid speculation or unsupported assumptions."
                                                ),
                                            },
                                            {"role": "user", "content": prompt},
                                        ]
                                    ):
                                        acc.append(tok)
                                        ph.markdown("".join(acc))
                                except Exception as e:
                                    st.error(f"AI answer failed: {e}")
        except Exception as e:
            st.error(f"AI attachments panel failed: {e}")
    pages: list[dict] = []
    if df_files is not None and not df_files.empty:
        for _, r in df_files.iterrows():
            b = None
            try:
                b = r.get("bytes")
            except Exception:
                b = None
            if not b:
                # Fallback: if only a file path is stored, try to read from disk.
                p = None
                try:
                    p = r.get("path")
                except Exception:
                    p = None
                if p:
                    try:
                        with open(p, "rb") as fh:
                            b = fh.read()
                    except Exception:
                        b = None
            if not b:
                continue
            mime = ""
            try:
                mime = r.get("mime") or ""
            except Exception:
                mime = ""
            try:
                texts = extract_text_pages(b, mime) or []
            except Exception:
                texts = []
            for i, t in enumerate(texts[:100], start=1):
                pages.append(
                    {
                        "file": (r.get("filename") or "") if hasattr(r, "get") else "",
                        "page": i,
                        "text": t or "",
                    }
                )

    if not pages:
        # Be explicit about why there are no pages.
        if df_files is None or df_files.empty:
            st.warning("No files are linked to this RFP yet. Upload files and try again.")
        else:
            st.warning("No readable pages found in linked files for this RFP.")
        return

    # Delegate to One‑Page renderer
    if 'run_rfp_analyzer_onepage' in globals() and callable(run_rfp_analyzer_onepage):
        run_rfp_analyzer_onepage(pages)
    else:
        st.info("One-Page Analyzer module is unavailable.")
def render_workspace_switcher(conn: "sqlite3.Connection") -> None:
    with st.sidebar.expander("Workspace", expanded=True):
        try:
            df_tenants = pd.read_sql_query("SELECT id, name FROM tenants ORDER BY id;", conn, params=())
        except Exception:
            df_tenants = pd.DataFrame(columns=["id","name"])
        try:
            cur_tid = int(pd.read_sql_query("SELECT ctid FROM current_tenant WHERE id=1;", conn, params=()).iloc[0]["ctid"])
        except Exception:
            cur_tid = 1
        opt = st.selectbox("Organization", options=(df_tenants["id"].astype(int).tolist() if not df_tenants.empty else [1]),
                           format_func=lambda i: (df_tenants.loc[df_tenants["id"]==i,"name"].values[0] if not df_tenants.empty else "Default"),
                           key="tenant_sel")
        if st.button("Switch", key="tenant_switch"):
            with closing(conn.cursor()) as cur:
                cur.execute("UPDATE current_tenant SET ctid=? WHERE id=1;", (int(opt),))
                conn.commit()
            st.session_state['tenant_id'] = int(opt)
            st.success("Workspace switched"); st.rerun()

        st.divider()
        new_name = st.text_input("New workspace name", key="tenant_new_name")
        if st.button("Create workspace", key="tenant_create"):
            if new_name.strip():
                with closing(conn.cursor()) as cur:
                    cur.execute("INSERT OR IGNORE INTO tenants(name, created_at) VALUES(?, datetime('now'));", (new_name.strip(),))
                    conn.commit()
                st.success("Workspace created"); st.rerun()
            else:
                st.warning("Enter a name")

# --- Phase U helper: namespaced keys for Streamlit ---
def ns(scope: str, key: str) -> str:
    """Generate stable, unique Streamlit widget keys."""
    return f"{scope}::{key}"
# === S1 Subcontractor Finder: Google Places ===




def _global_search_nav_to(conn: "sqlite3.Connection", ref_type: str, ref_id: str | int) -> None:
    """
    Navigate from a global search result into the appropriate page and record.
    """
    import streamlit as st
    from contextlib import closing as _closing

    try:
        rid = int(ref_id)
    except Exception:
        rid = ref_id

    if ref_type == "rfp":
        st.session_state["current_rfp_id"] = rid
        st.session_state["nav_target"] = "RFP Analyzer"
        try:
            router("RFP Analyzer", conn)
            st.stop()
        except Exception:
            st.rerun()
    elif ref_type == "proposal":
        # Proposal Builder is record-aware via proposal id session key.
        st.session_state["x7_current_proposal_id"] = rid
        st.session_state["nav_target"] = "Proposal Builder"
        try:
            router("Proposal Builder", conn)
            st.stop()
        except Exception:
            st.rerun()
    elif ref_type in ("deal_note", "contact_note", "note"):
        deal_id = None
        contact_id = None
        try:
            with _closing(conn.cursor()) as cur:
                cur.execute(
                    "SELECT deal_id, contact_id FROM activities WHERE id=?;",
                    (rid,),
                )
                row = cur.fetchone()
            if row:
                deal_id, contact_id = row[0], row[1]
        except Exception:
            deal_id = None
            contact_id = None

        if deal_id:
            st.session_state["current_deal_id"] = int(deal_id)
            st.session_state["nav_target"] = "Deals"
            try:
                router("Deals", conn)
                st.stop()
            except Exception:
                st.rerun()
        elif contact_id:
            st.session_state["current_contact_id"] = int(contact_id)
            st.session_state["nav_target"] = "Contacts"
            try:
                router("Contacts", conn)
                st.stop()
            except Exception:
                st.rerun()
        else:
            # Fallback to Deals list if we cannot resolve a specific record.
            st.session_state["nav_target"] = "Deals"
            try:
                router("Deals", conn)
                st.stop()
            except Exception:
                st.rerun()


def _global_search_panel(conn: "sqlite3.Connection") -> None:
    """
    Render global search results using the FTS index.
    """
    import streamlit as st
    import pandas as _pd
    from contextlib import closing as _closing

    query = (st.session_state.get("global_search_query") or "").strip()
    if not query:
        return

    try:
        _ensure_fts_docs(conn)
    except Exception:
        # FTS is optional; if not available just skip the panel.
        return

    st.markdown("### Global search results")
    st.caption("Matches across RFPs, proposals, contact notes, and deal notes indexed into fts_docs.")

    allowed_types = ["rfp", "proposal", "contact_note", "deal_note", "note"]
    clauses = ["fts_docs MATCH ?"]
    params: list[object] = [query]
    placeholders = ",".join("?" * len(allowed_types))
    clauses.append(f"ref_type IN ({placeholders})")
    params.extend(allowed_types)
    where_sql = " AND ".join(clauses)

    rows: list[tuple] = []
    error_msg = ""
    try:
        with _closing(conn.cursor()) as cur:
            sql = f"""
                SELECT
                    ref_type,
                    ref_id,
                    title,
                    snippet(fts_docs, 4, '…', '…', ' … ', 18) AS snippet
                FROM fts_docs
                WHERE {where_sql}
                LIMIT 75;
            """
            cur.execute(sql, params)
            rows = cur.fetchall()
    except Exception as e:
        error_msg = str(e)

    if error_msg:
        st.error("Global search failed. The full-text index may not be available.")
        st.text(error_msg)
        return

    if not rows:
        st.info("No results found for your query.")
        return

    df = _pd.DataFrame(rows, columns=["ref_type", "ref_id", "title", "snippet"])
    groups: dict[str, _pd.DataFrame] = {}
    for rtype in ["rfp", "proposal", "deal_note", "contact_note", "note"]:
        sub = df[df["ref_type"] == rtype]
        if not sub.empty:
            groups[rtype] = sub

    label_map = {
        "rfp": "RFPs",
        "proposal": "Proposals",
        "deal_note": "Deal notes",
        "contact_note": "Contact notes",
        "note": "Other notes",
    }

    for rtype in ["rfp", "proposal", "deal_note", "contact_note", "note"]:
        sub = groups.get(rtype)
        if sub is None or sub.empty:
            continue
        st.markdown(f"#### {label_map.get(rtype, rtype.title())}")
        for _, row in sub.iterrows():
            col1, col2 = st.columns([4, 1])
            title = row.get("title") or ""
            snippet = row.get("snippet") or ""
            ref_id = row.get("ref_id")
            with col1:
                st.markdown(f"**{title or '(untitled)'}**")
                if snippet:
                    st.caption(snippet)
            with col2:
                if st.button("Open", key=f"gs_open_{rtype}_{ref_id}"):
                    _global_search_nav_to(conn, rtype, ref_id)
                    st.stop()
        st.divider()

def run_knowledge_hub(conn) -> None:
    import streamlit as st
    import pandas as _pd
    from contextlib import closing as _closing

    st.title("Knowledge Hub — Full‑Text Search")
    st.caption(
        "Use this page to search across RFPs, proposals, and large notes that have been "
        "indexed into the full‑text engine (fts_docs)."
    )

    q = st.text_input(
        "Search query",
        key="kh_query",
        placeholder="security plan, past performance, clause, requirement, etc.",
    )

    c1, c2 = st.columns([2, 1])
    with c1:
        type_filter = st.multiselect(
            "Document types (optional filter)",
            options=["rfp", "proposal", "note"],
            default=[],
            help="Filter by how the document was tagged when it was indexed (ref_type column).",
        )
    with c2:
        limit = st.number_input(
            "Max results",
            min_value=10,
            max_value=500,
            value=50,
            step=10,
        )

    if not q:
        st.info(
            "Enter a few keywords above to search across all indexed documents. "
            "Advanced: you can use AND/OR, phrase search with quotes, and wildcard * where supported by SQLite FTS5."
        )
        return

    clauses = ["fts_docs MATCH ?"]
    params: list[object] = [q.strip()]

    if type_filter:
        placeholders = ",".join(["?"] * len(type_filter))
        clauses.append(f"ref_type IN ({placeholders})")
        params.extend(type_filter)

    where_sql = " AND ".join(clauses)

    rows = []
    error_msg = None
    try:
        with _closing(conn.cursor()) as cur:
            sql = f"""
                SELECT
                    rowid AS doc_rowid,
                    ref_type,
                    ref_id,
                    title,
                    snippet(fts_docs, 4, '…', '…', ' … ', 20) AS snippet,
                    bm25(fts_docs) AS rank
                FROM fts_docs
                WHERE {where_sql}
                ORDER BY rank ASC
                LIMIT ?
            """
            cur.execute(sql, (*params, int(limit)))
            rows = cur.fetchall()
    except Exception:
        try:
            with _closing(conn.cursor()) as cur:
                sql = f"""
                    SELECT
                        rowid AS doc_rowid,
                        ref_type,
                        ref_id,
                        title,
                        snippet(fts_docs, 4, '…', '…', ' … ', 20) AS snippet
                    FROM fts_docs
                    WHERE {where_sql}
                    LIMIT ?
                """
                cur.execute(sql, (*params, int(limit)))
                rows = cur.fetchall()
        except Exception as e2:
            error_msg = str(e2)

    if error_msg:
        st.error(
            "Search failed. The full‑text index may not be available in this SQLite build "
            "or the fts_docs table is missing."
        )
        st.text(error_msg)
        return

    if not rows:
        st.warning("No documents matched your search. Try different keywords or remove filters.")
        return

    df = _pd.DataFrame(
        rows,
        columns=["doc_rowid", "ref_type", "ref_id", "title", "snippet"][: len(rows[0])],
    )

    # Normalize columns in case bm25 is not available
    if "doc_rowid" not in df.columns:
        df["doc_rowid"] = None
    for col in ["ref_type", "ref_id", "title", "snippet"]:
        if col not in df.columns:
            df[col] = None

    df_view = df[["ref_type", "ref_id", "title", "snippet"]]

    st.markdown("### Results")
    st.dataframe(
        df_view,
        use_container_width=True,
        height=min(700, 100 + 30 * len(df_view)),
    )

    st.caption(
        "Tip: use this page as your internal knowledge hub. As more RFPs, proposals, and notes "
        "are indexed into fts_docs, they will automatically become searchable here."
    )

def router(page: str, conn: "sqlite3.Connection") -> None:
    """Dynamic router. Resolves run_<snake_case(page)> and executes safely."""
    import re as _re
    name = "run_" + _re.sub(r"[^a-z0-9]+", "_", (page or "").lower()).strip("_")
    fn = globals().get(name)
    # explicit fallbacks for known variant names
    if not callable(fn):
        alt = {
            "L and M Checklist": ["run_l_and_m_checklist", "run_lm_checklist"],
            "Backup & Data": ["run_backup_data", "run_backup_and_data"],
        }.get((page or "").strip(), [])
        for a in alt:
            fn = globals().get(a)
            if callable(fn):
                break
    if not callable(fn):
        import streamlit as _st
        _st.warning(f"No handler for page '{page}' resolved as {name}.")
        return
    _safe_route_call(fn, conn)
    # Hooks
    if (page or "").strip() == "Proposal Builder":
        _safe_route_call(globals().get("pb_phase_v_section_library", lambda _c: None), conn)

def run_start_here(conn: "sqlite3.Connection") -> None:
    """Guided onboarding for new users."""
    import streamlit as st

    render_page_title(
        "Start here",
        "Guided setup so a new user can get value from the app without training.",
    )

    # High level explanation of the workspace
    render_card(
        "What this workspace does",
        (
            "ELA GovCon Workspace helps you 1) watch SAM for new opportunities, "
            "2) build a capture and deals pipeline, 3) analyze RFPs and compliance, "
            "and 4) generate proposals and outreach without leaving the app."
        ),
        "Most work follows one of four journeys: Discover, Capture, Proposals, and Outreach.",
    )

    st.markdown("### The main journeys")
    st.markdown(
        "- **Discover work (SAM Watch):** Find matching notices and save the best.\n"
        "- **Capture & CRM (Deals and CRM):** Track pipeline, contacts, and pricing.\n"
        "- **RFP & Proposals:** Ingest RFPs, run checklists, and build proposals.\n"
        "- **Vendors & Outreach:** Find subcontractors and send campaigns."
    )

    st.markdown("### First-run checklist")
    st.caption(
        "Work through these in order. You can mark items done and jump directly to the right page."
    )

    steps = [
        {
            "title": "Add your company profile",
            "desc": "Fill in your organization profile so proposals, capability statements, and outreach auto-fill with the right details.",
            "target": "Capability Statement",
        },
        {
            "title": "Connect an email sender",
            "desc": "Go to Outreach and create at least one sender (for example a Gmail account) so Mail Merge & Send can deliver campaigns.",
            "target": "Outreach",
        },
        {
            "title": "Run a sample SAM Watch search",
            "desc": "Use SAM Watch with a NAICS code and state you care about, then run a smart search to pull in a few real notices.",
            "target": "SAM Watch",
        },
        {
            "title": "Push one notice into Deals",
            "desc": "From SAM Watch, pick a strong notice and use Add to Deals so it shows up in your deals pipeline.",
            "target": "Deals",
        },
        {
            "title": "Ingest one RFP into RFP Analyzer",
            "desc": "Create an RFP record, upload the main RFP PDF and key attachments, and let the analyzer build your RFP context.",
            "target": "RFP Analyzer",
        },
        {
            "title": "Generate a sample proposal draft",
            "desc": "Open Proposal Builder, select your RFP, and generate a first draft for at least one major section.",
            "target": "Proposal Builder",
        },
    ]

    completed = 0
    for idx, step in enumerate(steps):
        key = f"onboarding_step_{idx}"
        done = bool(st.session_state.get(key, False))
        cols = st.columns([0.08, 0.72, 0.2])
        with cols[0]:
            done = st.checkbox("", value=done, key=key)
        if done:
            completed += 1
        with cols[1]:
            st.markdown(f"**{step['title']}**")
            st.caption(step["desc"])
        with cols[2]:
            if st.button("Go", key=f"onboarding_go_{idx}"):
                # Set nav_target for the next run and immediately route, just like SAM Watch
                st.session_state["nav_target"] = step["target"]
                try:
                    router(step["target"], conn)
                    st.stop()
                except Exception:
                    st.rerun()

    st.caption(f"Checklist progress: {completed} of {len(steps)} steps completed this session.")

    st.markdown("### Where should I start?")
    st.write(
        "If you are brand new, complete the first three steps. After that, focus on one live opportunity "
        "and take it all the way from SAM Watch → Deals → RFP Analyzer → Proposal Builder."
    )


def run_help_docs(conn: "sqlite3.Connection") -> None:
    """In-app documentation that can also be shared with prospects."""
    import streamlit as st

    render_page_title(
        "Help & Docs",
        "Reference material for new team members and prospects.",
    )

    # Quick start guide
    sec_qs = render_section(
        "Quick start guide",
        "Use this as a one-page overview you can paste into emails or onboarding docs.",
        expanded=True,
    )
    sec_qs.markdown(
        """
1. **Set up your workspace**
   - Add your company profile under **Proposal Builder → Capability Statement**.
   - Confirm your current user name in the sidebar so activity is tracked correctly.

2. **Connect email for outreach**
   - Go to **Outreach** and add at least one sender account.
   - Save a default signature so every campaign looks consistent.

3. **Discover opportunities**
   - Use **SAM Watch** with a NAICS code, state, and smart search text.
   - Save strong notices and use **Add to Deals** to bring them into your pipeline.

4. **Capture and pricing**
   - In **Deals**, track stage, value, close date, and win probability.
   - Link contacts and basic pricing so your pipeline is always current.

5. **Analyze RFPs**
   - In **RFP Analyzer**, create an RFP record, ingest files, and review the L&M checklist.
   - Use the context tools to track CLINs, key dates, and POCs in one place.

6. **Build proposals and outreach**
   - Use **Proposal Builder** to generate cover letters, technical sections, and past performance.
   - Use **Outreach** to send tailored campaigns to agencies and subcontractors.
        """
    )

    # Data and security overview
    sec_ds = render_section(
        "Data and security overview",
        "High-level view of where your data lives in this self-hosted app.",
        expanded=False,
    )
    sec_ds.markdown(
        """
- **Data storage**
  - App data (deals, RFPs, contacts, templates, logs) is stored in a local SQLite database file.
  - File uploads such as RFP PDFs are stored on disk and referenced from the database.
- **Secrets and credentials**
  - API keys and email credentials are stored in the same database and used only to call the relevant services.
  - Rotate credentials periodically and remove access you no longer need.
- **Backups**
  - Use the **Backup & Data** page to export database snapshots regularly.
  - Keep at least one backup offline or in a separate storage location.
- **Access control**
  - Each user and tenant has separate records so activity is scoped per user and workspace.
  - Make sure each teammate has their own login rather than sharing accounts.
        """
    )

    # Capabilities and limitations
    sec_cl = render_section(
        "Current capabilities and limitations",
        "Share this with prospects so expectations are clear.",
        expanded=False,
    )
    sec_cl.markdown(
        """
**What the app does today**

- Watches SAM.gov using smart searches and saved filters.
- Tracks a deals pipeline with contacts, quotes, pricing, and win probability.
- Ingests RFPs and organizes context (files, CLINs, key dates, POCs).
- Generates proposal drafts and capability statements using your stored data.
- Sends batch outreach emails with merge tags and signatures.
- Stores all of this in a single workspace so you do not have to jump between tools.

**What the app does not do yet**

- It does not submit bids directly to SAM.gov or agency portals.
- It does not replace a human compliance review; you still own final sign-off.
- It does not provide legal, tax, or accounting advice.
- It is not a multi-tenant SaaS by default; deployments are currently per workspace instance.
        """
    )


def main() -> None:
    # Phase 1 re-init inside main
    # Bootstrap UI and sidebar
    try:
        _init_phase1_ui()
    except Exception:
        pass

    # Sidebar is critical. Try separately so it never gets skipped.
    try:
        _sidebar_brand()
    except Exception:
        with st.sidebar:
            st.markdown("### 🧭 ELA GovCon Suite")
            st.write("ELA Management LLC")
            st.write("999 Fortino Blvd Lot 246, Pueblo, CO 81008, US")
            st.write("CAGE 14ZP6 • UEI U32LBVK3DDF7 • DUNS 14-483-4790")

    conn = get_db()
    
    
    _force_safe_pd_read()
    try:
        ensure_unified_schemas(conn)
        ensure_pb_templates_schema(conn)
    except Exception as e:
        _debug_log(conn, 'main.ensure_unified_schemas', e)
    # Auto-send any due O5 follow-ups on each app run
    try:
        ensure_o5_schema(conn)
        try:
            _o5_send_due(conn, limit=200)
        except Exception as e2:
            _debug_log(conn, "o5_send_due_auto", e2)
    except Exception:
        # If O5 schema or helpers are not available yet, skip silently
        pass
    global _O4_CONN

    st.title(APP_TITLE)
    st.caption(BUILD_LABEL)

    try:
        _global_search_panel(conn)
    except Exception:
        pass
    # Y0 main panel (always on)
    try:
        y0_ai_panel()
    except Exception:
        pass
    override = st.session_state.pop('page_override', None)

    if override:

        router(override, conn)

        st.stop()

    router(nav(), conn)
# --- Outreach schema guard: fallback stub used if the full implementation is defined later ---
if "_o3_ensure_schema" not in globals():
    def _o3_ensure_schema(conn):
        try:
            from contextlib import closing
            with closing(conn.cursor()) as cur:
                # Minimal tables used by Outreach features
                cur.execute("CREATE TABLE IF NOT EXISTS vendors_t (id INTEGER PRIMARY KEY, name TEXT, email TEXT, phone TEXT, city TEXT, state TEXT, naics TEXT)")
                cur.execute("CREATE TABLE IF NOT EXISTS current_tenant (id INTEGER PRIMARY KEY, ctid INTEGER)")
                cur.execute("INSERT OR IGNORE INTO current_tenant(id, ctid) VALUES (1, 1)")
                cur.execute("CREATE TABLE IF NOT EXISTS outreach_templates (id INTEGER PRIMARY KEY, name TEXT, subject TEXT, body TEXT)")
                cur.execute("CREATE TABLE IF NOT EXISTS smtp_settings (id INTEGER PRIMARY KEY, host TEXT, port INTEGER, username TEXT, password TEXT, use_tls INTEGER)")
            conn.commit()
        except Exception:
            pass

# --- Outreach recipients UI: fallback stub used if the full implementation is defined later ---
if "_o3_collect_recipients_ui" not in globals():
    def _o3_collect_recipients_ui(conn):
        import pandas as _pd
        q = "SELECT id, name, email, phone, city, state, naics FROM vendors_t WHERE 1=1"
        params = []
        c1, c2, c3 = st.columns(3)
        with c1:
            f_naics = st.text_input("NAICS filter", key="o3_f_naics")
        with c2:
            f_state = st.text_input("State filter", key="o3_f_state")
        with c3:
            f_city = st.text_input("City filter", key="o3_f_city")
        if f_naics:
            q += " AND IFNULL(naics,'') LIKE ?"
            params.append(f"%{f_naics}%")
        if f_state:
            q += " AND IFNULL(state,'') LIKE ?"
            params.append(f"%{f_state}%")
        if f_city:
            q += " AND IFNULL(city,'') LIKE ?"
            params.append(f"%{f_city}%")
        try:
            df = _pd.__p_read_sql_query(q + " ORDER BY name ASC;", conn, params=tuple(params))
        except Exception:
            df = _pd.DataFrame(columns=["id","name","email","phone","city","state","naics"])
        if df is None or df.empty:
            # fallback to vendors table
            q = "SELECT id, name, email, phone, city, state, naics FROM vendors WHERE 1=1"
            try:
                df = _pd.__p_read_sql_query(q + " ORDER BY name ASC;", conn, params=tuple(params))
            except Exception:
                df = _pd.DataFrame(columns=["id","name","email","phone","city","state","naics"])
        st.caption(f"{len(df)} vendors match filters")
        if not df.empty:
            _styled_dataframe(df, use_container_width=True, hide_index=True)
        return df

# --- Outreach SMTP sender picker: working fallback ---
if "_o3_render_sender_picker" not in globals():
    def _o3_render_sender_picker_legacy():
        import streamlit as st

        conn = get_o4_conn()
        try:
            ensure_outreach_o1_schema(conn)
        except Exception:
            pass
        _ensure_email_accounts_schema(conn)
        rows = _get_senders(conn)
        if not rows:
            st.info("No sender accounts configured. Add one now:")
            with st.form("o4_inline_add_sender", clear_on_submit=True):
                email = st.text_input("Email address")
                display = st.text_input("Display name")
                pw = st.text_input("App password", type="password")
                ok = st.form_submit_button("Save sender")
                if ok and email:
                    try:
                        with conn:
                            conn.execute("""
                            INSERT INTO email_accounts(user_email, display_name, app_password)
                            VALUES(?,?,?)
                            ON CONFLICT(user_email) DO UPDATE SET
                                display_name=excluded.display_name,
                                app_password=excluded.app_password
                            """, (email.strip(), display or "", pw or ""))
                        st.success("Saved")
                        try:
                            import streamlit as st

                            st.session_state["o4_sender_sel"] = email.strip()
                        except Exception:
                            pass
                        st.rerun()
                    except Exception as e:
                        st.error(f"Save failed: {e}")
            if ok and email:
                conn.execute("INSERT OR REPLACE INTO email_accounts(user_email, display_name, app_password) VALUES(?,?,?)",
                             (email.strip().lower(), display.strip(), pw.strip()))
                conn.commit()
                rows = _get_senders(conn)
        if not rows:
            st.error("No sender accounts configured")
            return {"email": "", "app_password": ""}
        choices = [r[0] for r in rows] + ["<add new>"]
        sel = st.selectbox("From account", choices, key="o4_from_addr")
        if sel == "<add new>":
            st.info("Add a sender in Outreach -> Sender accounts")
            return {"email": "", "app_password": ""}
        pw = st.text_input("App password", type="password", key="o4_from_pw")
        return {"email": sel, "app_password": pw}

def y_auto_k(text: str) -> int:
    t = (text or "").lower()
    n = len(t)
    broad = any(k in t for k in [
        "overview", "summary", "summarize", "list all", "requirements", "compliance",
        "section l", "section m", "evaluation factors", "factors", "checklist",
        "compare", "differences", "conflict", "conflicts", "crosswalk", "matrix"
    ])
    if not t.strip():
        return 4
    base = 7 if broad else 4
    if n > 500:
        base += 1
    if n > 1200:
        base += 1
    return max(3, min(8, base))

# === PHASE 8: Latency and UX ===
# Memoize y1_search by (rfp_id, query) and a snapshot of the chunks table to keep cache fresh.
def _y1_snapshot(conn, rfp_id: int) -> str:
    try:
        df = pd.read_sql_query(
            "SELECT COUNT(1) AS c, COALESCE(MAX(id),0) AS m FROM rfp_chunks WHERE rfp_id=?;",
            conn, params=(int(rfp_id),)
        )
        c = int(df.iloc[0]["c"]) if df is not None and not df.empty else 0
        m = int(df.iloc[0]["m"]) if df is not None and not df.empty else 0
        return f"{c}:{m}"
    except Exception:
        return "0:0"

try:
    import streamlit as _st_phase8
except Exception:
    _st_phase8 = None

if _st_phase8 is not None:
    @_st_phase8.cache_data(show_spinner=False, ttl=600)
    def _y1_search_cached(db_path: str, rfp_id: int, query: str, k: int, snapshot: str):
        import sqlite3 as _sql8
        # Re-open a read-only connection to keep cache pure
        try:
            conn2 = _sql8.connect(db_path, check_same_thread=False)
        except Exception:
            conn2 = _sql8.connect(db_path)
        try:
            return _y1_search_uncached(conn2, int(rfp_id), query or "", int(k))
        finally:
            try:
                conn2.close()
            except Exception:
                pass
def y1_search(conn, rfp_id: int, query: str, k: int = 6):
    # Compute snapshot to invalidate cache if chunks changed
    snap = _y1_snapshot(conn, int(rfp_id))
    try:
        db_path = DB_PATH  # provided in app
    except Exception:
        # very conservative fallback
        db_path = "data/govcon.db"
    return _y1_search_cached(db_path, int(rfp_id), query or "", int(k), snap)

# Enable chunk-level streaming in Y2 and Y4
def y2_stream_answer(conn, rfp_id: int, thread_id: int, user_q: str, k: int = 6, temperature: float = 0.2, mode: str = 'Auto'):
    try:
        for tok in ask_ai_with_citations(conn, int(rfp_id), user_q or "", k=int(k), temperature=temperature, mode=mode):
            yield tok
    except NameError:
        # fallback if dependencies were not merged
        hits = []
        try:
            hits = y1_search(conn, int(rfp_id), user_q or "", k=int(k))
        except Exception:
            pass
        yield "[system] limited mode. rebuild index on Y1, then retry."

# Re-define y4_stream_review to ensure true token streaming (shadow any earlier stub)
def y4_stream_review(conn, rfp_id: int, draft_text: str, k: int = 6, temperature: float = 0.1):
    msgs = _y4_build_messages(conn, int(rfp_id), draft_text or "", k=int(k))
    client = get_ai()
    model_name = _resolve_model()
    try:
        resp = client.chat.completions.create(
            model=model_name,
            messages=msgs,
            temperature=float(temperature),
            stream=True
        )
    except Exception as _e:
        if "model_not_found" in str(_e) or "does not exist" in str(_e):
            resp = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=msgs,
                temperature=float(temperature),
                stream=True
            )
        else:
            yield f"AI unavailable: {type(_e).__name__}: {_e}"
            return
    for ch in resp:
        try:
            delta = ch.choices[0].delta
            if hasattr(delta, "content") and delta.content:
                yield delta.content
        except Exception:
            pass
# === end PHASE 8 ===

def _y1_cache_bust():
    try:
        st.session_state.pop("_y1_cache", None)
    except Exception:
        pass

# --- Phase 9: tiny test harness ---
def test_seed_cases():
    results = []
    try:
        _flags_ok = bool(CO_STRICT) and bool(PARSER_STRICT) and bool(EVIDENCE_GATE)
    except Exception:
        _flags_ok = False
    results.append(("flags_default_on", _flags_ok))

    import sqlite3
    from contextlib import closing as _closing
    conn = _db_connect(DB_PATH, check_same_thread=False)
    with _closing(conn.cursor()) as cur:
        cur.execute("CREATE TABLE IF NOT EXISTS rfp_chunks(rfp_id INTEGER, rfp_file_id INTEGER, file_name TEXT, page INTEGER, chunk_idx INTEGER, text TEXT, emb TEXT);")
        conn.commit()
    try:
        gen = ask_ai_with_citations(conn, 1, "What is the due date?", k=6, temperature=0.0)
        first = next(gen, "")
        _ev_gate = isinstance(first, str) and first.lower().startswith("[system]")
    except Exception:
        _ev_gate = False
    results.append(("evidence_gate_no_hits", _ev_gate))

    try:
        d1 = _y55_norm_date("Oct 1, 2025")
        d2 = _y55_norm_date("10/01/2025")
        _date_ok = (isinstance(d1, str) and d1.startswith("2025-10-01")) and (isinstance(d2, str) and d2.startswith("2025-10-01"))
    except Exception:
        _date_ok = True
    results.append(("parser_date_norm", _date_ok))

    try:
        with _closing(conn.cursor()) as cur:
            cur.execute("CREATE TABLE IF NOT EXISTS rfps(id INTEGER PRIMARY KEY, title TEXT);")
            cur.execute("INSERT INTO rfps(id,title) VALUES(1,'Test RFP');")
            cur.execute("CREATE TABLE IF NOT EXISTS lm_items(rfp_id INTEGER, section TEXT, item TEXT);")
            conn.commit()
        ok_gate, missing = require_LM_minimum(conn, 1)
        _lm_gate = (ok_gate is False) and isinstance(missing, list)
    except Exception:
        _lm_gate = True
    results.append(("lm_gate_detects_missing", _lm_gate))

    all_ok = all(v for _, v in results)
    return {"ok": all_ok, "results": results}

# === X16.1: Capability Statement — AI drafting helper ===
def run_capability_statement(conn):
    """
    Wrap the original capability statement page (if present) and add the X16.1
    AI drafting helper panel. This makes the Draft buttons actually call the
    same OpenAI backend used elsewhere in the app and save results into
    org_profile.
    """
    # First, try to call the original implementation if it exists.
    # If the legacy UI is broken (e.g., NameError: 'paragraph'), we fall back
    # quietly to the AI helper so the page still works.
    if "_orig_run_capability_statement" in globals():
        try:
            _orig_run_capability_statement(conn)
        except Exception as e:
            # Legacy Capability Statement UI is not available or failed.
            # Fall back to the AI helper-only view without a noisy message.
            st.header("Capability Statement")
            st.caption(
                "Use this page to generate and store tailored capability statements "
                "for specific agencies and NAICS codes."
            )
    else:
        st.header("Capability Statement")
        st.caption(
            "Use this page to generate and store tailored capability statements "
            "for specific agencies and NAICS codes."
        )

    # Then render the X16.1 helper
    try:
        with st.expander("X16.1 — AI drafting helper (OpenAI)", expanded=False):
            st.caption(
                "Draft tagline, core competencies, and differentiators using your "
                "org profile and recent RFP context."
            )

            # Load org profile seed values
            try:
                dfp = pd.read_sql_query("SELECT * FROM org_profile WHERE id=1;", conn)
            except Exception:
                dfp = None
            profile = (
                dfp.iloc[0].to_dict()
                if isinstance(dfp, pd.DataFrame) and not dfp.empty
                else {}
            )
            company = (profile.get("company_name") or "").strip() or "Your Company"
            tagline0 = profile.get("tagline") or ""
            core0 = profile.get("core_competencies") or ""
            diff0 = profile.get("differentiators") or ""

            st.write(f"**Company:** {company}")
            audience = st.text_input(
                "Audience focus (e.g., BOP Facilities, USAF MXG, VA VISN)",
                key="x161_aud",
            )
            tone = st.selectbox(
                "Tone",
                ["Crisp federal", "Technical", "Plain language"],
                index=0,
                key="x161_tone",
            )
            include_past_perf = st.checkbox(
                "Incorporate past performance bullets if available",
                value=True,
                key="x161_pp",
            )

            # Optional RFP context
            ctx = ""
            try:
                dfr = pd.read_sql_query(
                    "SELECT id, title FROM rfps ORDER BY id DESC;", conn
                )
                if isinstance(dfr, pd.DataFrame) and not dfr.empty:
                    options = [None] + dfr["id"].tolist()

                    def _fmt_rfp(i):
                        if i is None:
                            return "No additional context"
                        try:
                            title = dfr.loc[dfr["id"] == i, "title"].values[0]
                        except Exception:
                            title = ""
                        return f"#{i} — {title}"

                    rfp_sel = st.selectbox(
                        "Optional RFP context",
                        options=options,
                        format_func=_fmt_rfp,
                        key="x161_rfp",
                    )
                else:
                    rfp_sel = None
            except Exception:
                rfp_sel = None

            if rfp_sel:
                try:
                    hits = pd.read_sql_query(
                        "SELECT text FROM rfp_chunks WHERE rfp_id=? ORDER BY id LIMIT 40;",
                        conn,
                        params=(int(rfp_sel),),
                    )
                    if (
                        isinstance(hits, pd.DataFrame)
                        and not hits.empty
                        and "text" in hits.columns
                    ):
                        ctx = "\n\n".join(hits["text"].fillna("").tolist())[:12000]
                except Exception:
                    ctx = ""

            def _ask_x161(kind: str) -> str:
                """
                Use the shared ask_ai() helper to draft one piece of the
                capability statement and return the full text.
                """
                base_sys = (
                    "You are a senior U.S. federal capture and proposal writer. "
                    "Write concise, concrete bullets suitable for a one-page capability "
                    "statement. Avoid hype, avoid first person, and favor contracting "
                    "language."
                )
                user_prompt = f"""Company: {company}
Audience: {audience or "(general federal)"}
Tone: {tone}
Existing tagline: {tagline0[:200]}
Existing core competencies: {core0[:800]}
Existing differentiators: {diff0[:800]}
Include past performance: {bool(include_past_perf)}
Task: Draft {kind} for a one-page capability statement.
Constraints:
- For lists, use 4–7 bullets, 12–18 words each.
- Use federal terms where appropriate.
- If RFP context is provided, align language and priorities to it.

RFP context (may be empty):
{ctx or "(none)"}"""
                acc = []
                try:
                    for tok in ask_ai(
                        [
                            {"role": "system", "content": base_sys},
                            {"role": "user", "content": user_prompt},
                        ],
                        temperature=0.2,
                    ):
                        acc.append(tok)
                except Exception as e:
                    return f"AI error: {e}"
                return "".join(acc).strip()

            c1, c2, c3 = st.columns(3)
            with c1:
                if st.button("Draft Tagline", key="x161_tl"):
                    with st.spinner("Drafting tagline..."):
                        st.session_state["x161_tagline"] = _ask_x161(
                            "a concise 8–14 word tagline"
                        )
            with c2:
                if st.button("Draft Core Competencies", key="x161_cc"):
                    with st.spinner("Drafting core competencies..."):
                        st.session_state["x161_core"] = _ask_x161(
                            "Core Competencies bullets"
                        )
            with c3:
                if st.button("Draft Differentiators", key="x161_df"):
                    with st.spinner("Drafting differentiators..."):
                        st.session_state["x161_diff"] = _ask_x161(
                            "Differentiators bullets"
                        )

            # Initialize session-backed fields for the AI outputs
            if "x161_tagline" not in st.session_state:
                st.session_state["x161_tagline"] = tagline0
            if "x161_core" not in st.session_state:
                st.session_state["x161_core"] = core0
            if "x161_diff" not in st.session_state:
                st.session_state["x161_diff"] = diff0

            st.text_input(
                "Tagline (AI)",
                key="x161_tagline",
            )
            st.text_area(
                "Core Competencies (AI)",
                height=160,
                key="x161_core",
            )
            st.text_area(
                "Differentiators (AI)",
                height=160,
                key="x161_diff",
            )

            if st.button("Save AI fields into org_profile", key="x161_save"):
                try:
                    with closing(conn.cursor()) as cur:
                        # ensure profile row exists
                        cur.execute(
                            "INSERT OR IGNORE INTO org_profile(id, company_name) VALUES(1, ?);",
                            (company,),
                        )
                        cur.execute(
                            "UPDATE org_profile SET tagline=?, core_competencies=?, differentiators=? WHERE id=1;",
                            (
                                (st.session_state.get("x161_tagline", "") or "").strip(),
                                (st.session_state.get("x161_core", "") or "").strip(),
                                (st.session_state.get("x161_diff", "") or "").strip(),
                            ),
                        )
                        conn.commit()
                    st.success("Saved to org_profile")
                except Exception as e:
                    st.error(f"Save failed: {e}")
    except Exception as e:
        st.error(f"X16.1 panel error: {e}")
# === end X16.1 ===


# === S1 Subcontractor Finder: Google Places ====================================
def ensure_subfinder_s1_schema(conn):
    try:
        cols = [r[1] for r in conn.execute("PRAGMA table_info(vendors)").fetchall()]
    except Exception:
        cols = []
    with conn:
        if "place_id" not in cols:
            try: conn.execute("ALTER TABLE vendors ADD COLUMN place_id TEXT")
            except Exception: pass
        if "fit_score" not in cols:
            try: conn.execute("ALTER TABLE vendors ADD COLUMN fit_score REAL")
            except Exception: pass
        if "tags" not in cols:
            try: conn.execute("ALTER TABLE vendors ADD COLUMN tags TEXT")
            except Exception: pass
        conn.execute("CREATE UNIQUE INDEX IF NOT EXISTS idx_vendors_place_id ON vendors(place_id)")

def s1_normalize_phone(s:str)->str:
    s = (s or "").strip()
    return "".join(ch for ch in s if ch.isdigit())

def s1_get_google_api_key()->str|None:
    import os
    try:

        if "google" in st.secrets and "api_key" in st.secrets["google"]:
            return st.secrets["google"]["api_key"]
        if "google_api_key" in st.secrets:
            return st.secrets["google_api_key"]
    except Exception:
        pass
    return os.environ.get("GOOGLE_API_KEY")

def s1_geocode_address(address:str):
    key = s1_get_google_api_key()
    if not key: return None
    import urllib.parse, urllib.request, json
    params = urllib.parse.urlencode({"address": address, "key": key})
    url = f"https://maps.googleapis.com/maps/api/geocode/json?{params}"
    try:
        with urllib.request.urlopen(url, timeout=20) as resp:
            data = json.loads(resp.read().decode("utf-8"))
        if data.get("results"):
            loc = data["results"][0]["geometry"]["location"]
            return float(loc["lat"]), float(loc["lng"])
    except Exception:
        return None
    return None

def ensure_outreach_o1_schema(conn):
    """Create or migrate email_accounts table for Outreach."""
    with conn:
        conn.execute(
            """
            CREATE TABLE IF NOT EXISTS email_accounts(
                user_email TEXT PRIMARY KEY,
                display_name TEXT DEFAULT '',
                app_password TEXT DEFAULT '',
                smtp_host   TEXT DEFAULT 'smtp.gmail.com',
                smtp_port   INTEGER DEFAULT 465,
                use_ssl     INTEGER DEFAULT 1,
                use_tls     INTEGER DEFAULT 0,
                username    TEXT DEFAULT ''
            )
            """
        )
    __p_ensure_column(conn, "email_accounts", "display_name", "TEXT DEFAULT ''")
    __p_ensure_column(conn, "email_accounts", "app_password", "TEXT DEFAULT ''")
    __p_ensure_column(conn, "email_accounts", "smtp_host", "TEXT DEFAULT 'smtp.gmail.com'")
    __p_ensure_column(conn, "email_accounts", "smtp_port", "INTEGER DEFAULT 465")
    __p_ensure_column(conn, "email_accounts", "use_ssl", "INTEGER DEFAULT 1")
    __p_ensure_column(conn, "email_accounts", "use_tls", "INTEGER DEFAULT 0")
    __p_ensure_column(conn, "email_accounts", "username", "TEXT DEFAULT ''")
    # Optional: legacy outreach_sender_accounts
    try:
        __p_ensure_column(conn, "outreach_sender_accounts", "app_password", "TEXT DEFAULT ''")
        __p_ensure_column(conn, "outreach_sender_accounts", "smtp_host", "TEXT DEFAULT 'smtp.gmail.com'")
        __p_ensure_column(conn, "outreach_sender_accounts", "smtp_port", "INTEGER DEFAULT 587")
        __p_ensure_column(conn, "outreach_sender_accounts", "use_tls", "INTEGER DEFAULT 1")
        __p_ensure_column(conn, "outreach_sender_accounts", "is_active", "INTEGER DEFAULT 1")
    except Exception:
        pass
    return True
def o1_delete_email_account(conn, user_email:str):
    ensure_outreach_o1_schema(conn)
    with conn:
        conn.execute("DELETE FROM email_accounts WHERE user_email=?", (user_email.strip(),))

# === O2: Outreach Templates ====================================================
def ensure_email_templates(conn):
    with conn:
        conn.execute("""
        CREATE TABLE IF NOT EXISTS email_templates(
            id INTEGER PRIMARY KEY,
            name TEXT UNIQUE NOT NULL,
            subject TEXT NOT NULL DEFAULT '',
            html_body TEXT NOT NULL DEFAULT '',
            created_at TEXT DEFAULT CURRENT_TIMESTAMP,
            updated_at TEXT DEFAULT CURRENT_TIMESTAMP
        )""")
        conn.execute("""
        CREATE TRIGGER IF NOT EXISTS trg_email_templates_update
        AFTER UPDATE ON email_templates
        BEGIN
            UPDATE email_templates SET updated_at=CURRENT_TIMESTAMP WHERE id=NEW.id;
        END;""")

def email_template_list(conn):
    ensure_email_templates(conn)
    return conn.execute("SELECT id, name, subject, html_body FROM email_templates ORDER BY name").fetchall()

def email_template_get(conn, template_id:int):
    ensure_email_templates(conn)
    return conn.execute("SELECT id, name, subject, html_body FROM email_templates WHERE id=?", (int(template_id),)).fetchone()

def email_template_upsert(conn, name:str, subject:str, html_body:str, template_id:int|None=None):
    ensure_email_templates(conn)
    with conn:
        if template_id:
            conn.execute("UPDATE email_templates SET name=?, subject=?, html_body=? WHERE id=?",
                         (name.strip(), subject, html_body, int(template_id)))
            return template_id
        conn.execute("""
            INSERT INTO email_templates(name, subject, html_body)
            VALUES(?,?,?)
            ON CONFLICT(name) DO UPDATE SET subject=excluded.subject, html_body=excluded.html_body
        """, (name.strip(), subject, html_body))
        return conn.execute("SELECT id FROM email_templates WHERE name=?", (name.strip(),)).fetchone()[0]

def email_template_delete(conn, template_id:int):
    ensure_email_templates(conn)
    with conn:
        conn.execute("DELETE FROM email_templates WHERE id=?", (int(template_id),))

import re as _re_o2
MERGE_TAGS = {"company","email","title","solicitation","due","notice_id","first_name","last_name","city","state"}
def template_missing_tags(text:str, required:set[str]=MERGE_TAGS)->set[str]:
    """Return any unknown merge tags used in the template.

    We treat all supported tags as optional. This function only flags tags that
    appear in the template but are not in the supported set, so normal templates
    with no tags or a subset of tags will not raise warnings.
    """
    found = set(_re_o2.findall(r"{{\s*([a-zA-Z0-9_]+)\s*}}", text or ""))
    # Unknown tags = tags referenced in the template that are not supported
    return {t for t in found if t not in required}

def render_outreach_templates(conn):

    st.subheader("Email templates")
    st.caption("Use merge tags like {{first_name}}, {{last_name}}, {{company}}, {{title}}, {{city}}, {{state}}, {{email}}, {{due}}, {{solicitation}}, and {{notice_id}}. They must match the column names in your recipient table to be filled automatically.")
    ensure_email_templates(conn)
    rows = email_template_list(conn)
    names = ["<new>"] + [r[1] for r in rows]
    sel = st.selectbox("Template", names, key="tpl_sel")
    if sel == "<new>":
        tid = None
        name = st.text_input("Name", key="tpl_name")
        subject = st.text_input("Subject", value=st.session_state.get("outreach_subject",""))
        sig_html = st.text_area("HTML body", value=st.session_state.get("outreach_html",""), height=300)
    else:
        row = next(r for r in rows if r[1] == sel)
        tid = row[0]
        name = st.text_input("Name", value=row[1], key="tpl_name__2")
        subject = st.text_input("Subject", value=row[2], key="tpl_subject")
        sig_html = st.text_area("HTML body", value=row[3], key="tpl_html", height=240)
    missing = template_missing_tags((subject or "") + " " + (sig_html or ""))
    if missing:
        st.info("Unknown merge tags (will not be filled automatically): " + ", ".join(sorted(missing)))
    c1, c2, c3 = st.columns(3)
    with c1:
        if st.button("Save"):
            if not (name or "").strip():
                st.error("Name required")
            else:
                email_template_upsert(conn, name, subject or "", sig_html or "", tid)
                # Keep Outreach and Mail Merge fields in sync immediately after save
                try:
                    st.session_state["tpl_name"] = name or ""
                    st.session_state["tpl_subject"] = subject or ""
                    st.session_state["tpl_html"] = sig_html or ""
                    st.session_state["outreach_subject"] = subject or ""
                    st.session_state["outreach_html"] = sig_html or ""
                    st.session_state["o3_subject"] = subject or ""
                    st.session_state["o3_body"] = sig_html or ""
                except Exception:
                    pass
                st.success("Saved")
    with c2:
        if (tid is not None) and st.button("Duplicate"):
            email_template_upsert(conn, f"{name} copy", subject or "", sig_html or "", None)
            st.success("Duplicated")
            st.rerun()
    with c3:
        if (tid is not None) and st.button("Delete"):
            email_template_delete(conn, tid)
            st.success("Deleted")
            st.rerun()

def _tpl_picker_prefill(conn):

    rows = email_template_list(conn)
    if not rows:
        return None
    names = [r[1] for r in rows]
    choice = st.selectbox("Use template", ["<none>"] + names, key="tpl_pick")
    if choice != "<none>":
        row = next(r for r in rows if r[1] == choice)
       # Populate high-level outreach session keys
        st.session_state["outreach_subject"] = row[2]
        st.session_state["outreach_html"] = row[3]
        # Also sync the actual Mail Merge widget keys so the fields update immediately
        st.session_state["o3_subject"] = row[2]
        st.session_state["o3_body"] = row[3]
        return row
    return None

def seed_default_templates(conn):
    ensure_email_templates(conn)
    defaults = [
        ("RFQ Intro", "RFQ: {{title}} {{solicitation}} due {{due}}",
         "<p>Hello {{first_name}},</p><p>We are collecting quotes for {{title}} under {{solicitation}} due {{due}}.</p><p>Unsubscribe: reply STOP</p>"),
        ("Follow Up 1", "Follow up on {{title}} RFQ", "<p>Checking in on your quote for {{title}}.</p><p>Unsubscribe: reply STOP</p>")
    ]
    for n,s,h in defaults:
        email_template_upsert(conn, n, s, h, None)

# --- O4 wrapper: delegates to __p_o4_ui if present, else shows fallback UI ---

def o4_sender_accounts_ui(conn):
    import streamlit as _st
    from contextlib import closing
    try:
        import pandas as _pd
    except Exception:
        _pd = None
    
    _st.info("O4 sender accounts fallback UI loaded.")
    
    # Ensure modern multi-sender table exists
    with conn:
        conn.execute(
            "CREATE TABLE IF NOT EXISTS outreach_sender_accounts(" 
            "id INTEGER PRIMARY KEY, " 
            "label TEXT UNIQUE, " 
            "email TEXT, " 
            "app_password TEXT, " 
            "smtp_host TEXT DEFAULT 'smtp.gmail.com', " 
            "smtp_port INTEGER DEFAULT 587, " 
            "use_tls INTEGER DEFAULT 1, " 
            "is_active INTEGER DEFAULT 1, " 
            "daily_limit INTEGER DEFAULT 500, " 
            "minute_limit INTEGER DEFAULT 60, " 
            "sent_today_count INTEGER DEFAULT 0, " 
            "last_sent_at TEXT" 
            ")"
        )
# One-time migration from legacy smtp_settings (id=1) if multi-sender table is empty
    try:
        with closing(conn.cursor()) as cur:
            cur.execute("SELECT COUNT(*) FROM outreach_sender_accounts;")
            cnt = cur.fetchone()[0] or 0
            if cnt == 0:
                cur.execute(
                    "SELECT label, host, port, username, password, use_tls "
                    "FROM smtp_settings WHERE id=1"
                )
                row = cur.fetchone()
                if row:
                    label, host, port, username, password, use_tls = row
                    email = (username or "").strip()
                    if email:
                        conn.execute(
                            "INSERT OR IGNORE INTO outreach_sender_accounts"
                            "(label, email, app_password, smtp_host, smtp_port, use_tls, is_active) "
                            "VALUES(?,?,?,?,?,?,1);",
                            (
                                (label or "Default").strip() or email,
                                email,
                                (password or "").strip(),
                                (host or "smtp.gmail.com").strip(),
                                int(port or 587),
                                1 if (use_tls or 0) else 0,
                            ),
                        )
    except Exception:
        pass
    
    # Load existing senders
    rows = []
    try:
        rows = list(
            conn.execute(
                "SELECT id, label, email, smtp_host, smtp_port, use_tls, is_active "
                "FROM outreach_sender_accounts ORDER BY id;"
            ).fetchall()
        )
    except Exception:
        rows = []
    
    if rows:
        _st.markdown("**Saved sender accounts**")
        try:
            data_rows = []
            for r in rows:
                data_rows.append(
                    {
                        "id": r[0],
                        "label": r[1],
                        "email": r[2],
                        "host": r[3],
                        "port": r[4],
                        "use_tls": bool(r[5]),
                        "active": bool(r[6]),
                    }
                )
            if _pd is not None:
                df = _pd.DataFrame(data_rows)
                try:
                    _styled_dataframe(df, use_container_width=True, hide_index=True)  # type: ignore[name-defined]
                except Exception:
                    try:
                        _st.dataframe(df, use_container_width=True, hide_index=True)
                    except TypeError:
                        _st.dataframe(df)
            else:
                _st.write(rows)
        except Exception:
            pass
    else:
        _st.caption("No sender accounts saved yet. Use the form below to add one or more senders.")
    
    _st.markdown("**Add or update sender account**")
    with _st.form("o4_sender_accounts_form", clear_on_submit=True):
        c1, c2 = _st.columns(2)
        with c1:
            label = _st.text_input("Label", value="")
            email = _st.text_input("Sender email", value="")
            app_password = _st.text_input("App password (Gmail app password)", type="password", value="")
        with c2:
            smtp_host = _st.text_input("SMTP host", value="smtp.gmail.com")
            smtp_port = _st.number_input("SMTP port", 1, 65535, value=587)
            use_tls = _st.checkbox("Use STARTTLS (TLS)", value=True)
        is_active = _st.checkbox("Active", value=True)
        saved = _st.form_submit_button("Save sender")
        if saved:
            if not email or "@" not in email:
                _st.error("Please enter a valid sender email.")
            else:
                try:
                    with conn:
                        conn.execute(
                            "INSERT OR REPLACE INTO outreach_sender_accounts"
                            "(label, email, app_password, smtp_host, smtp_port, use_tls, is_active) "
                            "VALUES(?,?,?,?,?,?,?);",
                            (
                                (label or "").strip() or email.strip(),
                                email.strip(),
                                (app_password or "").strip(),
                                (smtp_host or "smtp.gmail.com").strip(),
                                int(smtp_port),
                                1 if use_tls else 0,
                                1 if is_active else 0,
                            ),
                        )
                    _st.success("Sender saved.")
                except Exception as e:
                    _st.error(f"Save failed: {e}")
    
    # Delete sender controls
    if rows:
        _st.markdown("**Delete sender account**")
        options = [
            (r[0], f"{(r[1] or '').strip()} — {str(r[2] or '').strip()}".strip(" —"))
            for r in rows
        ]
    
        def _fmt_choice(opt):
            try:
                return opt[1]
            except Exception:
                return str(opt)
    
        sel = _st.selectbox(
            "Select sender to delete",
            options,
            format_func=_fmt_choice,
            key="o4_sender_del",
        )
        if _st.button("Delete selected sender"):
            try:
                sender_id = int(sel[0])
            except Exception:
                sender_id = None
            if sender_id:
                try:
                    with conn:
                        conn.execute(
                            "DELETE FROM outreach_sender_accounts WHERE id=?;",
                            (sender_id,),
                        )
                    _st.success("Sender deleted.")
                except Exception as e:
                    _st.error(f"Delete failed: {e}")
    

def render_outreach_mailmerge(conn):
    globals()['_O4_CONN'] = conn
    import streamlit as st

    import pandas as _pd
    # 1) Recipients
    rows = _o3_collect_recipients_ui(conn) if "_o3_collect_recipients_ui" in globals() else None
    st.subheader("Mail Merge & Send")
    # 2) Template inputs
    # Seed widget state from outreach_* once, then let the widgets own their values.
    if "o3_subject" not in st.session_state:
        st.session_state["o3_subject"] = st.session_state.get("outreach_subject", "")
    if "o3_body" not in st.session_state:
        st.session_state["o3_body"] = st.session_state.get("outreach_html", "")
    subj = st.text_input("Subject", key="o3_subject")
    body = st.text_area("HTML Body", height=260, key="o3_body")
    st.caption("You can use the same merge tags here as in templates: {{first_name}}, {{last_name}}, {{company}}, {{title}}, {{city}}, {{state}}, {{email}}, {{due}}, {{solicitation}}, and {{notice_id}}. They are filled from the columns in your recipient table.")

    # Attachments for this blast
    ups = st.file_uploader("Attachments (optional)", type=["pdf","doc","docx","xls","xlsx","ppt","pptx","txt","csv","png","jpg","jpeg","zip"], accept_multiple_files=True, key="o3_attachments")
    attach_paths = st.session_state.get("o3_attach_paths", [])
    if ups:
        attach_paths = []
        for up in ups:
            try:
                p = save_uploaded_file(up, subdir="outreach")
                if p:
                    attach_paths.append(p)
            except Exception:
                pass
        st.session_state["o3_attach_paths"] = attach_paths
    if attach_paths:
        st.caption("Attachments to include: " + ", ".join([os.path.basename(p) for p in attach_paths]))
    # 3) Sender
    st.subheader("Sender")
    sender = _o3_render_sender_picker() if "_o3_render_sender_picker" in globals() else {}
    # normalize keys
    if sender and "username" in sender and "email" not in sender:
        sender["email"] = sender.get("username","")
    if sender and "password" in sender and "app_password" not in sender:
        sender["app_password"] = sender.get("password","")
    # Signature editor scoped to Mail Merge -> Sender
    try:
        st.subheader("Signature for selected sender")
        __p_call_sig_ui(conn)
    except Exception:
        pass

    c1, c2, c3 = st.columns([1,1,2])
    with c1:
        test = st.button("Test run (no send)", key="o3_test")
    with c2:
        do = st.button("Send batch", type="primary", key="o3_send")
    with c3:
        maxn = st.number_input("Max to send", min_value=1, max_value=5000, value=500, step=50, key="o3_max")

    if (test or do) and rows is not None and hasattr(rows, "empty") and not rows.empty:
        try:
            if "_o3_send_batch" in globals():
                total_rows = len(rows)
                large_batch = total_rows >= 50
                if test:
                    st.info("Test run: rendering only, no SMTP.")
                    # call with test_only=True
                    out = _o3_send_batch(conn, sender, rows, subj, body, True, int(maxn), attachments=attach_paths)
                    st.success("Test run executed")
                else:
                    if large_batch:
                        # For large sends, enqueue a job so it can be processed in the jobs worker.
                        try:
                            ensure_jobs_schema(conn)
                            try:
                                rows_payload = rows.to_dict(orient="records")
                            except Exception:
                                try:
                                    import pandas as _pd
                                    rows_payload = _pd.DataFrame(rows).to_dict(orient="records")
                                except Exception:
                                    rows_payload = []
                            payload = {
                                "sender": sender,
                                "subject_tpl": subj,
                                "html_tpl": body,
                                "max_send": int(maxn),
                                "attachments": attach_paths,
                                "row_count": int(total_rows),
                                "rows": rows_payload,
                            }
                            job_id = jobs_enqueue(
                                conn,
                                job_type="outreach_batch_send",
                                payload=payload,
                                created_by=get_current_user_name(),
                            )
                            jobs_update_status(
                                conn,
                                job_id,
                                status="queued",
                                progress=0.0,
                                mark_started=False,
                            )
                            st.success(f"Queued {int(total_rows)} recipients as job #{job_id}. You can monitor progress on the My Jobs tab.")
                        except Exception as e:
                            st.error(f"Failed to enqueue outreach batch job: {e}")
                    else:
                        try:
                            out = _o3_send_batch(conn, sender, rows, subj, body, False, int(maxn), attachments=attach_paths)
                            st.success("Send function executed")
                        except Exception as e:
                            st.error(f"Send failed: {e}")
                            raise
            else:
                st.warning("Send function not available in this build.")
        except Exception as e:
            st.error(f"Send failed: {e}")

def run_outreach(conn):
    import streamlit as st

    # O4 badge if present
    try:
        _o4_render_badge()
    except Exception:
        pass

    st.header("Outreach")
    st.caption("Use this page to send targeted email campaigns to agencies and vendors and track your outreach work.")
    st.markdown("**Primary action on this page:** build or pick a template, then use Mail Merge and Send to deliver batch emails.")
    with st.expander("Advanced: Compliance & Unsubscribe (O6)", expanded=False):
        render_outreach_o6_compliance(conn)

    # O6: handle unsubscribe links
    o6_handle_query_unsubscribe(conn)
    with st.expander("Advanced: Follow-ups & SLA (O5)", expanded=False):
        render_outreach_o5_followups(conn)

    # Sender accounts (O4)
    try:
        with st.expander("Sender accounts", expanded=True):
                o4_sender_accounts_ui(conn)
    except Exception as e:
        st.warning(f"O4 sender UI unavailable: {e}")

    # Templates (O2)
    try:
        _tpl_picker_prefill(conn)
        with st.expander("Templates", expanded=False):
            render_outreach_templates(conn)
    except Exception:
        pass

    # Mail merge + send (O3)
    try:
        with st.expander("Mail Merge & Send", expanded=True):
            render_outreach_mailmerge(conn)
    except Exception as e:
        ui_error("Mail merge panel error.", str(e))

def _o3_ensure_schema(conn):
    with _o3c(conn.cursor()) as cur:
        cur.execute("""CREATE TABLE IF NOT EXISTS outreach_optouts(
            id INTEGER PRIMARY KEY,
            email TEXT UNIQUE,
            reason TEXT,
            ts TEXT DEFAULT CURRENT_TIMESTAMP
        );""")
        cur.execute("""CREATE TABLE IF NOT EXISTS outreach_blasts(
            id INTEGER PRIMARY KEY,
            title TEXT,
            template_name TEXT,
            sender_email TEXT,
            created_at TEXT DEFAULT CURRENT_TIMESTAMP
        );""")
        cur.execute("""CREATE TABLE IF NOT EXISTS outreach_log(
            id INTEGER PRIMARY KEY,
            blast_id INTEGER,
            to_email TEXT,
            to_name TEXT,
            subject TEXT,
            status TEXT,
            error TEXT,
            created_at TEXT DEFAULT CURRENT_TIMESTAMP
        );""")
        # Add tracking columns used by newer Outreach features
        try:
            cur.execute("PRAGMA table_info(outreach_log);")
            cols = [r[1] for r in cur.fetchall()]
            if "opened_at" not in cols:
                cur.execute("ALTER TABLE outreach_log ADD COLUMN opened_at TEXT;")
            if "bounced_at" not in cols:
                cur.execute("ALTER TABLE outreach_log ADD COLUMN bounced_at TEXT;")
        except Exception:
            # If ALTER TABLE is not supported, continue without raising.
            pass
        conn.commit()
    with _o3c(conn.cursor()) as cur:
        cur.execute("""CREATE TABLE IF NOT EXISTS outreach_optouts(
            id INTEGER PRIMARY KEY,
            email TEXT UNIQUE,
            reason TEXT,
            ts TEXT DEFAULT CURRENT_TIMESTAMP
        );""")
        cur.execute("""CREATE TABLE IF NOT EXISTS outreach_blasts(
            id INTEGER PRIMARY KEY,
            title TEXT,
            template_name TEXT,
            sender_email TEXT,
            created_at TEXT DEFAULT CURRENT_TIMESTAMP
        );""")
        cur.execute("""CREATE TABLE IF NOT EXISTS outreach_log(
            id INTEGER PRIMARY KEY,
            blast_id INTEGER,
            to_email TEXT,
            to_name TEXT,
            subject TEXT,
            status TEXT,
            error TEXT,
            created_at TEXT DEFAULT CURRENT_TIMESTAMP
        );""")
        conn.commit()

def _o3_wrap_email_html(sig_html: str) -> str:
    # Basic, client-safe wrapper to ensure readable default font size.
    # Uses table layout and inline styles for Gmail/Outlook compatibility.
    safe = sig_html or ""
    return (
        "<!doctype sig_html>"
        "<sig_html><head><meta charset=\"utf-8\"></head>"
        "<body style=\"margin:0;padding:0;background:#ffffff;\">"
        "<table role=\"presentation\" width=\"100%\" cellpadding=\"0\" cellspacing=\"0\">"
        "<tr><td style=\"font-family: Arial, sans-serif; font-size:16px; line-height:1.5; color:#222222;\">"
        + safe +
        "</td></tr></table>"
        "</body></sig_html>"
    )
def _o3_merge(text, data: dict) -> str:
    import re as _re
    t = str(text or "")
    def rep(m):
        k = m.group(1).strip()
        return str(data.get(k, ""))
    t = _re.sub(r"\{\{\s*([a-zA-Z0-9_]+)\s*\}\}", rep, t)
    return t

def _o3_load_vendors_df(conn):
    import pandas as _pd
    try:
        df_v = _pd.__p_read_sql_query(
            "SELECT v.id as vendor_id, v.name as company, v.city, v.state, v.naics, v.phone, v.email, v.website FROM vendors v ORDER BY v.name;",
            conn, params=()
        )
    except Exception:
        df_v = _pd.DataFrame()
    try:
        df_c = _pd.__p_read_sql_query(
            "SELECT vc.vendor_id, vc.name as contact_name, vc.email as contact_email, vc.phone as contact_phone, vc.role FROM vendor_contacts vc ORDER BY vc.id DESC;",
            conn, params=()
        )
    except Exception:
        df_c = _pd.DataFrame()
    if df_v is None: df_v = _pd.DataFrame()
    if df_c is None: df_c = _pd.DataFrame()
    if not df_c.empty:
        df = df_c.merge(df_v, on="vendor_id", how="left")
        df["email"] = df["contact_email"].fillna(df.get("email"))
        df["name"] = df["contact_name"].fillna("")
        df["phone"] = df["contact_phone"].fillna(df.get("phone"))
    else:
        df = df_v.copy()
        df["name"] = ""
    cols = ["email","name","company","phone","naics","city","state","website"]
    for c in cols:
        if c not in df.columns:
            df[c] = ""
    df = df[cols].copy()
    # No mandatory email filter here. Sending pipeline will skip invalid emails.
    return df

def _o3_collect_recipients_ui(conn):
    import streamlit as st

    _o3_ensure_schema(conn)
    st.subheader("Recipients")
    tabs = st.tabs(["From Vendors","Upload CSV","Manual"])
    all_rows = _pd.DataFrame(columns=["email","name","company","phone","naics","city","state","website"])
    with tabs[0]:
        df = _o3_load_vendors_df(conn)
        f1, f2 = st.columns([2,2])
        with f1:
            f_naics = st.text_input("Filter NAICS contains", key="o3_naics")
        with f2:
            f_state = st.text_input("Filter State equals (e.g., TX)", key="o3_state")
        if f_naics:
            df = df[df["naics"].fillna("").str.contains(f_naics, case=False, na=False)]
        if f_state:
            df = df[df["state"].fillna("").str.upper()==f_state.strip().upper()]
        st.caption(f"{len(df)} vendor rows")
        _styled_dataframe(df, use_container_width=True, hide_index=True)
        if st.button("Add all filtered vendors", key="o3_add_vendors"):
            all_rows = _pd.concat([all_rows, df], ignore_index=True)
    with tabs[1]:
        st.caption("CSV columns: email,name,company,phone,naics,city,state,website,first_name,last_name,title,solicitation,due,notice_id")
        up = st.file_uploader("Upload CSV", type=["csv"], key="o3_csv_up")
        if up:
            try:
                dfu = _pd.read_csv(up)
                _styled_dataframe(dfu.head(50), use_container_width=True, hide_index=True)
                if st.button("Add uploaded rows", key="o3_add_csv"):
                    all_rows = _pd.concat([all_rows, dfu], ignore_index=True)
            except Exception as e:
                st.error(f"CSV read error: {e}")
    with tabs[2]:
        st.caption("Paste one email per line or 'email, name, company'")
        txt = st.text_area("Lines", height=150, key="o3_paste")
        if st.button("Add pasted", key="o3_add_paste"):
            rows = []
            for line in (txt or "").splitlines():
                parts = [p.strip() for p in line.split(",")]
                if not parts: continue
                em = parts[0] if "@" in parts[0] else ""
                if not em: continue
                name = parts[1] if len(parts)>1 else ""
                comp = parts[2] if len(parts)>2 else ""
                rows.append({"email": em, "name": name, "company": comp, "phone":"", "naics":"", "city":"", "state":"", "website":""})
            if rows:
                all_rows = _pd.concat([all_rows, _pd.DataFrame(rows)], ignore_index=True)
    cur = st.session_state.get("o3_rows")
    if cur is not None and isinstance(cur, _pd.DataFrame) and not cur.empty:
        all_rows = _pd.concat([cur, all_rows], ignore_index=True)
    if not all_rows.empty:
        all_rows = all_rows.dropna(subset=["email"])
        all_rows = all_rows[all_rows["email"].astype(str).str.contains("@", na=False)]
        all_rows = all_rows.drop_duplicates(subset=["email"])
    st.session_state["o3_rows"] = all_rows
    if not all_rows.empty:
        st.write(f"Total recipients: {len(all_rows)}")
        _styled_dataframe(all_rows, use_container_width=True, hide_index=True)
        csv_bytes = all_rows.to_csv(index=False).encode("utf-8")
        st.download_button("Download recipients CSV", data=csv_bytes, file_name="o3_recipients.csv", mime="text/csv", key="o3_dl_recip")
    return all_rows

def _o3_sender_accounts_from_secrets():
    try:

        accs = []
        try:
            for row in (st.secrets.get("gmail_accounts") or []):
                if row.get('email') and row.get('app_password'):
                    accs.append({"email":row["email"],"app_password":row["app_password"],"name":row.get("name","")})
        except Exception:
            pass
        if not accs:
            g = st.secrets.get("gmail") or {}
            if g.get("email") and g.get("app_password"):
                accs.append({"email":g["email"],"app_password":g["app_password"],"name":g.get("name","")})
        return accs
    except Exception:
        return []

# --- O3 SMTP shim (fallback) ---
try:
    import _o3smtp  # if provided elsewhere
except Exception:
    class _o3smtp:
        import smtplib as _smtplib
        SMTP_SSL = _smtplib.SMTP_SSL
        SMTP = _smtplib.SMTP

def _o3_send_batch(conn, sender, rows, subject_tpl, html_tpl, test_only=False, max_send=500, attachments=None):
    """
    Send a batch of outreach emails and log per-recipient status.

    Enhancements:
    - Logs invalid and opt-out addresses instead of silently skipping.
    - Writes one row per recipient into outreach_log.
    - Adds basic tracking columns (opened_at, bounced_at) that can be updated later.
    - Injects a lightweight open-tracking pixel when an O6 base URL is configured.
    """
    import streamlit as st
    from contextlib import closing as _closing
    import datetime as _dt
    import uuid as _uuid

    try:
        from email.mime.multipart import MIMEMultipart as _O3MIMEMultipart
        from email.mime.text import MIMEText as _O3MIMEText
        from email.mime.image import MIMEImage as _O3MIMEImage
        import smtplib as _o3smtp
    except Exception:
        st.error("Email libraries not available in this environment.")
        return 0, []

    # Ensure schemas
    _o3_ensure_schema(conn)
    try:
        ensure_o6_schema(conn)
    except Exception:
        # Older builds may not have O6 helpers wired yet; ignore.
        pass

    if rows is None or getattr(rows, "empty", True):
        st.error("No recipients")
        return 0, []

    import pandas as _pd  # local alias

    # Blast metadata
    from datetime import datetime as _dt_datetime
    default_title = f"Outreach {_dt_datetime.utcnow().strftime('%Y-%m-%d %H:%M')}"
    blast_title = st.text_input("Blast name", value=default_title, key="o3_blast_name")
    if not blast_title:
        blast_title = "Outreach"

    template_name = st.session_state.get("tpl_sel", "")
    if not isinstance(template_name, str):
        template_name = ""

    with _o3c(conn.cursor()) as cur:
        cur.execute(
            "INSERT INTO outreach_blasts(title, template_name, sender_email, owner_user) VALUES(?,?,?,?);",
            (blast_title, template_name, (sender.get("email") or sender.get("username") or "").strip(), get_current_user_name()),
        )
        conn.commit()
        blast_id = cur.lastrowid

    # Load opt-outs
    try:
        opt_df = _pd.__p_read_sql_query("SELECT email FROM outreach_optouts;", conn)
        blocked = {str(e).strip().lower() for e in opt_df.get("email", []) if str(e).strip()}
    except Exception:
        blocked = set()

    # Also honor contact-level unsubscribe_all flag if present
    try:
        contact_df = _pd.__p_read_sql_query(
            "SELECT email FROM contacts WHERE unsubscribe_all = 1;",
            conn,
        )
        blocked |= {
            str(e).strip().lower()
            for e in contact_df.get("email", [])
            if str(e).strip()
        }
    except Exception:
        # Best-effort; if contacts table or column is missing, skip
        pass

# Configure SMTP
    sent = 0
    logs = []
    smtp = None
    if not test_only:
        host = sender.get("host") or sender.get("smtp_host") or "smtp.gmail.com"
        port = int(sender.get("port") or sender.get("smtp_port") or 587)
        use_tls = bool(sender.get("use_tls", True) or port == 587)
        try:
            if use_tls:
                smtp = _o3smtp.SMTP(host, port, timeout=20)
                smtp.ehlo()
                try:
                    smtp.starttls()
                except Exception:
                    pass
                smtp.login(sender["email"], sender.get("app_password") or sender.get("password") or "")
            else:
                smtp = _o3smtp.SMTP_SSL(host, port, timeout=20)
                smtp.login(sender["email"], sender.get("app_password") or sender.get("password") or "")
        except Exception as e:
            st.error(f"SMTP login failed: {e}")
            return 0, []

    # Helper for template merge
    def _render_merge(tpl: str, data: dict) -> str:
        try:
            return _o3_merge(tpl or "", data or {})
        except Exception:
            return tpl or ""

    # Open tracking base URL, if configured
    try:
        base_url = o6_get_base_url(conn) or ""
    except Exception:
        base_url = ""

    # Limit rows
    try:
        max_n = int(max_send or 0)
    except Exception:
        max_n = 0
    if max_n > 0:
        rows_iter = rows.head(max_n).iterrows()
    else:
        rows_iter = rows.iterrows()

    for _, r in rows_iter:
        to_email = str(r.get("email", "") or "").strip()
        data = {str(k): ("" if r.get(k) is None else str(r.get(k))) for k in getattr(r, "index", [])}

        # Normalize some common merge keys
        if "name" not in data:
            data["name"] = str(r.get("name") or r.get("Name") or "")
        to_name = data.get("name", "")

        subj = _render_merge(subject_tpl or "", data)

        # Handle invalid address
        if not to_email or "@" not in to_email:
            status = "Skipped: invalid email"
            err = "Missing or malformed address"
            with _closing(conn.cursor()) as cur:
                cur.execute(
                    "INSERT INTO outreach_log(blast_id,to_email,to_name,subject,status,error,owner_user) VALUES (?,?,?,?,?,?,?);",
                    (blast_id, to_email, to_name, subj, status, err, get_current_user_name()),
                )
                conn.commit()
            logs.append({"email": to_email, "status": status, "error": err})
            continue

        # Handle opt-out suppression
        if to_email.lower() in blocked:
            status = "Skipped: opt-out"
            err = ""
            with _closing(conn.cursor()) as cur:
                cur.execute(
                    "INSERT INTO outreach_log(blast_id,to_email,to_name,subject,status,error,owner_user) VALUES (?,?,?,?,?,?,?);",
                    (blast_id, to_email, to_name, subj, status, err, get_current_user_name()),
                )
                conn.commit()
            logs.append({"email": to_email, "status": status, "error": err})
            continue

        # Merge body
        sig_html = _render_merge(html_tpl or "", data)

        # Basic unsubscribe safety net if template forgot macro
        if "unsubscribe" not in (sig_html or "").lower():
            sig_html += "<br><br><small>To unsubscribe, reply 'STOP' to this email.</small>"

        log_id = None
        open_code = None

        # Pre-create log row so we can attach tracking info
        if not test_only:
            with _closing(conn.cursor()) as cur:
                # opened_at and bounced_at may not exist on older DBs
                try:
                    cur.execute(
                        "INSERT INTO outreach_log(blast_id,to_email,to_name,subject,status,error,opened_at,bounced_at) "
                        "VALUES (?,?,?,?,?,?,NULL,NULL);",
                        (blast_id, to_email, to_name, subj, "Queued", ""),
                    )
                except Exception:
                    cur.execute(
                        "INSERT INTO outreach_log(blast_id,to_email,to_name,subject,status,error,owner_user) "
                        "VALUES (?,?,?,?,?,?,?);",
                        (blast_id, to_email, to_name, subj, "Queued", "", get_current_user_name()),
                    )
                log_id = cur.lastrowid
                conn.commit()

            # Create open tracking code if base URL is available
            if base_url and log_id:
                try:
                    with conn:
                        conn.execute(
                            "CREATE TABLE IF NOT EXISTS outreach_open_codes("
                            "code TEXT PRIMARY KEY, "
                            "log_id INTEGER, "
                            "created_at TEXT DEFAULT CURRENT_TIMESTAMP, "
                            "opened_at TEXT, "
                            "FOREIGN KEY(log_id) REFERENCES outreach_log(id)"
                            ");"
                        )
                        open_code = _uuid.uuid4().hex
                        conn.execute(
                            "INSERT OR IGNORE INTO outreach_open_codes(code,log_id) VALUES(?,?);",
                            (open_code, int(log_id)),
                        )
                except Exception:
                    open_code = None

        # Build HTML with optional tracking pixel
        html_to_send = sig_html or ""
        if open_code and base_url:
            track_url = base_url.rstrip("/") + f"/?open={open_code}"
            html_to_send += (
                f'<img src="{track_url}" alt="" width="1" height="1" '
                f'style="display:none;border:0;outline:none;" />'
            )

        if test_only:
            status = "Preview"
            err = ""
        else:
            bounce_ts = None
            try:
                msg = _O3MIMEMultipart("alternative")
                msg["From"] = f"{sender.get('name') or sender['email']} <{sender['email']}>"
                msg["To"] = to_email
                msg["Subject"] = subj

                # Apply saved signature for the active sender
                try:
                    _sig_conn = (get_o4_conn() if "get_o4_conn" in globals() else None) or conn
                    html_to_send, inline_imgs = __p_render_signature(
                        _sig_conn,
                        (sender.get("email") or sender.get("username") or "").strip(),
                        html_to_send,
                    )
                except Exception:
                    pass

                # Wrap links in tracking redirect URLs (click tracking)
                try:
                    if open_code and base_url and "o6_wrap_click_links" in globals():
                        html_to_send = o6_wrap_click_links(html_to_send, base_url, open_code)
                except Exception:
                    pass

                wrapped_html = _o3_wrap_email_html(html_to_send)
                msg.attach(_O3MIMEText(wrapped_html, "html", "utf-8"))

                # Attach inline signature images (for example, the sender logo)
                try:
                    if 'inline_imgs' in locals() and inline_imgs:
                        for _img in inline_imgs:
                            try:
                                _content = _img.get("content") or b""
                                if not _content:
                                    continue
                                part = _O3MIMEImage(_content)
                                _cid = _img.get("cid") or "siglogo"
                                part.add_header("Content-ID", f"<{_cid}>")
                                part.add_header("Content-Disposition", "inline", filename=_cid)
                                msg.attach(part)
                            except Exception:
                                # Ignore logo-specific issues so the rest of the batch can send.
                                pass
                except Exception:
                    pass


                # Attach files if provided
                try:
                    if attachments:
                        import mimetypes as _mt
                        from email.mime.base import MIMEBase as _MBase
                        from email.mime.application import MIMEApplication as _MApp
                        from email.mime.image import MIMEImage as _MImg
                        from email.mime.text import MIMEText as _MTxt
                        from email import encoders as _enc
                        import os as _os

                        safe_exts = {
                            ".pdf", ".doc", ".docx", ".txt", ".rtf",
                            ".xls", ".xlsx", ".csv",
                            ".png", ".jpg", ".jpeg", ".gif"
                        }

                        for _ap in attachments:
                            try:
                                if not _ap:
                                    continue
                                _ext = (_os.path.splitext(_ap)[1] or "").lower()
                                if _ext and _ext not in safe_exts:
                                    # Skip potentially unsafe attachment types
                                    continue

                                _ctype, _encd = _mt.guess_type(_ap)
                                if not _ctype:
                                    _ctype = "application/octet-stream"
                                _maintype, _subtype = _ctype.split("/", 1)

                                with open(_ap, "rb") as _f:
                                    _data = _f.read()

                                if _maintype == "text":
                                    try:
                                        part = _MTxt(_data.decode("utf-8", "ignore"), _subtype or "plain", "utf-8")
                                    except Exception:
                                        part = _MBase(_maintype, _subtype)
                                        part.set_payload(_data)
                                        _enc.encode_base64(part)
                                elif _maintype == "image":
                                    part = _MImg(_data, _subtype or None)
                                elif _maintype == "application":
                                    part = _MApp(_data, _subtype or "octet-stream")
                                else:
                                    part = _MBase(_maintype, _subtype)
                                    part.set_payload(_data)
                                    _enc.encode_base64(part)

                                part.add_header(
                                    "Content-Disposition",
                                    f'attachment; filename="{_os.path.basename(_ap)}"',
                                )
                                msg.attach(part)
                            except Exception:
                                # Ignore attachment-specific issues so the rest of the batch can send.
                                pass
                except Exception:
                    # Attachment issues should not block the overall send.
                    pass

                smtp.sendmail(sender["email"], [to_email], msg.as_string())
                status = "Sent"
                err = ""
                sent += 1
            except Exception as e:
                err = str(e)[:500]
                low = err.lower()
                # Basic heuristic for "kicked back" or hard bounce errors
                if any(
                    phrase in low
                    for phrase in (
                        "user unknown",
                        "no such user",
                        "mailbox unavailable",
                        "recipient address rejected",
                        "550 ",
                    )
                ):
                    status = "Bounced"
                    bounce_ts = _dt.datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ")
                else:
                    status = "Error"

            # Update log row with final status
            if log_id is not None:
                try:
                    with _closing(conn.cursor()) as cur:
                        if bounce_ts:
                            try:
                                cur.execute(
                                    "UPDATE outreach_log "
                                    "SET status=?, error=?, bounced_at=COALESCE(bounced_at, ?) "
                                    "WHERE id=?;",
                                    (status, err, bounce_ts, int(log_id)),
                                )
                            except Exception:
                                cur.execute(
                                    "UPDATE outreach_log SET status=?, error=? WHERE id=?;",
                                    (status, err, int(log_id)),
                                )
                        else:
                            cur.execute(
                                "UPDATE outreach_log SET status=?, error=? WHERE id=?;",
                                (status, err, int(log_id)),
                            )
                        conn.commit()
                except Exception:
                    pass

        logs.append({"email": to_email, "status": status, "error": err})

    if smtp is not None:
        try:
            smtp.quit()
        except Exception:
            pass

    try:
        df = _pd.DataFrame(logs)
        st.download_button(
            "Download send log CSV",
            data=df.to_csv(index=False).encode("utf-8"),
            file_name=f"o3_send_log_{blast_id}.csv",
            mime="text/csv",
            key=f"o3_log_{blast_id}",
        )
    except Exception:
        pass

    st.success(f"Batch complete. Sent={sent}, Total processed={len(logs)}")
    return sent, logs

def _export_past_perf_docx(path: str, records: list) -> Optional[str]:
    try:
        import docx  # type: ignore
        from docx.shared import Inches  # type: ignore
    except Exception:
        pass

        st.error("python-docx is required. pip install python-docx")
        return None
    try:
        doc = docx.Document()
        for s in doc.sections:
            s.top_margin = Inches(1); s.bottom_margin = Inches(1); s.left_margin = Inches(1); s.right_margin = Inches(1)
        doc.add_heading("Past Performance", level=1)
        for rec in records or []:
            title = str(rec.get("title") or rec.get("project") or "Project").strip()
            doc.add_heading(title, level=2)
            for k in ["customer","period","value","cpars_rating"]:
                v = rec.get(k)
                if v:
                    doc.add_paragraph(f"**{k.title()}:** {v}")
            body = str(rec.get("summary") or rec.get("description") or rec.get("results") or "").strip()
            if body:
                for para in body.split("\n\n"):
                    if para.strip():
                        doc.add_paragraph(para.strip())
        doc.save(path)
        return path
    except Exception as e:
        pass

        st.error(f"Past Performance export failed: {e}")
        return None

# === O4: Multi-sender accounts + opt-outs + audit UI ================================
def _o4_accounts_ui(conn):
    import streamlit as st

    import streamlit as st

    ensure_outreach_o1_schema(conn)
    rows = conn.execute("SELECT user_email, display_name, smtp_host, smtp_port, use_ssl FROM email_accounts ORDER BY user_email").fetchall()
    if rows:
        _styled_dataframe(_pd.DataFrame(rows, columns=["Email","Display name","SMTP host","SMTP port","SSL"]), use_container_width=True, hide_index=True)
    st.markdown("**Add or update account**")
    c1,c2 = st.columns([3,2])
    with c1:
        email = st.text_input("Email", key="o4_ac_email")
        display = st.text_input("Display name", key="o4_ac_display")
        app_pw = st.text_input("Gmail App password", type="password", key="o4_ac_pw")
    with c2:
        host = st.text_input("SMTP host", value="smtp.gmail.com", key="o4_ac_host")
        port = st.number_input("SMTP port", min_value=1, max_value=65535, value=465, step=1, key="o4_ac_port")
        ssl = st.checkbox("Use SSL", value=True, key="o4_ac_ssl")
    c3, c4 = st.columns(2)
    with c3:
        if st.button("Save account", key="o4_ac_save"):
            if not email:
                st.error("Email required")
            else:
                with conn:
                    conn.execute("""
                    INSERT INTO email_accounts(user_email, display_name, app_password, smtp_host, smtp_port, use_ssl)
                    VALUES (?, ?, ?, ?, ?,?,?)
                    ON CONFLICT(user_email) DO UPDATE SET
                        display_name=excluded.display_name,
                        app_password=excluded.app_password,
                        smtp_host=excluded.smtp_host,
                        smtp_port=excluded.smtp_port,
                        use_ssl=excluded.use_ssl
                    """, (email.strip(), display or "", app_pw or "", host or "smtp.gmail.com", int(port or 465), 1 if ssl else 0))
                st.success("Saved")
    try:

        st.session_state["o4_sender_sel"] = email.strip()
    except Exception:
        pass
    st.rerun()
    with c4:
        if st.button("Delete account", key="o4_ac_del"):
            if not email:
                st.error("Enter the Email of the account to delete")
            else:
                with conn:
                    conn.execute("DELETE FROM email_accounts WHERE user_email=?", (email.strip(),))
                st.success("Deleted")

def _o3_render_sender_picker():
    import streamlit as st

    # Uses the unified sender discovery and lets you pick from
    # all saved accounts (including multiple Gmail senders).
    conn = get_o4_conn() if "get_o4_conn" in globals() else globals().get("_O4_CONN")
    if conn is None:
        st.warning("No sender accounts configured.")
        return {"email": "", "app_password": ""}

    ensure_outreach_o1_schema(conn)

    try:
        rows = _get_senders(conn) or []
    except Exception:
        rows = []

    if not rows:
        st.info("No sender accounts found yet. Add one under Outreach → Sender accounts, then select it here.")
        return {"email": "", "app_password": ""}

    # rows: list of (email, display_name, app_password)
    labels = []
    for email, name, _pw in rows:
        email = email or ""
        name = name or ""
        if name and name != email:
            labels.append(f"{name} — {email}")
        else:
            labels.append(email)

    # Remember last chosen email across reruns
    default_email = st.session_state.get("o4_sender_sel_email") or rows[0][0]
    try:
        default_idx = next(i for i, (e, _n, _p) in enumerate(rows) if e == default_email)
    except StopIteration:
        default_idx = 0

    sel_label = st.selectbox("From account", labels, index=default_idx, key="o4_sender_sel")
    sel_idx = labels.index(sel_label)
    email, name, _pw = rows[sel_idx]
    st.session_state["o4_sender_sel_email"] = email

    # Base sender payload
    chosen = {
        "email": email,
        "name": name or email,
        "display_name": name or "",
        "app_password": "",
        "smtp_host": "smtp.gmail.com",
        "smtp_port": 587,
        "use_tls": 1,
        "use_ssl": 0,
    }

    # Prefer modern outreach_sender_accounts settings
    try:
        row = conn.execute(
            "SELECT app_password, smtp_host, smtp_port, use_tls "
            "FROM outreach_sender_accounts WHERE email=?",
            (email,),
        ).fetchone()
        if row:
            chosen["app_password"] = row[0] or chosen["app_password"]
            chosen["smtp_host"] = row[1] or chosen["smtp_host"]
            try:
                chosen["smtp_port"] = int(row[2] or chosen["smtp_port"])
            except Exception:
                pass
            try:
                chosen["use_tls"] = int(row[3] if row[3] is not None else chosen.get("use_tls", 1))
            except Exception:
                pass
    except Exception:
        pass

    # Fallback to email_accounts if present
    try:
        row = conn.execute(
            "SELECT app_password, smtp_host, smtp_port, use_ssl, use_tls "
            "FROM email_accounts WHERE user_email=?",
            (email,),
        ).fetchone()
        if row:
            if not chosen.get("app_password"):
                chosen["app_password"] = row[0] or chosen["app_password"]
            if not chosen.get("smtp_host") or chosen["smtp_host"] == "smtp.gmail.com":
                chosen["smtp_host"] = row[1] or chosen["smtp_host"]
            try:
                chosen["smtp_port"] = int(row[2] or chosen["smtp_port"])
            except Exception:
                pass
            try:
                chosen["use_ssl"] = int(row[3] if row[3] is not None else chosen.get("use_ssl", 0))
            except Exception:
                pass
            try:
                chosen["use_tls"] = int(row[4] if row[4] is not None else chosen.get("use_tls", 1))
            except Exception:
                pass
    except Exception:
        pass

    # Backwards compatibility: smtp_settings table
    try:
        row = conn.execute(
            "SELECT password, host, port, use_tls "
            "FROM smtp_settings WHERE username=? OR label=?",
            (email, name or email),
        ).fetchone()
        if row:
            if not chosen.get("app_password"):
                chosen["app_password"] = row[0] or chosen["app_password"]
            if not chosen.get("smtp_host") or chosen["smtp_host"] == "smtp.gmail.com":
                chosen["smtp_host"] = row[1] or chosen["smtp_host"]
            try:
                chosen["smtp_port"] = int(row[2] or chosen["smtp_port"])
            except Exception:
                pass
            try:
                chosen["use_tls"] = int(row[3] if row[3] is not None else chosen.get("use_tls", 1))
            except Exception:
                pass
    except Exception:
        pass

    st.caption(f"Using {chosen['email']} via {chosen.get('smtp_host')}:{chosen.get('smtp_port')}")
    return chosen

def _o4_optout_ui(conn):
    import streamlit as st

    # tables already created by O3; ensure again for safety
    with conn:
        conn.execute("CREATE TABLE IF NOT EXISTS outreach_optouts(id INTEGER PRIMARY KEY, email TEXT UNIQUE)")
    st.markdown("**Opt-outs**")
    em = st.text_input("Add single email to opt-out", key="o4_opt_one")
    if st.button("Add opt-out", key="o4_opt_add") and em:
        with conn:
            conn.execute("INSERT OR IGNORE INTO outreach_optouts(email) VALUES(?)", (em.strip().lower(),))
        st.success("Added")
        st.rerun()
    up = st.file_uploader("Bulk upload CSV with 'email' column", type=["csv"], key="o4_opt_csv")
    if up is not None:
        try:
            df = _pd.read_csv(up)
            emails = [str(x).strip().lower() for x in df.get("email", []) if str(x).strip()]
            with conn:
                conn.executemany("INSERT OR IGNORE INTO outreach_optouts(email) VALUES(?)", [(e,) for e in emails])
            st.success(f"Imported {len(emails)} emails")
        except Exception as e:
            st.error(f"CSV error: {e}")
    try:
        df2 = _pd.__p_read_sql_query("SELECT email FROM outreach_optouts ORDER BY email LIMIT 500", conn)
        _styled_dataframe(df2, use_container_width=True, hide_index=True)
    except Exception:
        pass

def _o4_audit_ui(conn):
    import streamlit as st
    import pandas as _pd

    # Recent blast-level activity
    try:
        blasts = _pd.__p_read_sql_query(
            "SELECT id, title, sender_email, template_name, created_at "
            "FROM outreach_blasts ORDER BY id DESC LIMIT 50;",
            conn,
        )
        st.markdown("**Recent blasts**")
        _styled_dataframe(blasts, use_container_width=True, hide_index=True)
    except Exception:
        st.caption("No blasts yet")

    # Recent per-recipient activity (including tracking columns when available)
    try:
        try:
            logs = _pd.__p_read_sql_query(
                "SELECT created_at, to_email, status, opened_at, bounced_at, subject, error "
                "FROM outreach_log ORDER BY id DESC LIMIT 200;",
                conn,
            )
        except Exception:
            # Fallback for databases created before opened_at and bounced_at existed
            logs = _pd.__p_read_sql_query(
                "SELECT created_at, to_email, status, subject, error "
                "FROM outreach_log ORDER BY id DESC LIMIT 200;",
                conn,
            )
        st.markdown("**Recent sends**")
        _styled_dataframe(logs, use_container_width=True, hide_index=True)
    except Exception:
        st.caption("No logs yet")

def _o5_now_iso():
    return __import__("datetime").datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ")

def ensure_o5_schema(conn: "sqlite3.Connection") -> None:
    with conn:
        conn.execute("""CREATE TABLE IF NOT EXISTS outreach_sequences(
            id INTEGER PRIMARY KEY, name TEXT UNIQUE NOT NULL
        );""")
        conn.execute("""CREATE TABLE IF NOT EXISTS outreach_steps(
            id INTEGER PRIMARY KEY, seq_id INTEGER NOT NULL,
            step_no INTEGER NOT NULL, delay_hours INTEGER NOT NULL DEFAULT 72,
            subject TEXT DEFAULT '', body_html TEXT DEFAULT '',
            FOREIGN KEY(seq_id) REFERENCES outreach_sequences(id)
        );""")
        conn.execute("""CREATE TABLE IF NOT EXISTS outreach_schedules(
            id INTEGER PRIMARY KEY, seq_id INTEGER NOT NULL, step_no INTEGER NOT NULL,
            to_email TEXT NOT NULL, vendor_id INTEGER,
            send_at TEXT NOT NULL, status TEXT NOT NULL DEFAULT 'queued',
            last_error TEXT DEFAULT '',
            subject TEXT DEFAULT '', body_html TEXT DEFAULT '',
            created_at TEXT DEFAULT CURRENT_TIMESTAMP,
            FOREIGN KEY(seq_id) REFERENCES outreach_sequences(id)
        );""")
        conn.execute("""CREATE TABLE IF NOT EXISTS outreach_contact_sequences(
            id INTEGER PRIMARY KEY,
            contact_id INTEGER NOT NULL,
            seq_id INTEGER NOT NULL,
            status TEXT NOT NULL DEFAULT 'active',
            current_step_no INTEGER NOT NULL DEFAULT 1,
            last_step_sent_at TEXT,
            created_at TEXT DEFAULT CURRENT_TIMESTAMP,
            FOREIGN KEY(contact_id) REFERENCES contacts(id),
            FOREIGN KEY(seq_id) REFERENCES outreach_sequences(id)
        );""")


def _o5_list_sequences(conn):
    try:
        return _pd.__p_read_sql_query("SELECT id, name FROM outreach_sequences ORDER BY name;", conn)
    except Exception:
        return _pd.DataFrame(columns=["id","name"])

def _o5_list_steps(conn, seq_id: int):
    try:
        return _pd.__p_read_sql_query("SELECT id, step_no, delay_hours, subject FROM outreach_steps WHERE seq_id=? ORDER BY step_no;", conn, params=(seq_id,))
    except Exception:
        return _pd.DataFrame(columns=["id","step_no","delay_hours","subject"])

def _o5_upsert_sequence(conn, name: str):
    with conn:
        conn.execute("INSERT INTO outreach_sequences(name) VALUES(?) ON CONFLICT(name) DO NOTHING;", (name.strip(),))

def _o5_add_step(conn, seq_id: int, step_no: int, delay_hours: int, subject: str, body_html: str):
    with conn:
        conn.execute(
            "INSERT INTO outreach_steps(seq_id, step_no, delay_hours, subject, body_html, owner_user) "
            "VALUES (?, ?, ?, ?, ?, ?)", 
            (seq_id, step_no, int(delay_hours), subject or "", body_html or "", get_current_user_name()),
        )

def _o5_queue_followups(conn, seq_id: int, emails: list[str], start_at_iso: str | None = None):
    if not start_at_iso:
        start_at_iso = _o5_now_iso()
    steps = _pd.__p_read_sql_query("SELECT step_no, delay_hours, subject, body_html FROM outreach_steps WHERE seq_id=? ORDER BY step_no;", conn, params=(seq_id,))
    if steps is None or steps.empty:
        return 0
    count = 0
    with conn:
        for em in emails:
            em = (em or "").strip().lower()
            if not em:
                continue
            base = __import__("datetime").datetime.fromisoformat(start_at_iso.replace("Z","+00:00"))
            for _, row in steps.iterrows():
                eta = base + __import__("datetime").timedelta(hours=int(row["delay_hours"] or 0))
                conn.execute("""INSERT INTO outreach_schedules(seq_id, step_no, to_email, send_at, status, subject, body_html)
                                VALUES (?, ?, ?, ?, ?, ?, ?)""",                             (seq_id, int(row["step_no"]), em, eta.strftime("%Y-%m-%dT%H:%M:%SZ"), "queued", row["subject"] or "", row["body_html"] or ""))
                base = eta
                count += 1
    return count

def _o5_pick_sender_from_session():
    host = st.session_state.get("smtp_host") or (st.session_state.get("smtp_profile") or {}).get("host")
    port = st.session_state.get("smtp_port", 587)
    tls = bool(st.session_state.get("smtp_tls", True))
    username = st.session_state.get("smtp_username", "")
    password = st.session_state.get("smtp_password", "")
    return {"host": host or "smtp.gmail.com", "port": int(port or 587), "tls": tls, "username": username, "password": password}

def _o5_smtp_send(sender: dict, to_email: str, subject: str, sig_html: str):
    # Apply signature HTML inside the function scope
    try:
        _sig_conn = (get_o4_conn() if 'get_o4_conn' in globals() else None)
        sig_html, _imgs = __p_render_signature(_sig_conn or None, (sender.get('email') or sender.get('username') or '').strip(), sig_html)
    except Exception:
        pass
    msg = MIMEMultipart("alternative"); msg["Subject"] = subject or ""; msg["From"] = sender.get("username") or sender.get("email"); msg["To"] = to_email
    msg.attach(MIMEText(sig_html or "", "sig_html"))
    if sender.get('tls'):
        server = smtplib.SMTP(sender["host"], sender["port"]); server.starttls(); server.login(sender.get("username") or sender.get("email"), sender.get("password") or sender.get("app_password",""))
    else:
        server = smtplib.SMTP_SSL(sender["host"], sender["port"], context=ssl.create_default_context()); server.login(sender.get("username") or sender.get("email"), sender.get("password") or sender.get("app_password",""))
    server.sendmail(sender.get("username") or sender.get("email"), [to_email], msg.as_string()); server.quit()
def _o5_send_due(conn, limit: int = 200):
    now = _o5_now_iso()
    df = _pd.__p_read_sql_query("SELECT id, to_email, subject, body_html FROM outreach_schedules WHERE status='queued' AND send_at<=? ORDER BY send_at LIMIT ?;", conn, params=(now, int(limit)))
    if df is None or df.empty: return 0, 0
    ok = 0; fail = 0; sender = _o5_pick_sender_from_session()
    for _, r in df.iterrows():
        try:
            _o5_smtp_send(sender, r["to_email"], r["subject"], r["body_html"]); 
            with conn: conn.execute("UPDATE outreach_schedules SET status='sent' WHERE id=?", (int(r["id"]),)); ok += 1
        except Exception as e:
            with conn: conn.execute("UPDATE outreach_schedules SET status='error', last_error=? WHERE id=?", (str(e)[:500], int(r["id"]))); fail += 1
    return ok, fail

def render_outreach_o5_followups(conn):
    ensure_o5_schema(conn)
    st.subheader("O5 — Follow-ups & SLA")
    seq_df = _o5_list_sequences(conn); names = ["— New —"] + ([] if seq_df is None or seq_df.empty else seq_df["name"].tolist())
    c1, c2 = st.columns([2,3])
    with c1:
        sel = st.selectbox("Sequence", names, key="o5_seq_sel")
        new_name = st.text_input("New sequence name", key="o5_seq_name") if sel == "— New —" else sel
        if st.button("Save sequence", key="o5_seq_save"):
            if new_name and new_name.strip(): _o5_upsert_sequence(conn, new_name.strip()); st.success("Sequence saved"); st.rerun()
    with c2:
        if sel != "— New —" and (seq_df is not None and not seq_df.empty):
            seq_id = int(seq_df.loc[seq_df["name"]==sel, "id"].iloc[0])
            st.markdown("**Steps**"); steps = _o5_list_steps(conn, seq_id)
            if steps is not None and not steps.empty: _styled_dataframe(steps, use_container_width=True, hide_index=True)
            st.markdown("**Add step**"); s1,s2,s3 = st.columns(3)
            with s1: step_no = st.number_input("Step #", 1, 20, value=(int(steps["step_no"].max())+1 if steps is not None and not steps.empty else 1))
            with s2: delay = st.number_input("Delay hours", 0, 720, value=0)
            with s3: subj = st.text_input("Subject", key="o5_step_subj")
            body = st.text_area("HTML body", height=180, key="o5_step_body")
            if st.button("Add step", key="o5_step_add"): _o5_add_step(conn, seq_id, int(step_no), int(delay), subj, body); st.success("Step added"); st.rerun()
    st.markdown("---"); st.markdown("**Queue follow-ups**")
    if sel != "— New —" and (seq_df is not None and not seq_df.empty):
        seq_id = int(seq_df.loc[seq_df["name"]==sel, "id"].iloc[0])
    else:
        seq_id = None
    emails_txt = st.text_area("Paste recipient emails (one per line)", height=120, key="o5_emails")
    if st.button("Queue follow-ups", key="o5_queue"):
        if not seq_id: st.error("Select an existing sequence first")
        else:
            emails = [e.strip() for e in (emails_txt or "").splitlines() if e.strip()]
            n = _o5_queue_followups(conn, seq_id, emails); st.success(f"Queued {n} follow-up sends")
    st.markdown("**Send due now**")
    if st.button("Send due follow-ups", key="o5_send_due"):
        ok, fail = _o5_send_due(conn, limit=200); st.success(f"Sent {ok}, failed {fail}")
# === End O5 ================================================================

# === O6: Compliance — Unsubscribe & Suppression =============================
import uuid, urllib.parse, json

def ensure_o6_schema(conn):
    with conn:
        conn.execute("""CREATE TABLE IF NOT EXISTS outreach_optouts(
            id INTEGER PRIMARY KEY, email TEXT UNIQUE, reason TEXT, created_at TEXT DEFAULT CURRENT_TIMESTAMP
        );""")
        conn.execute("""CREATE TABLE IF NOT EXISTS outreach_unsub_codes(
            code TEXT PRIMARY KEY, email TEXT, created_at TEXT DEFAULT CURRENT_TIMESTAMP, used_at TEXT
        );""")
        conn.execute("""CREATE TABLE IF NOT EXISTS kv_store(
            k TEXT PRIMARY KEY, v TEXT
        );""")
        # Per-recipient open-tracking codes (linked back to outreach_log.id)
        conn.execute("""CREATE TABLE IF NOT EXISTS outreach_open_codes(
            code TEXT PRIMARY KEY,
            log_id INTEGER,
            created_at TEXT DEFAULT CURRENT_TIMESTAMP,
            opened_at TEXT,
            FOREIGN KEY(log_id) REFERENCES outreach_log(id)
        );""")
        conn.execute("""CREATE TABLE IF NOT EXISTS outreach_event(
            id INTEGER PRIMARY KEY,
            outreach_log_id INTEGER NOT NULL,
            type TEXT,
            created_at TEXT DEFAULT CURRENT_TIMESTAMP,
            meta_json TEXT,
            FOREIGN KEY(outreach_log_id) REFERENCES outreach_log(id)
        );""")


def _o6_bump_engagement(conn, outreach_log_id, event_type):
    """Best-effort engagement scoring for contacts and deals based on outreach events."""
    try:
        from contextlib import closing as _closing
        with _closing(conn.cursor()) as cur:
            # Discover available columns on outreach_log
            try:
                cur.execute("PRAGMA table_info(outreach_log);")
                rows = cur.fetchall() or []
                cols = [str(r[1]) for r in rows]
            except Exception:
                cols = []
            select_cols = []
            if "contact_id" in cols:
                select_cols.append("contact_id")
            if "deal_id" in cols:
                select_cols.append("deal_id")
            if "to_email" in cols:
                select_cols.append("to_email")
            if not select_cols:
                return
            sql = "SELECT " + ", ".join(select_cols) + " FROM outreach_log WHERE id=? LIMIT 1;"
            try:
                cur.execute(sql, (outreach_log_id,))
                row = cur.fetchone()
            except Exception:
                row = None
            if not row:
                return
            idx = 0
            contact_id = None
            deal_id = None
            email = None
            if "contact_id" in cols:
                contact_id = row[idx]
                idx += 1
            if "deal_id" in cols:
                deal_id = row[idx]
                idx += 1
            if "to_email" in cols and idx < len(row):
                email = row[idx]

            et = (event_type or "").lower()
            if et == "open":
                delta = 1.0
            elif et == "click":
                delta = 3.0
            else:
                delta = 0.0
            if not delta:
                return

            # Update contacts
            try:
                if contact_id is not None:
                    cur.execute(
                        "UPDATE contacts "
                        "SET engagement_score=COALESCE(engagement_score, 0)+?, "
                        "last_engagement_at=CURRENT_TIMESTAMP "
                        "WHERE id=?;",
                        (delta, int(contact_id)),
                    )
                elif email:
                    cur.execute(
                        "UPDATE contacts "
                        "SET engagement_score=COALESCE(engagement_score, 0)+?, "
                        "last_engagement_at=CURRENT_TIMESTAMP "
                        "WHERE LOWER(email)=LOWER(?);",
                        (delta, email),
                    )
            except Exception:
                pass

            # Update deals
            try:
                if deal_id is not None:
                    cur.execute(
                        "UPDATE deals "
                        "SET engagement_score=COALESCE(engagement_score, 0)+?, "
                        "last_engagement_at=CURRENT_TIMESTAMP "
                        "WHERE id=?;",
                        (delta, int(deal_id)),
                    )
            except Exception:
                pass
        try:
            conn.commit()
        except Exception:
            pass
    except Exception:
        # Tracking failures should never break the app
        pass


def o6_log_event(conn, outreach_log_id, event_type, meta=None):
    """Insert a row into outreach_event for analytical tracking.

    event_type is expected to be 'open' or 'click'.
    meta can be a dict (will be JSON-encoded) or a JSON string.
    """
    try:
        if meta is None:
            meta_json = "{}"
        elif isinstance(meta, str):
            meta_json = meta
        else:
            meta_json = json.dumps(meta)
        with conn:
            conn.execute(
                "INSERT INTO outreach_event(outreach_log_id, type, meta_json) VALUES(?,?,?)",
                (outreach_log_id, event_type, meta_json),
            )
            try:
                _o6_bump_engagement(conn, outreach_log_id, event_type)
            except Exception:
                # Engagement scoring should never break sending or tracking
                pass
    except Exception:
        # Tracking failures should never break the app
        pass




def o6_wrap_click_links(html, base_url, tracking_code):
    """Wrap http(s) links in the given HTML with click-tracking redirect URLs.

    Links that are mailto:, tel:, javascript:, existing tracking links, or
    unsubscribe links are left untouched.
    """
    if not html or not base_url or not tracking_code:
        return html
    try:
        import re as _re
        from urllib.parse import quote_plus as _quote_plus
    except Exception:
        return html

    base = str(base_url or "").rstrip("/")

    def _wrap_href(match):
        quote = match.group(1)
        url = match.group(2) or ""
        if not url:
            return match.group(0)
        lower = url.lower()

        # Skip non-http(s) schemes and internal anchors
        if not (lower.startswith("http://") or lower.startswith("https://")):
            return match.group(0)
        if lower.startswith("#"):
            return match.group(0)

        # Skip obvious unsubscribe or already-tracked links
        if "unsubscribe=" in lower:
            return match.group(0)
        if "/track/click" in lower and "token=" in lower:
            return match.group(0)

        # Skip mailto/tel/javascript just in case
        if lower.startswith("mailto:") or lower.startswith("tel:") or lower.startswith("javascript:"):
            return match.group(0)

        try:
            encoded = _quote_plus(url)
            track_url = f"{base}/track/click?token={tracking_code}&u={encoded}"
            return f"href={quote}{track_url}{quote}"
        except Exception:
            return match.group(0)

    pattern = _re.compile(r'href=(["\'])(.+?)\1', _re.IGNORECASE)
    try:
        return pattern.sub(_wrap_href, html)
    except Exception:
        return html


def o6_set_base_url(conn, url):
    with conn:
        conn.execute("INSERT INTO kv_store(k,v) VALUES('o6_base_url', ?) ON CONFLICT(k) DO UPDATE SET v=excluded.v;", (url,))

def o6_get_base_url(conn):
    try:
        cur = conn.execute("SELECT v FROM kv_store WHERE k='o6_base_url' LIMIT 1;")
        row = cur.fetchone()
        if row and row[0]:
            return row[0]
    except Exception:
        pass
    try:
        import streamlit as _st
        return _st.secrets.get("app_base_url", "")
    except Exception:
        return ""

def o6_is_suppressed(conn, email):
    try:
        em = (email or "").strip().lower()
        row = conn.execute("SELECT 1 FROM outreach_optouts WHERE lower(email)=? LIMIT 1;", (em,)).fetchone()
        return bool(row)
    except Exception:
        return False

def o6_add_optout(conn, email, reason="user_unsubscribe"):
    try:
        with conn:
            conn.execute("INSERT INTO outreach_optouts(email,reason) VALUES(?,?) ON CONFLICT(email) DO NOTHING;", ((email or "").strip().lower(), reason))
    except Exception:
        pass

def o6_new_code(conn, email):
    c = uuid.uuid4().hex
    with conn:
        conn.execute("INSERT INTO outreach_unsub_codes(code,email) VALUES(?,?)", (c,(email or "").strip().lower()))
    return c

def o6_unsub_link_for(conn, email):
    base = o6_get_base_url(conn) or ""
    if not base:
        return ""
    code = o6_new_code(conn, email)
    sep = "&" if "?" in base else "?"
    return f"{base}{sep}unsubscribe={code}"

def o6_handle_query_unsubscribe(conn):
    """Handle unsubscribe links and open-tracking callbacks from email.

    This is invoked on each app load; if the current URL has one of the
    supported query parameters, we update the appropriate tables and stop.
    """
    try:
        import streamlit as _st
        # Normalize Streamlit query params into a simple dict-of-lists
        raw_qp = dict(_st.query_params)
        qp = {}
        for k, v in raw_qp.items():
            if isinstance(v, list):
                qp[k] = v
            else:
                qp[k] = [v]

        # 1) Open tracking: ?open=<code>
        if "open" in qp:
            code_val = (qp.get("open", [None]) or [None])[0]
            if code_val:
                try:
                    row = conn.execute(
                        "SELECT log_id FROM outreach_open_codes WHERE code=? LIMIT 1;",
                        (code_val,),
                    ).fetchone()
                except Exception:
                    row = None
                if row and row[0]:
                    log_id = int(row[0])
                    try:
                        with conn:
                            conn.execute(
                                "UPDATE outreach_open_codes "
                                "SET opened_at=CURRENT_TIMESTAMP "
                                "WHERE code=?;",
                                (code_val,),
                            )
                            # opened_at column may not exist in very old DBs; ignore failures
                            try:
                                conn.execute(
                                    "UPDATE outreach_log "
                                    "SET opened_at=CURRENT_TIMESTAMP "
                                    "WHERE id=?;",
                                    (log_id,),
                                )
                            except Exception:
                                pass
                            try:
                                o6_log_event(conn, log_id, "open", {"code": code_val})
                            except Exception:
                                pass
                    except Exception:
                        pass
                    # Render a minimal confirmation so the page is not blank
                    _st.write("Thanks for reading.")
                    return True

        # 2) Click tracking: /track/click?token=...&u=...
        if "click" in qp or "token" in qp:
            code_val = (qp.get("click") or qp.get("token") or [None])[0]
            url_val = (qp.get("u") or [None])[0]
            log_id = None
            target_url = None

            if url_val:
                try:
                    from urllib.parse import unquote_plus as _unquote_plus
                    target_url = _unquote_plus(url_val)
                except Exception:
                    target_url = url_val

            if code_val:
                try:
                    row = conn.execute(
                        "SELECT log_id FROM outreach_open_codes WHERE code=? LIMIT 1;",
                        (code_val,),
                    ).fetchone()
                except Exception:
                    row = None
                if row and row[0]:
                    log_id = int(row[0])
                    try:
                        o6_log_event(conn, log_id, "click", {"code": code_val, "url": target_url})
                    except Exception:
                        pass

            # Render a redirect page so users reach the real link
            if target_url:
                if target_url.startswith("http://") or target_url.startswith("https://"):
                    _st.markdown(
                        f'<meta http-equiv="refresh" content="0; url={target_url}"/>'
                        f'<p>Redirecting&hellip; <a href="{target_url}">Continue</a></p>',
                        unsafe_allow_html=True,
                    )
                else:
                    _st.write("Click recorded.")
                return True
            elif code_val:
                _st.write("Click recorded.")
                return True

        # 3) Standard unsubscribe flow: ?unsubscribe=<code>
        if "unsubscribe" in qp:
            code = (qp.get("unsubscribe", [None]) or [None])[0]
            if code:
                row = conn.execute(
                    "SELECT email FROM outreach_unsub_codes WHERE code=? LIMIT 1;",
                    (code,),
                ).fetchone()
                if row and row[0]:
                    email = row[0]
                    o6_add_optout(conn, email, reason="link_click")
                    try:
                        with conn:
                            conn.execute(
                                "UPDATE contacts "
                                "SET unsubscribe_all = 1, "
                                "    unsubscribe_reason = COALESCE(unsubscribe_reason, ?), "
                                "    last_unsubscribed_at = CURRENT_TIMESTAMP "
                                "WHERE lower(email) = lower(?);",
                                ("link_click", email),
                            )
                    except Exception:
                        # Do not block unsubscribe flow if contacts table/columns are absent
                        pass
                    with conn:
                        conn.execute(
                            "UPDATE outreach_unsub_codes "
                            "SET used_at=CURRENT_TIMESTAMP WHERE code=?;",
                            (code,),
                        )
                    _st.success(f"{email} unsubscribed.")
                    return True
                else:
                    _st.warning("Invalid or expired unsubscribe link.")
                    return True

        # 3) Legacy direct email param: ?unsubscribe_email=<address>
        if "unsubscribe_email" in qp:
            email = (qp.get("unsubscribe_email", [None]) or [None])[0]
            if email:
                o6_add_optout(conn, email, reason="direct_param")
                try:
                    with conn:
                        conn.execute(
                            "UPDATE contacts "
                            "SET unsubscribe_all = 1, "
                            "    unsubscribe_reason = COALESCE(unsubscribe_reason, ?), "
                            "    last_unsubscribed_at = CURRENT_TIMESTAMP "
                            "WHERE lower(email) = lower(?);",
                            ("direct_param", email),
                        )
                except Exception:
                    # Ignore failures; core unsubscribe still recorded via outreach_optouts
                    pass
                _st.success(f"{email} unsubscribed.")
                return True
    except Exception:
        pass
    return False

def render_outreach_o6_compliance(conn):
    ensure_o6_schema(conn)
    import pandas as _pd
    st.subheader("O6 — Compliance")
    base = st.text_input("Unsubscribe base URL", value=o6_get_base_url(conn) or "", help="Example: https://yourapp.yourdomain/")
    if st.button("Save base URL", key="o6_save_base"):
        o6_set_base_url(conn, base.strip())
        st.success("Saved")
    st.caption("Use {{UNSUB_LINK}} macro in templates. If absent, a default unsubscribe footer will be appended.")
    # Show list
    df = _pd.__p_read_sql_query("SELECT email, reason, created_at FROM outreach_optouts ORDER BY created_at DESC LIMIT 500;", conn)
    _styled_dataframe(df, use_container_width=True, hide_index=True)

# Wrap _o3_send_batch to enforce suppression and inject unsubscribe link
def _o6_wrap_o3_send_batch():
    g = globals()
    orig = g.get("_o3_send_batch")
    if not callable(orig) or getattr(orig, "_o6_wrapped", False):
        return
    def wrapped(*args, **kwargs):
        import pandas as _pd
        # Normalize to new signature: (conn, sender, rows_df, subj, sig_html, test_only=False, max_send=500)
        conn = kwargs.get("conn")
        sender = kwargs.get("sender")
        rows = kwargs.get("rows")
        subj = kwargs.get("subj")
        sig_html = kwargs.get("sig_html")
        test_only = kwargs.get("test_only", False)
        max_send = kwargs.get("max_send", 500)
        attachments = kwargs.get("attachments")

        # Positional fallbacks
        if conn is None or sender is None or rows is None or subj is None or sig_html is None:
            if len(args) >= 5:
                conn, sender, rows, subj, sig_html = args[:5]
                if len(args) >= 6: test_only = args[5]
                if len(args) >= 7: max_send = args[6]
            elif len(args) == 2:
                # Old call style: _o3_send_batch(sender, [{to, subject, sig_html}, ])
                sender, rows_list = args
                # Build conn via get_db if available
                if conn is None and "get_db" in g:
                    try:
                        conn = g["get_db"]()
                    except Exception:
                        conn = kwargs.get("conn")
                # Convert rows_list to DataFrame
                if isinstance(rows_list, list):
                    rows = _pd.DataFrame(rows_list)
                elif hasattr(rows_list, "to_dict"):
                    rows = _pd.DataFrame(rows_list)
                else:
                    rows = _pd.DataFrame([])
                # Derive subj and sig_html defaults per-row later
                # If provided in kwargs, use them
                subj = subj or (rows.iloc[0]["subject"] if not rows.empty and "subject" in rows.columns else "")
                sig_html = sig_html or (rows.iloc[0]["sig_html"] if not rows.empty and "sig_html" in rows.columns else "")
            else:
                # Unsupported call pattern
                raise TypeError("Unsupported _o3_send_batch call signature")


        # Apply sender throttling (daily_limit, minute_limit, sent_today_count, last_sent_at)
        throttle_info = None
        try:
            if conn is not None and sender:
                _email_key = (sender.get("email") or sender.get("username") or "").strip().lower()
                if _email_key:
                    cur = conn.cursor()
                    try:
                        cur.execute(
                            "SELECT id,daily_limit,minute_limit,sent_today_count,last_sent_at "
                            "FROM outreach_sender_accounts WHERE lower(email)=? LIMIT 1;",
                            (_email_key,),
                        )
                        row = cur.fetchone()
                    finally:
                        cur.close()
                    if row:
                        import datetime as _dt
                        _sid, _dlim, _mlim, _sent_today, _last_ts = row
                        _now = _dt.datetime.utcnow()
                        _today = _now.date()
                        _sent_today = int(_sent_today or 0)
                        # Reset daily counter if last_sent_at is from a previous day
                        if _last_ts:
                            try:
                                _last_dt = _dt.datetime.fromisoformat(str(_last_ts).replace("Z", "+00:00"))
                                if _last_dt.date() != _today:
                                    _sent_today = 0
                            except Exception:
                                pass
                        _daily_limit = int(_dlim) if _dlim not in (None, "", 0) else None
                        _minute_limit = int(_mlim) if _mlim not in (None, "", 0) else None
                        _remaining_daily = None
                        if _daily_limit is not None:
                            _remaining_daily = max(_daily_limit - _sent_today, 0)
                        throttle_info = {
                            "sender_id": int(_sid),
                            "sent_today": _sent_today,
                            "daily_limit": _daily_limit,
                            "minute_limit": _minute_limit,
                            "remaining_daily": _remaining_daily,
                            "now_iso": _now.replace(microsecond=0).isoformat() + "Z",
                        }
        except Exception:
            throttle_info = None

        if not test_only and throttle_info:
            _allowed = max_send if isinstance(max_send, int) else int(max_send or 0)
            if throttle_info.get("remaining_daily") is not None:
                _allowed = min(_allowed, throttle_info["remaining_daily"])
            if throttle_info.get("minute_limit") not in (None, 0):
                _allowed = min(_allowed, int(throttle_info["minute_limit"]))
            if _allowed <= 0:
                # Nothing allowed to send for this sender right now
                return 0
            max_send = int(_allowed)
            # Optionally trim rows to the allowed window so we do not over-queue
            try:
                if rows is not None and hasattr(rows, "head"):
                    rows = rows.head(max_send)
            except Exception:
                pass

        # Ensure schema and suppression filter
        ensure_o6_schema(conn)
        if rows is not None and hasattr(rows, "copy"):
            _rows = rows.copy()
            if "email" in _rows.columns:
                mask = _rows["email"].astype(str).str.lower().apply(lambda em: not o6_is_suppressed(conn, em))
                _rows = _rows[mask]
            rows = _rows

        # If subj/sig_html empty but rows contain per-recipient fields, send one by one
        if (not subj or not sig_html) and rows is not None and not rows.empty and {"subject","sig_html"}.issubset(set(map(str.lower, rows.columns))):
            # Normalize column names case-insensitively
            columns = {c.lower(): c for c in rows.columns}
            total = 0
            for _, r in rows.iterrows():
                subj_i = str(r.get(columns.get("subject"), subj) or "")
                html_i = str(r.get(columns.get("sig_html"), sig_html) or "")
                # Build single-row frame to keep orig API
                orig(conn, sender, rows=_pd.DataFrame([r]), subj=subj_i, sig_html=html_i, test_only=test_only, max_send=1, attachments=attachments)
                total += 1
            return total

        result = orig(conn, sender, rows, subj, sig_html, test_only=test_only, max_send=max_send, attachments=attachments)
        # Persist throttle counters back to outreach_sender_accounts
        if not test_only and throttle_info and conn is not None:
            try:
                _sent_count = int(result or 0)
            except Exception:
                _sent_count = 0
            try:
                if _sent_count > 0:
                    _new_sent = int(throttle_info.get("sent_today") or 0) + _sent_count
                    with conn:
                        conn.execute(
                            "UPDATE outreach_sender_accounts "
                            "SET sent_today_count=?, last_sent_at=? "
                            "WHERE id=?;",
                            (_new_sent, throttle_info.get("now_iso"), int(throttle_info.get("sender_id"))),
                        )
            except Exception:
                pass
        return result
    wrapped._o6_wrapped = True
    g["_o3_send_batch"] = wrapped

_o6_wrap_o3_send_batch()
# === End O6 ================================================================

# === S1D: Subcontractor Finder — dedupe + Google Places pagination ===========
import json as _json, time as _time
from typing import Any, List, Dict
import requests as _requests

def _s1d_get_api_key():
    try:
        return st.secrets["google"]["api_key"]
    except Exception:
        pass
    try:
        return st.secrets["GOOGLE_API_KEY"]
    except Exception:
        pass
    return ""

def _s1d_norm_phone(p: str) -> str:
    import re as _re
    if not p: return ""
    digits = "".join(_re.findall(r"\d+", str(p)))
    if len(digits) == 11 and digits.startswith("1"):
        digits = digits[1:]
    return digits

def _s1d_existing_vendor_keys(conn):
    return _s1d_select_existing_pairs(conn)

def _s1d_geocode(addr: str, key: str):
    if not addr: return None
    try:
        r = _requests.get("https://maps.googleapis.com/maps/api/geocode/json", params={"address": addr, "key": key}, timeout=10)
        js = r.json()
        if js.get("status") == "OK" and js.get("results"):
            loc = js["results"][0]["geometry"]["location"]
            return float(loc["lat"]), float(loc["lng"])
    except Exception:
        return None
    return None

def _s1d_places_textsearch(query: str, lat: float|None, lng: float|None, radius_m: int|None, page_token: str|None, key: str):
    base = "https://maps.googleapis.com/maps/api/place/textsearch/json"
    params = {"query": query, "key": key, "region": "us"}
    if page_token:
        params = {"pagetoken": page_token, "key": key}
    else:
        if lat is not None and lng is not None and radius_m:
            params.update({"location": f"{lat},{lng}", "radius": int(radius_m)})
    r = _requests.get(base, params=params, timeout=12)
    js = r.json()
    return js

def _s1d_places_nearby(keyword, lat, lng, pagetoken, key, rankby="distance"):
    import requests as _rq_nb
    params = {"key": key}
    if pagetoken:
        params = {"pagetoken": pagetoken, "key": key}
        url = "https://maps.googleapis.com/maps/api/place/nearbysearch/json"
    else:
        url = "https://maps.googleapis.com/maps/api/place/nearbysearch/json"
        params.update({"location": f"{lat},{lng}", "rankby": rankby})
        if keyword:
            params["keyword"] = keyword
    return _rq_nb.get(url, params=params, timeout=12).json()

def _s1d_place_details(pid: str, key: str):
    try:
        r = _requests.get("https://maps.googleapis.com/maps/api/place/details/json",
                          params={"place_id": pid, "fields": "formatted_phone_number,website,url", "key": key},
                          timeout=10)
        return r.json().get("result", {}) or {}
    except Exception:
        return {}


def _s1d_save_new_vendors(conn, rows: List[Dict[str,Any]]):

    # Determine writable table and ensure schema
    tbl = _s1d_vendor_write_table(conn)
    with conn:
        _s1d_ensure_vendor_table(conn, tbl)
    # Insert or update rows keyed by place_id
    saved = 0
    with conn:
        for r in rows or []:
            cur = conn.execute(
                f"""
                INSERT INTO {tbl}(
                    source,
                    place_id,
                    name,
                    email,
                    phone,
                    website,
                    address,
                    city,
                    state,
                    zip,
                    naics,
                    notes,
                    lat,
                    lon,
                    created_at
                )
                VALUES (
                    ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, datetime('now')
                )
                ON CONFLICT(place_id) DO UPDATE SET
                    name=COALESCE(excluded.name, {tbl}.name),
                    email=COALESCE(excluded.email, {tbl}.email),
                    phone=COALESCE(excluded.phone, {tbl}.phone),
                    website=COALESCE(excluded.website, {tbl}.website),
                    address=COALESCE(excluded.address, {tbl}.address),
                    city=COALESCE(excluded.city, {tbl}.city),
                    state=COALESCE(excluded.state, {tbl}.state),
                    zip=COALESCE(excluded.zip, {tbl}.zip),
                    naics=COALESCE(excluded.naics, {tbl}.naics),
                    notes=CASE
                        WHEN length({tbl}.notes) > 0 THEN {tbl}.notes
                        ELSE COALESCE(excluded.notes, {tbl}.notes)
                    END,
                    lat=COALESCE(excluded.lat, {tbl}.lat),
                    lon=COALESCE(excluded.lon, {tbl}.lon)
                """,
                (
                    str(r.get("source", "") or ""),
                    str(r.get("place_id", "") or ""),
                    str(r.get("name", "") or ""),
                    str(r.get("email", "") or ""),
                    str(r.get("phone", "") or ""),
                    str(r.get("website", "") or ""),
                    str(r.get("address", "") or ""),
                    str(r.get("city", "") or ""),
                    str(r.get("state", "") or ""),
                    str(r.get("zip", "") or ""),
                    str(r.get("naics_guess", "") or r.get("naics", "") or ""),
                    str(r.get("notes", "") or ""),
                    float(r.get("lat") or 0) if str(r.get("lat") or "").strip() else None,
                    float(r.get("lon") or 0) if str(r.get("lon") or "").strip() else None,
                ),
            )
            saved += 1
    return saved


def _s1d_render_from_cache(conn, df):
    import streamlit as st

    import pandas as _pd
    if df is None or getattr(df, "empty", True):
        st.info("No cached results.")
        if st.button("New search", key="s1d_new_search_empty"):
            st.session_state.pop("s1d_df", None)
        return
    # Show with links and dedupe flags
    def _mk_link(url, text):
        if not url: return text
        return f"<a href='{url}' target='_blank'>{text}</a>"
    show = df.copy()
    if "google_url" in show.columns:
        show["name"] = show.apply(lambda r: _mk_link(r.get("google_url",""), r.get("name","")), axis=1)
    if "website" in show.columns:
        show["website"] = show.apply(lambda r: _mk_link(r.get("website",""), "site") if r.get("website","") else "", axis=1)
    keep = df[~df.get("_dup", _pd.Series([False]*len(df)))].copy() if not df.empty else df
    # Results table
    cols = [c for c in ["name","phone","website","address","city","state","place_id","_dup"] if c in show.columns]
    if cols:
        st.markdown("**Results**")
        st.write(show[cols].to_html(escape=False, index=False), unsafe_allow_html=True)
    # Selection and save
    if keep.empty:
        st.success("All results are already in your vendor list.")
    else:
        st.caption(f"{len(keep)} new vendors can be saved")
        if "row_id" not in keep.columns:
            import hashlib as _h
            def _mk_id(r):
                pid = str(r.get("place_id","") or "")
                if pid: return pid
                s = f"{r.get('name',f'')}-{r.get('phone','')}-{r.get('city','')}"
                return _h.sha1(s.encode()).hexdigest()[:12]
            keep["row_id"] = keep.apply(_mk_id, axis=1)
        keep_view = keep[["row_id","name","phone","website","address","city","state","place_id"]].copy()
        # Restore selection
        sel_ids = set(st.session_state.get("s1d_selected_ids", []))
        keep_view.insert(1, "Select", keep_view["row_id"].isin(sel_ids))
        edited = st.data_editor(keep_view, hide_index=True, key="s1d_editor_cache")
        new_sel = set(edited.loc[edited["Select"]==True, "row_id"])
        st.session_state["s1d_selected_ids"] = list(new_sel)
        c1, c2, c3 = st.columns(3)
        with c1:
            if st.button("Save selected", key="s1d_save_selected_cache") and new_sel:
                sub = keep[keep["row_id"].isin(list(new_sel))].drop(columns=["row_id"], errors="ignore")
                n = _s1d_save_new_vendors(conn, sub.to_dict("records"))
                st.success(f"Saved {n} vendors")
        with c2:
            if st.button("Save all new vendors", key="s1d_save_all_cache"):
                sub = keep.drop(columns=["row_id"], errors="ignore")
                n = _s1d_save_new_vendors(conn, sub.to_dict("records"))
                st.success(f"Saved {n} vendors")
        with c3:
            if st.button("New search", key="s1d_new_search"):
                st.session_state.pop("s1d_df", None)
                st.session_state.pop("s1d_selected_ids", None)
        # rerun suppressed to keep table visible

def _s1d_vendor_write_table(conn):
    row = conn.execute("SELECT type, name, sql FROM sqlite_master WHERE name='vendors_t'").fetchone()
    if row and (row[0] or '').lower() == 'table':
        return 'vendors_t'
    if row and (row[0] or '').lower() == 'view':
        sql = row[2] or ''
        import re as _re
        m = _re.search(r'FROM\s+([A-Za-z_][A-Za-z0-9_]*)', sql, flags=_re.IGNORECASE)
        if m:
            return m.group(1)
    # fallback
    return 'vendors'

def _s1d_ensure_vendor_table(conn, table_name: str):
    # Create base table if needed
    conn.execute(f"""        CREATE TABLE IF NOT EXISTS {table_name}(
            id INTEGER PRIMARY KEY,
            source TEXT,
            place_id TEXT,
            name TEXT,
            email TEXT,
            phone TEXT,
            website TEXT,
            address TEXT,
            city TEXT,
            state TEXT,
            zip TEXT,
            naics TEXT,
            notes TEXT,
            lat REAL,
            lon REAL,
            created_at TEXT
        )
    """)
    # Add missing columns as app evolves
    cols = {r[1] for r in conn.execute(f"PRAGMA table_info({table_name})").fetchall()}
    needed = [
        "source","place_id","name","email","phone","website","address","city","state","zip","naics","notes","lat","lon","created_at"
    ]
    for c in needed:
        if c not in cols:
            try:
                conn.execute(f"ALTER TABLE {table_name} ADD COLUMN {c} TEXT")
            except Exception:
                pass

def _s1d_select_existing_pairs(conn):
    # Prefer vendors_t if present, else fallback to vendors
    srcs = [("vendors_t","view_or_table"), ("vendors","table")]
    for name, _ in srcs:
        row = conn.execute("SELECT name FROM sqlite_master WHERE name=?", (name,)).fetchone()
        if row:
            try:
                by_np, by_pid = _s1d_select_existing_pairs(conn)
                return by_np, by_pid
            except Exception:
                continue
    return set(), set()

def render_subfinder_s1d(conn):
    st.subheader("S1D — Subcontractor Finder")

    by_np, by_pid = _s1d_select_existing_pairs(conn)
    key = _s1d_get_api_key()
    if not key:
        st.error("Missing Google API key in secrets. Set google.api_key or GOOGLE_API_KEY.")
        return

    # State defaults
    for k, v in {
        "s1d_q": "",
        "s1d_loc": "Address",
        "s1d_addr": "",
        "s1d_lat": 38.8951,
        "s1d_lng": -77.0364,
        "s1d_radius": 25,
        "s1d_next_token": None,
        "s1d_df": None,
        "s1d_seen": set(),
    }.items():
        st.session_state.setdefault(k, v)

    # Form UI so controls never vanish
    with st.form("s1d_form", clear_on_submit=False):
        q = st.text_input("Search query", key="s1d_q", placeholder="e.g., HVAC contractors, plumbing, IT services")
        loc_choice = st.radio("Location", ["Address", "Lat/Lng"], horizontal=True, key="s1d_loc")
        if loc_choice == "Address":
            st.text_input("Place of performance address", key="s1d_addr")
            st.number_input("Radius (miles)", 1, 200, key="s1d_radius")
        else:
            c1, c2 = st.columns(2)
            with c1:
                st.number_input("Latitude", key="s1d_lat")
            with c2:
                st.number_input("Longitude", key="s1d_lng")
            st.number_input("Radius (miles)", 1, 200, key="s1d_radius")
        colA, colB = st.columns([1,1])
        with colA:
            do_search = st.form_submit_button("Search")
        with colB:
            do_next = st.form_submit_button("Next page")

    # Resolve center
    lat = lng = None
    if st.session_state["s1d_loc"] == "Address":
        if st.session_state["s1d_addr"]:
            ll = _s1d_geocode(st.session_state["s1d_addr"], key)
            if ll:
                lat, lng = ll
    else:
        lat = float(st.session_state["s1d_lat"])
        lng = float(st.session_state["s1d_lng"])

    radius_m = int(float(st.session_state["s1d_radius"]) * 1609.34)

    # Determine engine: Nearby when we have lat/lng so Google ranks by distance
    def _fetch_page(tok=None):
        if lat is not None and lng is not None and st.session_state["s1d_q"]:
            js = _s1d_places_nearby(st.session_state["s1d_q"], lat, lng, tok, key, rankby="distance")
        else:
            js = _s1d_places_textsearch(st.session_state["s1d_q"], lat, lng, radius_m, tok, key)
        return js

    # Handle submit
    if do_search:
        st.session_state["s1d_next_token"] = None
        st.session_state["s1d_seen"] = set()
    tok = st.session_state.get("s1d_next_token") if do_next else None

    if do_search or do_next:
        try:
            js = _fetch_page(tok)
            st.session_state["s1d_next_token"] = js.get("next_page_token")
            results = js.get("results", [])
        except Exception as e:
            st.error(f"Search failed: {e}")
            results = []

        # Build rows for this page only, filtering duplicates and hard distance filter
        rows = []
        seen = st.session_state.get("s1d_seen") or set()
        for r in results:
            name = r.get("name","") or ""
            pid = r.get("place_id","") or ""
            if pid in seen:
                continue
            addr = r.get("vicinity") or r.get("formatted_address","") or ""
            city = state = ""
            if "," in addr:
                parts = [p.strip() for p in addr.split(",")]
                if len(parts) >= 2:
                    city = parts[-2]
                    state = parts[-1].split()[0]
            gl = (r.get("geometry") or {}).get("location") or {}
            rlat = gl.get("lat")
            rlng = gl.get("lng")
            dist = _s1d_haversine_mi(lat, lng,
                                     float(rlat) if rlat is not None else None,
                                     float(rlng) if rlng is not None else None)

            # Hard filter outside radius
            if dist is not None and dist > float(st.session_state["s1d_radius"]):
                continue

            phone_disp = ""
            phone = ""
            website = ""
            google_url = f"https://www.google.com/maps/place/?q=place_id:{pid}"
            try:
                import requests as _rqd
                det = _rqd.get("https://maps.googleapis.com/maps/api/place/details/json",
                               params={"place_id": pid, "fields": "formatted_phone_number,website,url", "key": key},
                               timeout=10).json().get("result",{}) or {}
                phone_disp = det.get("formatted_phone_number","") or ""
                digits = "".join([c for c in phone_disp if c.isdigit()])
                if len(digits)==11 and digits.startswith("1"):
                    digits = digits[1:]
                phone = digits
                website = det.get("website","") or ""
                google_url = det.get("url","") or google_url
            except Exception:
                pass

            dup = ((name.strip().lower(), _s1d_norm_phone(phone)) in by_np) or (pid in by_pid)
            rows.append({
                "name": name,
                "address": addr,
                "city": city,
                "state": state,
                "phone": phone,
                "phone_display": phone_disp,
                "website": website,
                "place_id": pid,
                "google_url": google_url,
                "distance_mi": round(dist, 2) if dist is not None else None,
                "_dup": dup,
            })
            seen.add(pid)
            _time.sleep(0.03)

        # Store only this page to meet "new page" request
        import pandas as _pd
        df_page = _pd.DataFrame(rows)
        if not df_page.empty:
            try:
                df_page = df_page.sort_values(by=["distance_mi"], ascending=True, na_position="last")
            except Exception:
                pass
        st.session_state["s1d_df"] = df_page.to_dict("records")
        st.session_state["s1d_seen"] = seen

    # Results view
    import pandas as _pd
    df = _pd.DataFrame(st.session_state.get("s1d_df") or [])
    if df.empty:
        st.info("No results yet. Enter a query and click Search.")
        return

    from urllib.parse import urlparse
    def _mk_link(url, text):
        if not url:
            return text
        return f"<a href='{url}' target='_blank'>{text}</a>"
    def _tel_link(digits, label):
        if not digits:
            return label
        return f"<a href='tel:{digits}'>{label or digits}</a>"
    def _site_label(u):
        try:
            d = urlparse(u).netloc
            return d[4:] if d.startswith("www.") else d
        except Exception:
            return "website"

    show = df.copy()
    show["name"] = show.apply(lambda r: _mk_link(r.get("google_url",""), r["name"]), axis=1)
    show["website"] = show.apply(lambda r: _mk_link(r.get("website",""), _site_label(r.get("website",""))) if r.get("website","") else "", axis=1)
    show["phone"] = show.apply(lambda r: _tel_link(r.get("phone",""), r.get("phone_display","")), axis=1)
    show = show[["name","phone","website","address","city","state","distance_mi","place_id","_dup"]]

    st.markdown("**Results**")
    st.write(show.to_html(escape=False, index=False), unsafe_allow_html=True)

    c1, c2 = st.columns([1,1])
    with c1:
        keep = df[~df["_dup"]].copy()
        st.caption(f"{len(keep)} new vendors can be saved")
        if st.button("Save all new vendors", key="s1d_save_all") and not keep.empty:
            n = _s1d_save_new_vendors(conn, keep.to_dict("records"))
            st.success(f"Saved {n} vendors")
    with c2:
        if st.session_state.get("s1d_next_token"):
            if st.button("Next page ▶", key="s1d_next_under"):
                st.session_state["s1d_trigger"] = "next"
                st.rerun()

    # === S1D CARDS: Add Places to Vendors and Quick Edit Vendors ===
    import sqlite3 as _sqlite3
    import pandas as _pd

    def _is_view(_conn, name: str) -> bool:
        try:
            row = _pd.read_sql_query("SELECT type FROM sqlite_master WHERE name=?;", _conn, params=[name]).head(1)
            if not row.empty:
                return str(row.iloc[0]["type"]).lower() == "view"
        except Exception:
            pass
        return False

    def _vendor_write_table(_conn) -> str:
        # Prefer vendors_t if it is a real table, else vendors
        try:
            if not _is_view(_conn, "vendors_t"):
                # vendors_t might be a table or not exist
                row = _pd.read_sql_query("SELECT name, type FROM sqlite_master WHERE name='vendors_t';", _conn)
                if not row.empty and str(row.iloc[0]["type"]).lower() == "table":
                    return "vendors_t"
        except Exception:
            pass
        # Fallback
        return "vendors"

    def _paginate(df, page_size_key: str, page_key: str):
        import math
        import streamlit as st
        page_size = st.session_state.get(page_size_key, 12)
        page = st.session_state.get(page_key, 1)
        total = len(df)
        pages = max(1, math.ceil(total / page_size))
        page = max(1, min(page, pages))
        start = (page - 1) * page_size
        end = start + page_size
        c1, c2, c3, c4 = st.columns([1,1,2,2])
        with c1:
            st.number_input("Page", min_value=1, max_value=pages, key=page_key, value=page, step=1)
        with c2:
            st.number_input("Per page", min_value=4, max_value=40, key=page_size_key, value=page_size, step=4)
        with c3:
            st.caption(f"{total} items")
        with c4:
            st.caption(f"{pages} pages")
        page = st.session_state[page_key]
        page_size = st.session_state[page_size_key]
        start = (page - 1) * page_size
        end = start + page_size
        return df.iloc[start:end].copy()

    def _cards_add_places(conn):
        import streamlit as st
        df2 = _pd.DataFrame(st.session_state.get("s1d_df") or [])
        if df2.empty:
            st.caption("No S1D results to add")
            return
        # Filter UI
        cols = st.columns([2,2,1,2])
        with cols[0]:
            q = st.text_input("Filter by name or city", key="s1d_cards_q", placeholder="type to filter")
        with cols[1]:
            missing_only = st.checkbox("Only missing email or phone", key="s1d_cards_missing", value=True)
        with cols[2]:
            st.write("")
        with cols[3]:
            st.caption("Select cards then save")
        df = df2.copy()
        if "_dup" in df.columns:
            df = df.loc[~df["_dup"]].copy()
        for col in ("email","phone","website","address","city","state","place_id","name","distance_mi"):
            if col not in df.columns:
                df[col] = ""
        if q:
            ql = q.lower()
            df = df.loc[df["name"].astype(str).str.lower().str.contains(ql) | df["city"].astype(str).str.lower().str.contains(ql)]
        if missing_only:
            df = df.loc[(df["email"].astype(str)=="") | (df["phone"].astype(str)=="")]
        if df.empty:
            st.caption("No matches")
            return
        page_df = _paginate(df.reset_index(drop=True), "s1d_cards_ppg", "s1d_cards_page")
        selected_ids = []
        edited_rows = []
        # Render 3 columns grid
        grid = st.columns(3)
        for i, (_, r) in enumerate(page_df.iterrows()):
            col = grid[i % 3]
            with col:
                k = f"s1d_card_{r.get('place_id') or i}"
                st.container(border=True)
                st.markdown(f"**{str(r['name'])}**")
                st.caption(f"{str(r['address'])}, {str(r['city'])}, {str(r['state'])}")
                st.caption(f"~{str(r.get('distance_mi') or '')} mi")
                em = st.text_input("Email", key=f"{k}_em", value=str(r.get('email') or ""))
                ph = st.text_input("Phone", key=f"{k}_ph", value=str(r.get('phone') or ""))
                sel = st.checkbox("Select", key=f"{k}_sel", value=False)
                edited = dict(r)
                edited["email"] = em
                edited["phone"] = ph
                if sel:
                    selected_ids.append(k)
                edited_rows.append(edited)
        # Actions
        c1, c2 = st.columns(2)
        with c1:
            if st.button("Add selected to Vendors", key="s1d_cards_add_sel"):
                rows = [row for row in edited_rows if st.session_state.get(f"s1d_card_{row.get('place_id') or edited_rows.index(row)}_sel")]
                if rows:
                    try:
                        n = _s1d_save_new_vendors(conn, rows)
                        st.success(f"Saved {n} vendors")
                    except Exception as e:
                        st.warning(f"Save failed: {e}")
                else:
                    st.info("No cards selected")
        with c2:
            if st.button("Add all on page to Vendors", key="s1d_cards_add_page"):
                rows = edited_rows
                try:
                    n = _s1d_save_new_vendors(conn, rows)
                    st.success(f"Saved {n} vendors")
                except Exception as e:
                    st.warning(f"Save failed: {e}")

    def _cards_quick_edit_vendors(conn):
        import streamlit as st
        tbl_view = "vendors_t"
        write_tbl = _vendor_write_table(conn)
        # Read from view if available, else from base
        try:
            dfv = _pd.read_sql_query("SELECT id,name,email,phone,website,city,state FROM vendors_t ORDER BY name ASC;", conn)
        except Exception:
            dfv = _pd.read_sql_query("SELECT id,name,email,phone,website,city,state FROM vendors ORDER BY name ASC;", conn)
            tbl_view = "vendors"
        if dfv.empty:
            st.caption("Vendor list is empty")
            return
        cols = st.columns([2,2,1,2])
        with cols[0]:
            qv = st.text_input("Filter by name or city", key="s1d_vendor_q", placeholder="type to filter")
        with cols[1]:
            missing = st.checkbox("Only missing email or phone", key="s1d_vendor_missing", value=True)
        df = dfv.copy()
        if qv:
            qvl = qv.lower()
            df = df.loc[df["name"].astype(str).str.lower().str.contains(qvl) | df["city"].astype(str).str.lower().str.contains(qvl)]
        if missing:
            df = df.loc[(df["email"].astype(str)=="") | (df["phone"].astype(str)=="")]
        if df.empty:
            st.caption("No matches")
            return
        page_df = _paginate(df.reset_index(drop=True), "s1d_vendor_ppg", "s1d_vendor_page")
        changed = []
        grid = st.columns(3)
        for i, (_, r) in enumerate(page_df.iterrows()):
            col = grid[i % 3]
            with col:
                k = f"vend_card_{int(r['id'])}"
                st.container(border=True)
                st.markdown(f"**{str(r['name'])}**")
                st.caption(f"{str(r['city'])}, {str(r['state'])}")
                em = st.text_input("Email", key=f"{k}_em", value=str(r.get('email') or ""))
                ph = st.text_input("Phone", key=f"{k}_ph", value=str(r.get('phone') or ""))
                web = st.text_input("Website", key=f"{k}_web", value=str(r.get('website') or ""))
                if any([em != str(r.get('email') or ""), ph != str(r.get('phone') or ""), web != str(r.get('website') or "")]):
                    changed.append((em, ph, web, int(r["id"])))

                # Delete vendor from quick edit
                if st.button("Delete", key=f"{k}_del"):
                    try:
                        with conn:
                            conn.execute(f"DELETE FROM {write_tbl} WHERE id=?", (int(r["id"]),))
                        st.success("Vendor deleted")
                        st.rerun()
                    except Exception as e:
                        st.warning(f"Delete failed: {e}")
        if st.button("Save changes", key="s1d_vendor_cards_save"):
            if not changed:
                st.info("No changes detected")
            else:
                try:
                    with conn:
                        for em, ph, web, vid in changed:
                            conn.execute(f"UPDATE {write_tbl} SET email=?, phone=?, website=? WHERE id=?", (em, ph, web, vid))
                    st.success(f"Updated {len(changed)} vendors")
                except Exception as e:
                    st.warning(f"Save failed: {e}")

    # Render card-based expanders
    with st.expander("S1D: Add places to Vendors (cards)", expanded=True):
        _cards_add_places(conn)

    with st.expander("S1D: Quick edit Vendors (cards)", expanded=False):
        _cards_quick_edit_vendors(conn)
    # === END S1D CARDS ===
# === End S1D ================================================================

def _wrap_run_subfinder():
    g = globals()
    base = g.get("run_subcontractor_finder")
    if not callable(base) or getattr(base, "_s1d_wrapped", False):
        return
    def wrapped(conn):
        import streamlit as st

        st.header("Subcontractor Finder")
        try:
            render_subfinder_s1d(conn)
        except Exception as e:
            st.error(f"S1D error: {e}")
        base(conn)
    wrapped._s1d_wrapped = True
    g["run_subcontractor_finder"] = wrapped

_wrap_run_subfinder()

# =========================
# APPEND-ONLY PATCH • O2/O3/O4/O5/O6 + S1D
# This block does not remove or rename any of your code.
# =========================
import streamlit as _st
import sqlite3 as _sqlite3
import pandas as _pandas
import re as _re2, time as _time2, ssl as _ssl2, smtplib as _smtp2
from email.mime.text import MIMEText as _MText
from email.mime.multipart import MIMEMultipart as _MMulti

def __p_db(conn, q, args=()):
    cur = conn.cursor(); cur.execute(q, args); conn.commit(); return cur

def __p_ensure_core(conn):
    c = conn.cursor()
    c.execute("CREATE TABLE IF NOT EXISTS outreach_templates(id INTEGER PRIMARY KEY, name TEXT UNIQUE, subject TEXT, body TEXT)")
    c.execute("CREATE TABLE IF NOT EXISTS outreach_sender_accounts(id INTEGER PRIMARY KEY, label TEXT UNIQUE, email TEXT, app_password TEXT, smtp_host TEXT DEFAULT 'smtp.gmail.com', smtp_port INTEGER DEFAULT 587, use_tls INTEGER DEFAULT 1, is_active INTEGER DEFAULT 1, daily_limit INTEGER DEFAULT 500, minute_limit INTEGER DEFAULT 60, sent_today_count INTEGER DEFAULT 0, last_sent_at TEXT)")
    c.execute("CREATE TABLE IF NOT EXISTS outreach_audit(id INTEGER PRIMARY KEY, ts TEXT DEFAULT (datetime('now')), actor TEXT, action TEXT, meta TEXT)")
    c.execute("CREATE TABLE IF NOT EXISTS outreach_optouts(id INTEGER PRIMARY KEY, email TEXT UNIQUE, reason TEXT, ts TEXT DEFAULT (datetime('now')))")
    c.execute("CREATE TABLE IF NOT EXISTS outreach_sequences(id INTEGER PRIMARY KEY, name TEXT UNIQUE NOT NULL)")
    c.execute("CREATE TABLE IF NOT EXISTS outreach_steps(id INTEGER PRIMARY KEY, seq_id INTEGER NOT NULL, step_no INTEGER NOT NULL, delay_hours INTEGER NOT NULL DEFAULT 72, subject TEXT DEFAULT '', body_html TEXT DEFAULT '')")
    c.execute("CREATE TABLE IF NOT EXISTS outreach_schedules(id INTEGER PRIMARY KEY, seq_id INTEGER NOT NULL, step_no INTEGER NOT NULL, to_email TEXT NOT NULL, send_at TEXT NOT NULL, status TEXT NOT NULL DEFAULT 'queued', last_error TEXT DEFAULT '', subject TEXT DEFAULT '', body_html TEXT DEFAULT '', created_at TEXT DEFAULT CURRENT_TIMESTAMP)")
    c.execute("CREATE TABLE IF NOT EXISTS vendors_t(id INTEGER PRIMARY KEY, name TEXT, email TEXT, phone TEXT, website TEXT, city TEXT, state TEXT, naics TEXT, place_id TEXT)")
    conn.commit()

def __p_active_sender(conn):
    r = __p_db(conn, "SELECT label,email,app_password,smtp_host,smtp_port,use_tls FROM outreach_sender_accounts WHERE is_active=1 ORDER BY id DESC LIMIT 1").fetchone()
    return None if not r else dict(label=r[0], email=r[1], app_password=r[2], host=r[3], port=int(r[4] or 587), tls=bool(r[5]))

def __p_unsub_base(conn):
    try: return _st.secrets.get("app_base_url","")
    except Exception: return ""

def __p_unsub_link(conn, email):
    base = __p_unsub_base(conn)
    if not base: return ""
    import uuid as _uuid
    code = _uuid.uuid4().hex
    __p_db(conn, "INSERT INTO outreach_audit(actor,action,meta) VALUES(?,?,?)", ("system","UNSUB_CODE", f"{email}:{code}"))
    sep = "&" if "?" in base else "?"
    return f"{base}{sep}unsubscribe={code}"

def __p_is_supp(conn, email):
    row = __p_db(conn, "SELECT 1 FROM outreach_optouts WHERE lower(email)=lower(?) LIMIT 1", (email,)).fetchone()
    return bool(row)

def __p_smtp_send(sender, to_email, subject, sig_html, attachments: list[str] | None = None):
    msg = _MMulti("alternative"); msg["Subject"]=subject or ""; msg["From"]=sender["email"]; msg["To"]=to_email
# Inject signature HTML
    # Inject signature HTML
    try:
        _sig_conn = (get_o4_conn() if 'get_o4_conn' in globals() else None)
        sig_html, _imgs = __p_render_signature(_sig_conn or None, (sender.get('email') or sender.get('username') or '').strip(), sig_html)
    except Exception:
        pass
    msg.attach(_MText(sig_html or "", "sig_html"))
    # Attachments
    try:
        if attachments:
            from email.mime.base import MIMEBase as _MBase
            from email import encoders as _enc
            for _ap in attachments:
                try:
                    with open(_ap, 'rb') as _f:
                        _part = _MBase('application','octet-stream'); _part.set_payload(_f.read())
                    _enc.encode_base64(_part)
                    import os as _os
                    _part.add_header('Content-Disposition', f'attachment; filename="{_os.path.basename(_ap)}"')
                    msg.attach(_part)
                except Exception:
                    pass
    except Exception:
        pass
    if sender.get("tls", True):
        s = _smtp2.SMTP(sender["host"], int(sender.get("port",587))); s.ehlo(); s.starttls(context=_ssl2.create_default_context()); s.login(sender["email"], sender["app_password"])
    else:
        s = _smtp2.SMTP_SSL(sender["host"], int(sender.get("port",465)), context=_ssl2.create_default_context()); s.login(sender["email"], sender["app_password"])
    s.sendmail(sender["email"], [to_email], msg.as_string()); s.quit()

def __p_o4_ui(conn):
    _st.caption("Multiple Gmail senders via App Passwords.")
    with _st.form("__p_o4_add_sender", clear_on_submit=True):
        col1,col2 = _st.columns(2)
        with col1:
            label = _st.text_input("Label", placeholder="BD Gmail", key="__p_o4_lbl")
            email = _st.text_input("Gmail address", key="__p_o4_em")
            host  = _st.text_input("SMTP host", value="smtp.gmail.com", key="__p_o4_host")
        with col2:
            app_pw = _st.text_input("App password (16 chars)", type="password", key="__p_o4_pw")
            port   = _st.number_input("SMTP port", 1, 65535, value=587, key="__p_o4_port")
            tls    = _st.checkbox("Use STARTTLS", value=True, key="__p_o4_tls")
        if _st.form_submit_button("Save sender") and email:
            __p_db(conn, """INSERT INTO outreach_sender_accounts(label,email,app_password,smtp_host,smtp_port,use_tls,is_active)
                            VALUES (?, ?, ?, ?, ?,?,?,1)
                            ON CONFLICT(label) DO UPDATE SET email=excluded.email,app_password=excluded.app_password,
                            smtp_host=excluded.smtp_host,smtp_port=excluded.smtp_port,use_tls=excluded.use_tls""",
                   (label or email, email, app_pw, host, int(port), 1 if tls else 0))
            __p_db(conn, "INSERT INTO outreach_audit(actor,action,meta) VALUES(?,?,?)", ("system","O4_SAVE", email))
            _st.success(f"Saved {email}")
    rows = __p_db(conn, "SELECT label,email,smtp_host,smtp_port,use_tls,is_active FROM outreach_sender_accounts ORDER BY id DESC").fetchall()
    if rows:
        for lbl,em,host,port,tls,act in rows:
            _st.write(f"• **{lbl}** — {em} — {host}:{port} — TLS {bool(tls)} — {'Active' if act else 'Disabled'}")

def __p_o2_ui(conn):
    _st.caption("Save reusable templates. Supports {{name}}, {{company}}, {{UNSUB_LINK}}.")
    with _st.form("__p_o2_new", clear_on_submit=True):
        name = _st.text_input("Template name", key="__p_o2_name")
        subject = _st.text_input("Subject", key="__p_o2_subj")
        body = _st.text_area("Body", height=200, key="__p_o2_body")
        if _st.form_submit_button("Save template") and name:
            __p_db(conn, "INSERT OR REPLACE INTO outreach_templates(name,subject,body) VALUES(?,?,?)", (name,subject,body))
            _st.success(f"Saved template: {name}")
    df = _pandas.__p_read_sql_query("SELECT name, subject, substr(body,1,200) AS preview FROM outreach_templates ORDER BY name", conn)
    if not df.empty: __styled_dataframe(df, use_container_width=True, hide_index=True)

def __p_o3_ui(conn):
    sender = __p_active_sender(conn)
    if not sender: _st.info("Add a sender in O4 first."); return
    names = [r[0] for r in __p_db(conn, "SELECT name FROM outreach_templates ORDER BY name").fetchall()]
    tpl = _st.selectbox("Template", names) if names else None
    subj, body = ("","")
    if tpl:
        r = __p_db(conn, "SELECT subject,body FROM outreach_templates WHERE name=?", (tpl,)).fetchone()
        if r: subj, body = r[0] or "", r[1] or ""
    override = _st.text_input("Override subject (optional)", key="__p_o3_subj_override")
    if override: subj = override
    left,right = _st.columns(2)
    with left:
        raw = _st.text_area("Recipients (email,name,company)", height=220, placeholder="jane@acme.com,Jane,Acme", key="__p_o3_raw")
    with right:
        test_to = _st.text_input("Send test to", value=sender["email"], key="__p_o3_test_to")
        go_test = _st.button("Send test", key="__p_o3_sendtest")
        go_bulk = _st.button("Send bulk", key="__p_o3_sendbulk")
    rows = []
    for line in (raw or "").splitlines():
        p = [x.strip() for x in line.split(",")]
        if p and "@" in p[0]: rows.append(dict(email=p[0],name=(p[1] if len(p)>1 else ""),company=(p[2] if len(p)>2 else "")))
    def _render(s, r): return (s or "").replace("{{name}}", r.get("name","")).replace("{{company}}", r.get("company",""))
    base = __p_unsub_base(conn)
    def _with_unsub(sig_html, em):
        if "{{UNSUB_LINK}}" in (sig_html or ""): return (sig_html or "").replace("{{UNSUB_LINK}}", __p_unsub_link(conn, em) if base else "")
        if base: return (sig_html or "") + f"<hr><p style='font-size:12px;color:#666'>To unsubscribe click <a href='{__p_unsub_link(conn, em)}'>here</a>.</p>"
        return sig_html
    if go_test and subj and body:
        try:
            s_subj = _render(subj, {"name":"Test","company":"TestCo"})
            s_body = _with_unsub(_render(body, {"name":"Test","company":"TestCo"}), test_to)
            __p_smtp_send(sender, test_to, s_subj, s_body)
            __p_db(conn, "INSERT INTO outreach_audit(actor,action,meta) VALUES(?,?,?)", ("system","O3_TEST", test_to))
            _st.success("Test sent")
        except Exception as e:
            _st.error(f"Send failed: {e}")
    if go_bulk and rows and subj and body:
        sent=skip=fail=0
        for r in rows:
            em=r["email"]
            if __p_is_supp(conn, em): skip+=1; continue
            try:
                __p_smtp_send(sender, em, _render(subj,r), _with_unsub(_render(body,r), em))
                sent+=1; _time2.sleep(0.25)
            except Exception: fail+=1
        _st.success(f"Done. Sent {sent}. Skipped {skip}. Failed {fail}.")

def __p_o5_ui(conn):
    _st.caption("Sequences of follow-ups with delays.")
    seq_df = _pandas.__p_read_sql_query("SELECT id,name FROM outreach_sequences ORDER BY name", conn)
    names = ["— New —"] + ([] if seq_df.empty else seq_df["name"].tolist())
    c1,c2 = _st.columns([2,3])
    with c1:
        sel = _st.selectbox("Sequence", names, key="__p_o5_sel")
        new_name = _st.text_input("New sequence name", key="__p_o5_new") if sel=="— New —" else sel
        if _st.button("Save sequence", key="__p_o5_save") and new_name:
            __p_db(conn, "INSERT OR IGNORE INTO outreach_sequences(name) VALUES(?)", (new_name.strip(),)); _st.rerun()
    with c2:
        if sel!="— New —" and not seq_df.empty:
            seq_id = int(seq_df.loc[seq_df["name"]==sel,"id"].iloc[0])
            _st.markdown("**Steps**")
            steps = _pandas.__p_read_sql_query("SELECT step_no,delay_hours,subject FROM outreach_steps WHERE seq_id=? ORDER BY step_no", conn, params=(seq_id,))
            if not steps.empty: __styled_dataframe(steps, use_container_width=True, hide_index=True)
            s1,s2,s3 = _st.columns(3)
            with s1: step = _st.number_input("Step #", 1, 20, value=(int(steps["step_no"].max())+1 if not steps.empty else 1), key="__p_o5_step")
            with s2: delay = _st.number_input("Delay hours", 1, 720, value=72, key="__p_o5_delay")
            with s3: subj = _st.text_input("Subject", key="__p_o5_subj")
            body = _st.text_area("HTML body", height=120, key="__p_o5_body")
            if _st.button("Add step", key="__p_o5_add"):
                __p_db(
                    conn,
                    "INSERT INTO outreach_steps(seq_id,step_no,delay_hours,subject,body_html,owner_user) VALUES (?, ?, ?, ?, ?, ?)",
                    (seq_id, int(step), int(delay), subj or "", body or "", get_current_user_name()),
                ); _st.rerun()
    _st.markdown("---")
    _st.markdown("**Queue follow-ups**")
    if sel!="— New —" and not seq_df.empty: seq_id = int(seq_df.loc[seq_df["name"]==sel,"id"].iloc[0])
    else: seq_id=None
    emails_txt = _st.text_area("Emails, one per line", height=120, key="__p_o5_emails")
    if _st.button("Queue", key="__p_o5_queue"):
        if not seq_id: _st.error("Choose a sequence")
        else:
            steps = _pandas.__p_read_sql_query("SELECT step_no,delay_hours,subject,body_html FROM outreach_steps WHERE seq_id=? ORDER BY step_no", conn, params=(seq_id,))
            if steps.empty: _st.error("No steps")
            else:
                base = __import__("datetime").datetime.utcnow()
                n=0
                for em in [e.strip().lower() for e in (emails_txt or "").splitlines() if e.strip()]:
                    t = base
                    for _,row in steps.iterrows():
                        t = t + __import__("datetime").timedelta(hours=int(row["delay_hours"] or 0))
                        __p_db(conn,"INSERT INTO outreach_schedules(seq_id,step_no,to_email,send_at,status,subject,body_html) VALUES (?, ?, ?, ?, ?, ?, ?)",
                               (seq_id,int(row["step_no"]),em,t.strftime("%Y-%m-%dT%H:%M:%SZ"),row["subject"] or "",row["body_html"] or "")); n+=1
                _st.success(f"Queued {n}")
    if _st.button("Send due now", key="__p_o5_sendnow"):
        now = __import__("datetime").datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ")
        due = _pandas.__p_read_sql_query("SELECT id,to_email,subject,body_html FROM outreach_schedules WHERE status='queued' AND send_at<=? ORDER BY send_at LIMIT 200", conn, params=(now,))
        if due.empty: _st.info("No due items")
        else:
            s = __p_active_sender(conn)
            if not s: _st.error("No active sender in O4")
            else:
                ok=fail=0; base=__p_unsub_base(conn)
                def w(sig_html,em):
                    if "{{UNSUB_LINK}}" in (sig_html or ""): return (sig_html or "").replace("{{UNSUB_LINK}}", __p_unsub_link(conn, em) if base else "")
                    if base: return (sig_html or "")+f"<hr><p style='font-size:12px;color:#666'>To unsubscribe click <a href='{__p_unsub_link(conn, em)}'>here</a>.</p>"
                    return sig_html
                for _,r in due.iterrows():
                    em=r["to_email"]
                    if __p_is_supp(conn, em): __p_db(conn, "UPDATE outreach_schedules SET status='skipped' WHERE id=?", (int(r["id"]),)); continue
                    try: __p_smtp_send(s, em, r["subject"] or "", w(r["body_html"] or "", em)); __p_db(conn,"UPDATE outreach_schedules SET status='sent' WHERE id=?", (int(r["id"]),)); ok+=1; _time2.sleep(0.25)
                    except Exception as e: __p_db(conn,"UPDATE outreach_schedules SET status='error', last_error=? WHERE id=?", (str(e)[:500], int(r["id"]))); fail+=1
                _st.success(f"Sent {ok}, failed {fail}")

def __p_s1d_key():
    try: return _st.secrets["google"]["api_key"]
    except Exception:
        try: return _st.secrets["GOOGLE_API_KEY"]
        except Exception: return ""

def __p_s1d_norm_phone(p):
    digits="".join(_re2.findall(r"\d+", str(p or "")))
    return digits[1:] if len(digits)==11 and digits.startswith("1") else digits

def __p_s1d_existing(conn):
    rows = __p_db(conn,"SELECT name,COALESCE(phone,''),COALESCE(place_id,'') FROM vendors_t").fetchall()
    by_np=set(); by_pid=set()
    for r in rows: by_np.add(((r[0] or "").strip().lower(), __p_s1d_norm_phone(r[1] or "")))
    for r in rows:
        pid=(r[2] or "").strip()
        if pid: by_pid.add(pid)
    return by_np, by_pid

def __p_s1d_ui(conn):
    import pandas as pd
    by_np, by_pid = _s1d_select_existing_pairs(conn)
    _st.subheader("S1D — Google Places & Dedupe")
    key = __p_s1d_key()
    if not key:
        _st.error("Missing Google API key in secrets")
        return

    for k, v in {
        "__p_s1d_q": "",
        "__p_s1d_mode": "Address",
        "__p_s1d_addr": "",
        "__p_s1d_lat": 38.8951,
        "__p_s1d_lng": -77.0364,
        "__p_s1d_rad": 25,
        "__p_s1d_tok": None,
        "__p_s1d_df": None,
        "__p_s1d_seen": set(),
    }.items():
        _st.session_state.setdefault(k, v)

    with _st.form("__p_s1d_form", clear_on_submit=False):
        mode = _st.radio("Location mode", ["Address","Lat/Lng"], horizontal=True, key="__p_s1d_mode")
        if mode == "Address":
            _st.text_input("Place of performance address", key="__p_s1d_addr")
            _st.number_input("Radius (miles)", 1, 200, 25, key="__p_s1d_rad")
        else:
            c1, c2 = _st.columns(2)
            with c1:
                _st.number_input("Latitude", key="__p_s1d_lat")
            with c2:
                _st.number_input("Longitude", key="__p_s1d_lng")
            _st.number_input("Radius (miles)", 1, 200, 25, key="__p_s1d_rad")
        _st.text_input("Search query", key="__p_s1d_q", placeholder="e.g. janitorial contractors, HVAC service, landscaping")
        colA, colB = _st.columns([1,1])
        with colA:
            go = _st.form_submit_button("Search")
        with colB:
            nxt = _st.form_submit_button("Next page")

    lat = lng = None
    if _st.session_state["__p_s1d_mode"] == "Address":
        if _st.session_state["__p_s1d_addr"]:
            try:
                import requests as _rq
                js = _rq.get("https://maps.googleapis.com/maps/api/geocode/json",
                             params={"address": _st.session_state["__p_s1d_addr"], "key": key}, timeout=10).json()
                if js.get("status") == "OK":
                    loc = js["results"][0]["geometry"]["location"]
                    lat, lng = float(loc["lat"]), float(loc["lng"])
            except Exception:
                pass
    else:
        lat = float(_st.session_state["__p_s1d_lat"])
        lng = float(_st.session_state["__p_s1d_lng"])

    radius_m = int(float(_st.session_state["__p_s1d_rad"]) * 1609.34)

    def _fetch_page(tok=None):
        if lat is not None and lng is not None and _st.session_state["__p_s1d_q"]:
            js = _s1d_places_nearby(_st.session_state["__p_s1d_q"], lat, lng, tok, key, rankby="distance")
        else:
            js = _s1d_places_textsearch(_st.session_state["__p_s1d_q"], lat, lng, radius_m, tok, key)
        return js

    if go:
        _st.session_state["__p_s1d_tok"] = None
        _st.session_state["__p_s1d_df"] = None
        _st.session_state["__p_s1d_seen"] = set()
    tok = _st.session_state.get("__p_s1d_tok") if nxt else None

    if go or nxt:
        try:
            js = _fetch_page(tok)
            _st.session_state["__p_s1d_tok"] = js.get("next_page_token")
            results = js.get("results", [])
        except Exception as e:
            _st.error(f"Search failed: {e}")
            results = []

        rows = []
        seen = _st.session_state.get("__p_s1d_seen") or set()
        for r in results:
            name = r.get("name","")
            pid = r.get("place_id","") or ""
            if pid in seen:
                continue
            addr = r.get("vicinity") or r.get("formatted_address","") or ""
            city = state = ""
            if "," in addr:
                parts = [p.strip() for p in addr.split(",")]
                if len(parts) >= 2:
                    city = parts[-2]
                    state = parts[-1].split()[0]
            gl = (r.get("geometry") or {}).get("location") or {}
            rlat, rlng = gl.get("lat"), gl.get("lng")
            dist = _s1d_haversine_mi(lat, lng,
                                     float(rlat) if rlat is not None else None,
                                     float(rlng) if rlng is not None else None)
            if dist is not None and dist > float(_st.session_state["__p_s1d_rad"]):
                continue

            phone_disp = ""
            phone = ""
            website = ""
            gurl = f"https://www.google.com/maps/place/?q=place_id:{pid}"
            try:
                import requests as _rqd2
                det = _rqd2.get("https://maps.googleapis.com/maps/api/place/details/json",
                                params={"place_id": pid, "fields": "formatted_phone_number,website,url", "key": key},
                                timeout=10).json().get("result",{}) or {}
                phone_disp = det.get("formatted_phone_number","") or ""
                digits = "".join([c for c in phone_disp if c.isdigit()])
                if len(digits)==11 and digits.startswith("1"):
                    digits = digits[1:]
                phone = digits
                website = det.get("website","") or ""
                gurl = det.get("url","") or gurl
            except Exception:
                pass

            dup = ((name.strip().lower(), __p_s1d_norm_phone(phone)) in by_np) or (pid in by_pid)
            rows.append(dict(name=name, address=addr, city=city, state=state, phone=phone, phone_display=phone_disp,
                             website=website, place_id=pid, google_url=gurl,
                             distance_mi=(round(dist,2) if dist is not None else None), _dup=dup))
            seen.add(pid)
            _time.sleep(0.03)

        import pandas as _pandas
        df_page = _pandas.DataFrame(rows)
        if not df_page.empty:
            try:
                df_page = df_page.sort_values(by=["distance_mi"], ascending=True, na_position="last")
            except Exception:
                pass
        _st.session_state["__p_s1d_df"] = df_page.to_dict("records")
        _st.session_state["__p_s1d_seen"] = seen

    import pandas as _pandas
    df = _pandas.DataFrame(_st.session_state.get("__p_s1d_df") or [])
    if df.empty:
        _st.info("Enter a query and click Search.")
        return

    from urllib.parse import urlparse
    def _link(u, t):
        return f"<a href='{u}' target='_blank'>{t}</a>" if u else t
    def _tel(digits, label):
        return f"<a href='tel:{digits}'>{label or digits}</a>" if digits else (label or "")
    def _site_label(u):
        try:
            d = urlparse(u).netloc
            return d[4:] if d.startswith("www.") else d
        except Exception:
            return "website"

    show = df.copy()
    show["name"] = show.apply(lambda r: _link(r.get("google_url",""), r["name"]), axis=1)
    show["website"] = show.apply(lambda r: _link(r.get("website",""), _site_label(r.get("website",""))) if r.get("website","") else "", axis=1)
    show["phone"] = show.apply(lambda r: _tel(r.get("phone",""), r.get("phone_display","")), axis=1)
    show = show[["name","phone","website","address","city","state","distance_mi","place_id","_dup"]]
    _st.markdown("**Results**")
    _st.write(show.to_html(escape=False, index=False), unsafe_allow_html=True)

    keep = df[~df["_dup"]]
    _st.caption(f"{len(keep)} new vendors can be saved")
    if _st.button("Save all new vendors", key="__p_s1d_save") and not keep.empty:
        n = _s1d_save_new_vendors(conn, keep.to_dict("records"))
        _st.success(f"Saved {n} new vendors")


# Removed legacy monkeypatch block at load time

# =========================



# =========================
# Chat+ Assistant (Attachment-first, no-RFP dependency)
# =========================
def _detect_mime_light_plus(name: str) -> str:
    n = (name or "").lower()
    if n.endswith(".pdf"): return "application/pdf"
    if n.endswith(".docx"): return "application/vnd.openxmlformats-officedocument.wordprocessingml.document"
    if n.endswith(".xlsx"): return "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
    if n.endswith(".pptx"): return "application/vnd.openxmlformats-officedocument.presentationml.presentation"
    if n.endswith(".csv"): return "text/csv"
    if n.endswith(".md"): return "text/markdown"
    if n.endswith(".txt"): return "text/plain"
    return "application/octet-stream"

def _extract_any_to_text(name: str, data: bytes) -> str:
    """Best-effort text extractor that does NOT require an RFP record.
       Uses existing extract_text_pages() for pdf/docx/xlsx and safe fallbacks for others.
    """
    try:
        mime = _detect_mime_light_plus(name)
    except Exception:
        mime = "application/octet-stream"
    low = (mime or "").lower()
    # Prefer existing multi-backend for common types
    if any(k in low for k in ["pdf", "wordprocessingml", "spreadsheetml", "text/plain"]):
        try:
            pages = extract_text_pages(data, mime) or []
        except Exception:
            pages = []
        if pages:
            return "\\n\\n".join(pages)
        # fallback to raw decode if needed
        try:
            return data.decode("utf-8", errors="ignore")
        except Exception:
            try:
                return data.decode("latin-1", errors="ignore")
            except Exception:
                return ""
    # CSV
    if "text/csv" in low or (name or "").lower().endswith(".csv"):
        try:
            import pandas as _pd  # type: ignore
            import io as _io
            df = _pd.read_csv(_io.BytesIO(data))
            return df.fillna("").astype(str).to_csv(sep="\\t", index=False)
        except Exception:
            try:
                return data.decode("utf-8", errors="ignore")
            except Exception:
                return data.decode("latin-1", errors="ignore")
    # Markdown
    if "text/markdown" in low or (name or "").lower().endswith(".md"):
        try:
            return data.decode("utf-8", errors="ignore")
        except Exception:
            return data.decode("latin-1", errors="ignore")
    # PPTX (optional)
    if "presentationml.presentation" in low or (name or "").lower().endswith(".pptx"):
        try:
            from pptx import Presentation  # type: ignore
            import io as _io
            prs = Presentation(_io.BytesIO(data))
            texts = []
            for slide in prs.slides[:100]:
                for shape in slide.shapes:
                    try:
                        if hasattr(shape, "text") and shape.text:
                            texts.append(shape.text)
                    except Exception:
                        pass
            return "\\n".join(texts)
        except Exception:
            # graceful degrade
            try:
                return data.decode("utf-8", errors="ignore")
            except Exception:
                return ""
    # Default fallback
    try:
        return data.decode("utf-8", errors="ignore")
    except Exception:
        try:
            return data.decode("latin-1", errors="ignore")
        except Exception:
            return ""

def _score_snippets_by_query(snippets: list[str], query: str, top_k: int = 8) -> list[str]:
    import re as _re
    q = (query or "").strip()
    norm_q = _re.sub(r"[^A-Za-z0-9 ]+", " ", q).lower()
    terms = [w for w in norm_q.split() if len(w) > 1]
    scored = []
    for s in snippets:
        t = _re.sub(r"[^A-Za-z0-9 ]+", " ", s or "").lower()
        score = 0
        for w in terms:
            try:
                score += t.count(" " + w + " ")
            except Exception:
                pass
        scored.append((score, s))
    scored.sort(key=lambda x: x[0], reverse=True)
    return [s for _, s in scored[:max(1, int(top_k))]]

def _chat_plus_compose_messages(context_text: str, history: list[dict], question: str, mode: str = "Auto") -> list[dict]:
    rules = []
    m = (mode or "Auto").lower()
    if m.startswith("checklist"):
        rules.append("Format as a concise checklist with imperative items and short sub-bullets.")
    elif m.startswith("phone"):
        rules.append("Return a phone call script with opener, 6-10 targeted questions, and a closing commitment line.")
    elif m.startswith("email"):
        rules.append("Draft a succinct business email with Subject, Greeting, 3 short paragraphs, a numbered list of required attachments, and a clear ask with a date.")
    elif m.startswith("pricing"):
        rules.append("List the exact pricing inputs you need, clarify unit drivers, and note risk flags or missing data.")
    else:
        rules.append("Write a direct answer. Use short, declarative sentences. No citations.")

    system = (
        "You are a federal contracting chat assistant. "
        "Use only the user's attachments and chat history as your source. "
        "If a fact is not present, say what is missing. "
        "No citations. No tags. Be precise. "
        "When asked for vendor outreach or subcontractor questions, output concrete, verifiable asks."
    )
    msgs = [{"role": "system", "content": system}]
    for h in history or []:
        if h.get("role") in ("user", "assistant") and (h.get("content") or "").strip():
            msgs.append({"role": h["role"], "content": h["content"]})
    if (context_text or "").strip():
        msgs.append({"role": "system", "content": "Attachment context:\n" + context_text[:24000]})
    msgs.append({"role": "user", "content": question.strip() + "\\n\\nRules: " + " ".join(rules)})
    return msgs

def _chat_plus_call_openai(messages: list[dict], temperature: float = 0.2) -> str:
    client = _resolve_openai_client()
    if not client:
        return "AI unavailable. Configure OPENAI_API_KEY in Streamlit secrets."
    try:
        resp = client.chat.completions.create(
            model=_resolve_model(),
            messages=messages,
            temperature=float(temperature),
        )
        return (resp.choices[0].message.content or "").strip()
    except Exception as e:
        return f"AI error: {e}"


def _list_rfps(conn) -> list[tuple[str, str]]:
    """Return list of (rfp_id, label). Defensive over possible schemas."""
    try:
        cur = conn.cursor()
        cur.execute("SELECT name FROM sqlite_master WHERE type='table'")
        tables = [r[0] for r in cur.fetchall()]
    except Exception:
        return []
    # candidate tables likely to hold RFP metadata
    candidates = [t for t in tables if re.search(r"rfp", t, re.I)]
    results = []
    for t in candidates:
        # probe common id/title columns
        id_cols = ["id","rfp_id","rid"]
        title_cols = ["title","name","subject","rfp_title"]
        for idc in id_cols:
            for tc in title_cols:
                try:
                    cur.execute(f"SELECT {idc}, {tc} FROM {t} ORDER BY 1 DESC LIMIT 50")
                    for rid, title in cur.fetchall():
                        rid_s = str(rid)
                        title_s = str(title) if title is not None else ""
                        label = f"{rid_s} — {title_s} [{t}]"
                        results.append((rid_s, label))
                    if results:
                        return results
                except Exception:
                    pass
        # fallback: only id
        for idc in id_cols:
            try:
                cur.execute(f"SELECT {idc} FROM {t} ORDER BY 1 DESC LIMIT 50")
                for (rid,) in cur.fetchall():
                    rid_s = str(rid)
                    label = f"{rid_s} [{t}]"
                    results.append((rid_s, label))
                if results:
                    return results
            except Exception:
                pass
    return results

def _load_rfp_context(conn, rfp_id: str, max_chars: int = 200000) -> str:
    """Try multiple table/column patterns to get joined text for a given rfp_id."""
    if not conn or not rfp_id:
        return ""
    try:
        cur = conn.cursor()
        cur.execute("SELECT name FROM sqlite_master WHERE type='table'")
        tables = [r[0] for r in cur.fetchall()]
    except Exception:
        return ""
    candidates = [t for t in tables if re.search(r"rfp", t, re.I)]
    text_cols = ["text","content","full_text","page_text","body"]
    id_cols = ["rfp_id","id","rid"]
    chunks = []
    for t in candidates:
        for idc in id_cols:
            for tc in text_cols:
                try:
                    cur.execute(f"SELECT {tc} FROM {t} WHERE {idc}=? LIMIT 500", (rfp_id,))
                    rows = cur.fetchall()
                    if rows:
                        for (val,) in rows:
                            if isinstance(val, (bytes, bytearray)):
                                try:
                                    val = val.decode("utf-8", errors="ignore")
                                except Exception:
                                    val = val.decode("latin-1", errors="ignore")
                            if val:
                                chunks.append(str(val))
                except Exception:
                    pass
    text = "\n\n---\n\n".join(chunks)
    if not text and candidates:
        # last resort: any rows from 'context' tables
        for t in candidates:
            if re.search(r"context|pages|docs", t, re.I):
                for tc in text_cols:
                    try:
                        cur.execute(f"SELECT {tc} FROM {t} LIMIT 200")
                        rows = cur.fetchall()
                        if rows:
                            for (val,) in rows[:50]:
                                if isinstance(val, (bytes, bytearray)):
                                    try:
                                        val = val.decode("utf-8", errors="ignore")
                                    except Exception:
                                        val = val.decode("latin-1", errors="ignore")
                                if val:
                                    chunks.append(str(val))
                    except Exception:
                        pass
                if chunks:
                    break
    return (text or "\n\n---\n\n".join(chunks))[:max_chars]


def run_chat_assistant(conn: "sqlite3.Connection") -> None:
    import streamlit as st
    import pandas as pd
    import os
    st.header("Chat Assistant")
    st.caption("Attachment-first. Works without an RFP. No citations.")

    # Session state
    files_key = "chat_plus_files"
    hist_key = "chat_plus_history"
    tmp_key = "chat_plus_tmp"

    if files_key not in st.session_state:
        st.session_state[files_key] = []  # list of dicts: {name, mime, text, size}
    if hist_key not in st.session_state:
        st.session_state[hist_key] = []   # list of chat messages

    
    # Track clears without mutating the uploader widget directly
    cleared_hash_key = "chat_plus_cleared_hash"
    if cleared_hash_key not in st.session_state:
        st.session_state[cleared_hash_key] = None

    def _chat_plus_hash_uploads(files):
        """Compute a simple hash for the current uploader contents."""
        if not files:
            return None
        import hashlib
        h = hashlib.sha256()
        try:
            it = list(files)
        except TypeError:
            it = files
        for f in it:
            name = getattr(f, "name", "") or ""
            size = getattr(f, "size", 0) or 0
            h.update(str(name).encode("utf-8", "ignore"))
            h.update(b"|")
            h.update(str(size).encode("utf-8"))
            h.update(b";")
        return h.hexdigest()

# Controls
    c0, c1, c2, c3 = st.columns([2,3,2,2])
    with c0:
        source = st.selectbox("Source", ["Attachments only","RFP context only","Both"], index=0, key="chat_plus_source")
    with c1:
        ups = st.file_uploader(
            "Add attachments",
            type=["pdf","docx","xlsx","pptx","csv","txt","md"],
            accept_multiple_files=True,
            key="chat_plus_uploader",
            help="Upload any files you want me to use. I will answer from these files only."
        )
    with c2:
        mode = st.selectbox("Output mode", ["Auto","Checklist","Phone script","Email to vendor","Pricing inputs"], index=0, key="chat_plus_mode")
    with c3:
        temp = st.slider("Temperature", 0.0, 1.0, 0.2, 0.1, key="chat_plus_temp")


    # Optional RFP selection
    selected_rfp_id = None
    if source in ("RFP context only","Both"):
        st.divider()
        st.subheader("RFP context")
        options = _list_rfps(conn)
        if options:
            labels = [label for _, label in options]
            idx = st.selectbox("Choose RFP", list(range(len(labels))), format_func=lambda i: labels[i], key="chat_plus_rfp_idx") if labels else 0
            if options:
                selected_rfp_id = options[idx][0]
        manual = st.text_input("Or enter RFP ID manually", value="", key="chat_plus_rfp_manual")
        if manual.strip():
            selected_rfp_id = manual.strip()
        if selected_rfp_id:
            if st.button("Preview RFP context", key="chat_plus_preview_rfp"):
                preview = _load_rfp_context(conn, selected_rfp_id, max_chars=5000)
                if preview:
                    st.text_area("RFP context preview", preview, height=200, key="chat_plus_rfp_preview_box")
                else:
                    st.info("No RFP context found for that ID.")


    
    # Ingest uploads into session
    new_rows = []
    if ups:
        # Compute a stable signature for the current uploader contents.
        # This lets us respect "Clear attachments" without touching the
        # file_uploader widget state (which Streamlit does not allow).
        cur_hash = _chat_plus_hash_uploads(ups)
        cleared_hash = st.session_state.get(cleared_hash_key)
        ingest_allowed = not (cleared_hash and cur_hash == cleared_hash)

        if ingest_allowed:
            for f in ups:
                try:
                    try:
                        data = f.getbuffer().tobytes()
                    except Exception:
                        data = f.read()
                    txt = _extract_any_to_text(f.name, data)
                    rec = {
                        "name": f.name,
                        "mime": _detect_mime_light_plus(f.name),
                        "size": len(data),
                        "text": txt or "",
                    }
                    new_rows.append(rec)
                except Exception:
                    continue
            if new_rows:
                st.session_state[files_key].extend(new_rows)
                st.success(f"Added {len(new_rows)} attachment(s).")
# Attachment table
    files = st.session_state[files_key]
    if files:
        with st.expander("Attachments in this chat", expanded=True):
            df = pd.DataFrame([{"File": r["name"], "Bytes": r["size"], "Preview": (r["text"][:200] + ("..." if len(r["text"])>200 else ""))} for r in files])
            st.dataframe(df, use_container_width=True, hide_index=True)
            colA, colB, colC = st.columns(3)
            with colA:
                if st.button("Clear attachments", key="chat_plus_clear_files"):
                    # Clear the in-memory attachment list
                    st.session_state[files_key] = []
                    # Remember the current uploader hash so we do not
                    # immediately re-ingest the same files on rerun.
                    try:
                        cur_hash = _chat_plus_hash_uploads(ups)
                    except Exception:
                        cur_hash = None
                    st.session_state[cleared_hash_key] = cur_hash
            with colB:
                if st.button("Clear chat", key="chat_plus_clear_chat"):
                    st.session_state[hist_key] = []
            with colC:
                # small helper to save a merged text file for inspection if needed
                if st.button("Export merged context", key="chat_plus_export_ctx"):
                    merged = "\\n\\n---\\n\\n".join([r["text"] for r in files if (r.get("text") or "").strip()])
                    outp = os.path.join(DATA_DIR if 'DATA_DIR' in globals() else ".", "chat_plus_context.txt")
                    try:
                        with open(outp, "w", encoding="utf-8") as fh:
                            fh.write(merged)
                        try:
                            with open(outp, "rb") as _f:
                                _data = _f.read()
                            st.download_button(
                                "Download context",
                                data=_data,
                                file_name=os.path.basename(outp),
                                mime="text/plain",
                                key="dl_ai_context",
                            )
                        except Exception as _e:
                            st.warning(f"Export failed: {_e}")
                    except Exception as e:
                        st.warning(f"Export failed: {e}")
    else:
        st.info("No attachments yet. You can still ask general questions.")

    # Chat UI
    st.subheader("Ask a question")
    q = st.text_area("Your question", key="chat_plus_q", height=100, placeholder="Ask about requirements, draft an email to a subcontractor, list pricing inputs, etc.")
    ask = st.button("Ask", type="primary", key="chat_plus_ask")
    # render prior messages
    for m in st.session_state[hist_key]:
        with st.chat_message(m["role"]):
            st.markdown(m["content"])

    if ask and (q or "").strip():
        # Build context: select top snippets by overlap
        # Gather attachment text
        att_texts = []
        for _r in st.session_state[files_key]:
            _txt = (_r.get('text') or '').strip()
            if _txt:
                _name = str(_r.get('name') or 'file')
                att_texts.append('Source: ' + _name + '\n\n' + _txt)

        # Optionally add RFP context
        rfp_text = ""
        if source in ("RFP context only","Both") and selected_rfp_id:
            try:
                rfp_text = _load_rfp_context(conn, selected_rfp_id, max_chars=120000)
            except Exception:
                rfp_text = ""
        # Build candidate texts per source
        base_texts = []
        if source == "Attachments only":
            base_texts = att_texts
        elif source == "RFP context only":
            base_texts = [rfp_text] if (rfp_text or "").strip() else []
        else:
            base_texts = att_texts + ([rfp_text] if (rfp_text or "").strip() else [])
        # chunk long texts
        try:
            chunks = []
            for t in base_texts:
                cs = y5_chunk_text(t, target_chars=9000, overlap=500)  # reuse chunker if present
                chunks.extend(cs or [])
        except Exception:
            # fallback chunk
            chunks = []
            for t in base_texts:
                for i in range(0, len(t), 8000):
                    chunks.append(t[i:i+8000])
        top = _score_snippets_by_query(chunks or base_texts or [""], q, top_k=10)
        context_text = "\\n\\n---\\n\\n".join([s[:1800] for s in top])[:24000]

        # Compose and call model
        history = st.session_state[hist_key]
        messages = _chat_plus_compose_messages(context_text, history, q, mode=mode)
        ans = _chat_plus_call_openai(messages, temperature=temp)

        # Append to history
        history.append({"role": "user", "content": q})
        history.append({"role": "assistant", "content": ans})
        st.session_state[hist_key] = history

        # Render last assistant message immediately
        with st.chat_message("assistant"):
            st.markdown(ans)



# ===================== APPEND-ONLY PATCH • Chat+ Memory (Threads + Summaries) =====================
# This wraps run_chat_assistant(conn) to add persistent threads and rolling summaries in SQLite.
# It syncs st.session_state["chat_plus_history"] to cp_messages and reloads by thread id.
# Safe to import repeatedly; guarded by _CHAT_ASSISTANT_WRAPPED.

def _cp_db_exec(conn, sql, params=()):
    from contextlib import closing
    with closing(conn.cursor()) as cur:
        cur.execute(sql, params)
    conn.commit()

def _cp_db_query(conn, sql, params=()):
    from contextlib import closing
    with closing(conn.cursor()) as cur:
        cur.execute(sql, params)
        rows = cur.fetchall()
    return rows

def _cp_ensure_schema(conn):
    _cp_db_exec(conn, """
    CREATE TABLE IF NOT EXISTS cp_threads(
      id INTEGER PRIMARY KEY AUTOINCREMENT,
      title TEXT NOT NULL,
      created_at TEXT DEFAULT CURRENT_TIMESTAMP
    );""")
    _cp_db_exec(conn, """
    CREATE TABLE IF NOT EXISTS cp_messages(
      id INTEGER PRIMARY KEY AUTOINCREMENT,
      thread_id INTEGER NOT NULL,
      role TEXT NOT NULL,
      content TEXT NOT NULL,
      created_at TEXT DEFAULT CURRENT_TIMESTAMP
    );""")
    _cp_db_exec(conn, """
    CREATE TABLE IF NOT EXISTS cp_summaries(
      thread_id INTEGER PRIMARY KEY,
      summary TEXT NOT NULL,
      updated_at TEXT DEFAULT CURRENT_TIMESTAMP
    );""")
    _cp_db_exec(conn, "CREATE INDEX IF NOT EXISTS idx_cp_messages_thread ON cp_messages(thread_id);")

def _cp_list_threads(conn):
    rows = _cp_db_query(conn, "SELECT id, COALESCE(title,'') FROM cp_threads ORDER BY id DESC;")
    return [(int(r[0]), str(r[1])) for r in rows]

def _cp_create_thread(conn, title="General"):
    _cp_db_exec(conn, "INSERT INTO cp_threads(title) VALUES(?);", (title or "Untitled",))
    tid = _cp_db_query(conn, "SELECT last_insert_rowid();")[0][0]
    return int(tid)

def _cp_rename_thread(conn, thread_id, new_title):
    _cp_db_exec(conn, "UPDATE cp_threads SET title=? WHERE id=?;", (new_title or "Untitled", int(thread_id)))

def _cp_delete_thread(conn, thread_id):
    tid = int(thread_id)
    _cp_db_exec(conn, "DELETE FROM cp_messages WHERE thread_id=?;", (tid,))
    _cp_db_exec(conn, "DELETE FROM cp_summaries WHERE thread_id=?;", (tid,))
    _cp_db_exec(conn, "DELETE FROM cp_threads WHERE id=?;", (tid,))

def _cp_get_messages(conn, thread_id):
    rows = _cp_db_query(conn, "SELECT role, content FROM cp_messages WHERE thread_id=? ORDER BY id ASC;", (int(thread_id),))
    return [{"role": str(r[0]), "content": str(r[1])} for r in rows]

def _cp_append_message(conn, thread_id, role, content):
    _cp_db_exec(conn, "INSERT INTO cp_messages(thread_id, role, content) VALUES(?,?,?);",
                (int(thread_id), str(role), str(content)))

def _cp_get_summary(conn, thread_id):
    rows = _cp_db_query(conn, "SELECT summary FROM cp_summaries WHERE thread_id=?;", (int(thread_id),))
    return (rows[0][0] if rows else "").strip()

def _cp_upsert_summary(conn, thread_id, summary):
    if _cp_get_summary(conn, thread_id):
        _cp_db_exec(conn, "UPDATE cp_summaries SET summary=?, updated_at=CURRENT_TIMESTAMP WHERE thread_id=?;",
                    (str(summary), int(thread_id)))
    else:
        _cp_db_exec(conn, "INSERT INTO cp_summaries(thread_id, summary) VALUES(?,?);",
                    (int(thread_id), str(summary)))

def _cp_maybe_autosummarize(conn, thread_id, every_n=6, max_chars=9000):
    """Every N total messages or when history grows large, refresh a compact summary."""
    import textwrap
    msgs = _cp_get_messages(conn, thread_id)
    if not msgs:
        return
    n_total = len([m for m in msgs if m["role"] in ("user","assistant")])
    do = (n_total % every_n == 0) or (len("\\n".join(m["content"] for m in msgs)) > max_chars)
    if not do:
        return
    try:
        if "_y6_chat" in globals():
            prompt = [
                {"role": "system", "content": "Summarize the thread for recall. Keep under 220 words. List decisions, next steps, and unresolved questions."},
                {"role": "user", "content": "\\n\\n".join([f"{m['role'].upper()}: {m['content']}" for m in msgs][-30:])[:12000]},
            ]
            s = _y6_chat(prompt, temperature=0.0)
        elif "_chat_plus_call_openai" in globals():
            prompt = [
                {"role": "system", "content": "Summarize the thread for recall. Keep under 220 words. List decisions, next steps, and unresolved questions."},
                {"role": "user", "content": "\\n\\n".join([f"{m['role'].upper()}: {m['content']}" for m in msgs][-30:])[:12000]},
            ]
            s = _chat_plus_call_openai(prompt, temperature=0.0)
        else:
            last = msgs[-10:]
            s = "Recent points:\\n" + "\\n".join(["- " + textwrap.shorten(m["content"], width=140, placeholder="…") for m in last])
    except Exception:
        last = msgs[-10:]
        s = "Recent points:\\n" + "\\n".join(["- " + textwrap.shorten(m["content"], width=140, placeholder="…") for m in last])
    _cp_upsert_summary(conn, thread_id, s)

def _cp_load_into_session(conn, thread_id):
    import streamlit as st
    hist = _cp_get_messages(conn, thread_id)
    hist = [m for m in hist if m["role"] in ("user","assistant")]
    st.session_state["chat_plus_history"] = hist

def _cp_sync_from_session(conn, thread_id, before_len):
    import streamlit as st
    after = st.session_state.get("chat_plus_history") or []
    if before_len is None:
        return
    if len(after) <= int(before_len):
        return
    for m in after[int(before_len):]:
        role = str(m.get("role") or "")
        content = str(m.get("content") or "")
        if role in ("user","assistant") and content.strip():
            _cp_append_message(conn, thread_id, role, content)
    _cp_maybe_autosummarize(conn, thread_id)

def _wrap_chat_assistant():
    """Wrap existing run_chat_assistant(conn) with thread picker and memory sync."""
    _orig = run_chat_assistant

    def _wrapped(conn):
        import streamlit as st
        _cp_ensure_schema(conn)

        st.subheader("Threads")
        threads = _cp_list_threads(conn)
        if not threads:
            tid = _cp_create_thread(conn, "General")
            threads = _cp_list_threads(conn)

        cur_tid = st.session_state.get("cp_thread_id") or (threads[0][0] if threads else None)

        cols = st.columns([3,1,1,1])
        with cols[0]:
            labels = [f"#{tid}  {title or 'Untitled'}" for tid, title in threads]
            index_map = {i: tid for i, (tid, _) in enumerate(threads)}
            sel_idx = st.selectbox("Thread", list(range(len(labels))) or [0], format_func=lambda i: labels[i] if labels else "0", key="cp_thread_idx")
            sel_tid = index_map.get(sel_idx, cur_tid)
        with cols[1]:
            if st.button("New", key="cp_new_thread"):
                sel_tid = _cp_create_thread(conn, f"Thread {max(1, len(threads)+1)}")
        with cols[2]:
            current_title = dict(threads).get(sel_tid, "Untitled") if sel_tid else "Untitled"
            rn = st.text_input("Rename", value=current_title, key="cp_rename_input")
            if st.button("Apply", key="cp_rename_apply") and sel_tid:
                _cp_rename_thread(conn, sel_tid, rn or "Untitled")
        with cols[3]:
            if st.button("Delete", key="cp_delete_thread") and sel_tid:
                _cp_delete_thread(conn, sel_tid)
                sel_tid = _cp_create_thread(conn, "General")

        st.session_state["cp_thread_id"] = sel_tid

        with st.expander("Memory summary", expanded=False):
            cur_sum = _cp_get_summary(conn, sel_tid) or "No summary yet. It will auto update as you chat."
            st.text_area("Summary", cur_sum, height=160, key="cp_sum_view")
            if st.button("Regenerate summary", key="cp_force_sum"):
                _cp_maybe_autosummarize(conn, sel_tid, every_n=1)
                st.rerun()

        if "chat_plus_history" not in st.session_state or not isinstance(st.session_state["chat_plus_history"], list):
            _cp_load_into_session(conn, sel_tid)
        before_len = len(st.session_state.get("chat_plus_history") or [])

        _orig(conn)

        _cp_sync_from_session(conn, sel_tid, before_len)

    return _wrapped

# Install the wrapper before any main() dispatch if possible
try:
    _CHAT_ASSISTANT_WRAPPED  # sentinel
except NameError:
    try:
        run_chat_assistant = _wrap_chat_assistant()
        _CHAT_ASSISTANT_WRAPPED = True
    except Exception:
        # If run_chat_assistant is not defined yet, defer by setting a flag.
        _DEFER_WRAP_CHAT_ASSISTANT = True

# If the definition appears later, wrap on first import after it's defined
try:
    _DEFER_WRAP_CHAT_ASSISTANT
    # If run_chat_assistant exists now, wrap and clear flag
    if callable(globals().get("run_chat_assistant", None)):
        run_chat_assistant = _wrap_chat_assistant()
        _CHAT_ASSISTANT_WRAPPED = True
        del _DEFER_WRAP_CHAT_ASSISTANT
except NameError:
    pass
# ===================== END PATCH =====================



# =========================
# Job System • lightweight async tracking
# =========================

def ensure_jobs_schema(conn: "sqlite3.Connection") -> None:
    """Create the jobs table and basic indexes if they do not exist.

    This is intentionally lightweight and idempotent so it can be called
    anywhere before enqueueing or listing jobs.
    """
    from contextlib import closing as _closing
    try:
        with _closing(conn.cursor()) as cur:
            cur.execute(
                """
                CREATE TABLE IF NOT EXISTS jobs(
                    id INTEGER PRIMARY KEY,
                    type TEXT NOT NULL,
                    status TEXT NOT NULL DEFAULT 'queued',
                    created_by_user TEXT,
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                    started_at TEXT,
                    finished_at TEXT,
                    progress REAL DEFAULT 0.0,
                    payload_json TEXT,
                    result_json TEXT,
                    error_message TEXT
                );
                """
            )
            cur.execute("CREATE INDEX IF NOT EXISTS idx_jobs_status ON jobs(status);")
            cur.execute("CREATE INDEX IF NOT EXISTS idx_jobs_user ON jobs(created_by_user, created_at);")
            conn.commit()
    except Exception:
        # Schema creation should never break the app
        pass


def jobs_enqueue(conn: "sqlite3.Connection", job_type: str, payload: dict | None = None, created_by: str | None = None) -> int:
    """Insert a new queued job row and return its id.

    Actual background processing is handled elsewhere; this is just
    the shared persistence layer other features can call.
    """
    import json as _json
    from contextlib import closing as _closing

    ensure_jobs_schema(conn)
    try:
        user_name = created_by
        if not user_name:
            try:
                user_name = get_current_user_name()
            except Exception:
                user_name = None
        payload_json = _json.dumps(payload or {}, ensure_ascii=False)
        with _closing(conn.cursor()) as cur:
            cur.execute(
                "INSERT INTO jobs(type, status, created_by_user, created_at, progress, payload_json) "
                "VALUES(?, 'queued', ?, datetime('now'), 0.0, ?);",
                (str(job_type), user_name, payload_json),
            )
            conn.commit()
            try:
                return int(cur.lastrowid)
            except Exception:
                return 0
    except Exception:
        return 0


def jobs_update_status(
    conn: "sqlite3.Connection",
    job_id: int,
    status: str | None = None,
    progress: float | None = None,
    error_message: str | None = None,
    result: dict | None = None,
    mark_started: bool | None = None,
    mark_finished: bool | None = None,
) -> None:
    """Best effort update for a job row.

    This keeps the API flexible without forcing every caller to pass
    all fields all the time.
    """
    import json as _json
    from contextlib import closing as _closing

    ensure_jobs_schema(conn)
    fields: list[str] = []
    params: list[object] = []

    if status is not None:
        fields.append("status = ?")
        params.append(str(status))
    if progress is not None:
        try:
            p = float(progress)
        except Exception:
            p = 0.0
        fields.append("progress = ?")
        params.append(p)
    if error_message is not None:
        fields.append("error_message = ?")
        params.append(str(error_message)[:2000])
    if result is not None:
        fields.append("result_json = ?")
        params.append(_json.dumps(result, ensure_ascii=False))
    if mark_started:
        fields.append("started_at = COALESCE(started_at, datetime('now'))")
    if mark_finished:
        fields.append("finished_at = datetime('now')")

    if not fields:
        return

    sql = "UPDATE jobs SET " + ", ".join(fields) + " WHERE id = ?;"
    params.append(int(job_id))
    try:
        with _closing(conn.cursor()) as cur:
            cur.execute(sql, tuple(params))
            conn.commit()
    except Exception:
        # Do not hard fail callers on job status issues
        pass



# === Background jobs worker helpers (single-file worker mode) ===

def _jobs_worker_connect_db():
    """Open a connection for the background worker.

    Reuses the same DATABASE_URL / DB_PATH logic as the main app so the worker
    stays in sync whether you are on SQLite or Postgres.
    """
    import os as _os
    import sqlite3 as _sqlite3
    from pathlib import Path as _Path

    db_url = (_os.getenv("DATABASE_URL", "") or "").strip()

    if db_url.startswith("postgres://") or db_url.startswith("postgresql://"):
        try:
            import psycopg2  # type: ignore
        except Exception as e:  # pragma: no cover - defensive
            raise RuntimeError("DATABASE_URL points at Postgres, but psycopg2 is not installed for the worker.") from e
        conn = psycopg2.connect(db_url)
        conn.autocommit = True
        ensure_jobs_schema(conn)
        return conn

    # SQLite path (default)
    db_path = _Path(DB_PATH)
    db_path.parent.mkdir(parents=True, exist_ok=True)

    conn = _sqlite3.connect(str(db_path), check_same_thread=False)
    conn.row_factory = _sqlite3.Row  # convenient dict-like rows
    ensure_jobs_schema(conn)
    return conn



def _jobs_worker_fetch_next_queued_job(conn):
    """Atomically fetch the next queued job and mark it as running.

    Returns (job_id, job_type, payload_dict) or None if nothing is queued.
    """
    import json as _json
    cur = conn.cursor()
    # BEGIN IMMEDIATE obtains a RESERVED lock; suitable for a single worker.
    cur.execute("BEGIN IMMEDIATE;")
    cur.execute(
        "SELECT id, type, payload_json FROM jobs "
        "WHERE status = 'queued' ORDER BY created_at ASC LIMIT 1;"
    )
    row = cur.fetchone()
    if not row:
        cur.execute("COMMIT;")
        return None

    job_id = int(row["id"])
    job_type = str(row["type"] or "")
    payload_json = row["payload_json"] or "{}"
    try:
        payload = _json.loads(payload_json) if payload_json else {}
    except Exception:

        payload = {}

    # Mark as running and set started_at
    cur.execute(
        "UPDATE jobs SET status = 'running', started_at = CURRENT_TIMESTAMP WHERE id = ?;",
        (job_id,),
    )
    cur.execute("COMMIT;")
    return job_id, job_type, payload


def _jobs_worker_handle_backup_full(conn, job_id: int, payload: dict):
    """Handler for job type='backup_full'.

    Copies the SQLite DB file to a timestamped backup under a 'backups'
    folder alongside the main DB file. Progress is written to jobs.progress
    so the UI can display a progress bar.
    """
    import time as _time
    import shutil as _shutil
    from pathlib import Path as _Path

    db_path = _Path(payload.get("db_path") or DB_PATH)
    if not db_path.exists():
        raise FileNotFoundError(f"DB file not found at {db_path}")

    backups_dir = db_path.parent / "backups"
    backups_dir.mkdir(parents=True, exist_ok=True)

    ts = _time.strftime("%Y%m%d_%H%M%S")
    backup_path = backups_dir / f"backup_{ts}.db"

    # Start progress
    try:
        jobs_update_status(conn, job_id, progress=0.1, mark_started=True)
    except Exception:
        # best-effort; failure here should not abort the backup
        pass

    # Copy DB file
    _shutil.copy2(str(db_path), str(backup_path))

    # Finalize
    jobs_update_status(
        conn,
        job_id,
        status="done",
        progress=1.0,
        result={"backup_path": str(backup_path)},
        mark_finished=True,
    )


def _jobs_worker_handle_restore_from_backup(conn, job_id: int, payload: dict):
    """Handler for job type='restore_from_backup'.

    Overwrites the live DB file with the uploaded backup path in
    payload['upload_path']. Keeps a pre-restore copy of the current DB.
    """
    import time as _time
    import shutil as _shutil
    from pathlib import Path as _Path

    db_path = _Path(payload.get("db_path") or DB_PATH)
    upload_path = _Path(payload.get("upload_path") or "")

    if not upload_path.exists():
        raise FileNotFoundError(f"Uploaded backup not found at {upload_path}")

    db_path.parent.mkdir(parents=True, exist_ok=True)

    # Optional safety: keep a pre-restore copy if DB exists
    pre_backup_path = None
    if db_path.exists():
        ts = _time.strftime("%Y%m%d_%H%M%S")
        pre_backup_path = db_path.parent / f"pre_restore_{ts}.db"
        _shutil.copy2(str(db_path), str(pre_backup_path))

    try:
        jobs_update_status(conn, job_id, progress=0.25, mark_started=True)
    except Exception:
        pass

    # Perform restore (overwrite DB)
    _shutil.copy2(str(upload_path), str(db_path))

    jobs_update_status(
        conn,
        job_id,
        status="done",
        progress=1.0,
        result={
            "restored_from": str(upload_path),
            "db_path": str(db_path),
            "pre_restore_backup": str(pre_backup_path) if pre_backup_path else None,
        },
        mark_finished=True,
    )

def _jobs_worker_handle_sam_live_search(conn, job_id: int, payload: dict):
    """Worker handler for job_type='sam_live_search'.

    Delegates to _sam_run_live_search_job so heavy SAM.gov calls happen
    outside the Streamlit UI process.
    """
    params = {}
    try:
        raw = payload.get("params") or {}
        if isinstance(raw, dict):
            params = dict(raw)
    except Exception:
        params = {}
    _sam_run_live_search_job(conn, job_id, params)




def _jobs_worker_handle_rfp_ingest_analyze(conn, job_id: int, payload: dict):
    """Worker handler for job_type='rfp_ingest_analyze'.

    Runs the One-Click RFP analyze pipeline in the background based on rfp_id.
    """
    rfp_id = 0
    sam_url = None
    try:
        if isinstance(payload, dict):
            rfp_id = int(payload.get("rfp_id") or 0)
            sam_url = payload.get("sam_url") or None
    except Exception:
        rfp_id = 0
        sam_url = None
    if not rfp_id:
        try:
            jobs_update_status(
                conn,
                job_id,
                status="failed",
                error_message="Missing rfp_id in job payload.",
                mark_finished=True,
            )
        except Exception:
            pass
        return

    try:
        jobs_update_status(
            conn,
            job_id,
            status="running",
            mark_started=True,
            progress=0.0,
        )
    except Exception:
        pass

    try:
        _one_click_analyze(conn, int(rfp_id), sam_url)
        try:
            jobs_update_status(
                conn,
                job_id,
                status="done",
                mark_finished=True,
                progress=1.0,
                result={"rfp_id": int(rfp_id)},
            )
        except Exception:
            pass
    except Exception as exc:
        try:
            jobs_update_status(
                conn,
                job_id,
                status="failed",
                error_message=str(exc),
                mark_finished=True,
            )
        except Exception:
            pass



def _jobs_worker_handle_pb_draft_all(conn, job_id: int, payload: dict):
    """Worker handler for job_type='pb_draft_all'.

    Generates one-page proposal sections in the background using the same
    _draft_section helper as the synchronous one-page analyzer.
    """
    from typing import Dict
    import json as _json

    sections: list[str] = []
    context: str = ""

    try:
        if isinstance(payload, dict):
            raw_sections = payload.get("sections") or []
            if isinstance(raw_sections, (list, tuple, set)):
                sections = [str(s) for s in raw_sections]
            else:
                sections = [str(raw_sections)]
            ctx = payload.get("context") or payload.get("combined") or ""
            if ctx is None:
                ctx = ""
            context = str(ctx)
    except Exception:
        sections = []
        context = ""

    if not sections:
        try:
            jobs_update_status(
                conn,
                job_id,
                status="failed",
                error_message="No sections provided for pb_draft_all job.",
                mark_finished=True,
            )
        except Exception:
            pass
        return

    total = max(len(sections), 1)
    try:
        jobs_update_status(
            conn,
            job_id,
            status="running",
            mark_started=True,
            progress=0.0,
        )
    except Exception:
        pass

    draft: Dict[str, str] = {}
    error: Exception | None = None

    try:
        for idx, sec in enumerate(sections):
            try:
                draft[sec] = _draft_section(sec, context)
            except Exception as _sec_exc:
                draft[sec] = f"[Draft failed for section '{sec}': {_sec_exc!s}]"
            try:
                jobs_update_status(
                    conn,
                    job_id,
                    progress=(idx + 1) / float(total),
                )
            except Exception:
                pass
    except Exception as exc:
        error = exc

    if error is not None:
        try:
            jobs_update_status(
                conn,
                job_id,
                status="failed",
                error_message=str(error),
                mark_finished=True,
            )
        except Exception:
            pass
        return

    try:
        jobs_update_status(
            conn,
            job_id,
            status="done",
            progress=1.0,
            result={"drafted_sections": list(draft.keys()), "draft": draft},
            mark_finished=True,
        )
    except Exception:
        pass
def _jobs_worker_handle_unknown(conn, job_id: int, job_type: str, payload: dict):
    """Fallback handler for job types the worker does not yet implement."""
    jobs_update_status(
        conn,
        job_id,
        status="failed",
        error_message=f"Worker does not handle job type '{job_type}' yet.",
        mark_finished=True,
    )


def _jobs_worker_handle_fts_index_rebuild(conn, job_id: int, payload: dict):
    """Worker handler for job_type='fts_index_rebuild'.

    Rebuilds the Y1 semantic index for a single RFP in the background using
    y1_index_rfp, so heavy PDF parsing and OCR work does not block the main UI.
    """
    rfp_id = 0
    try:
        if isinstance(payload, dict):
            rfp_id = int(payload.get("rfp_id") or 0)
    except Exception:
        rfp_id = 0
    if not rfp_id:
        try:
            jobs_update_status(
                conn,
                job_id,
                status="failed",
                error_message="Missing rfp_id in job payload.",
                mark_finished=True,
            )
        except Exception:
            pass
        return

    try:
        jobs_update_status(
            conn,
            job_id,
            status="running",
            mark_started=True,
            progress=0.0,
        )
    except Exception:
        pass

    try:
        out = y1_index_rfp(conn, int(rfp_id), rebuild=True)
        added = 0
        if isinstance(out, dict):
            try:
                added = int(out.get("added", 0) or 0)
            except Exception:
                added = 0
        jobs_update_status(
            conn,
            job_id,
            status="done",
            progress=1.0,
            mark_finished=True,
            result={"ok": True, "added": added},
        )
    except Exception as exc:
        try:
            jobs_update_status(
                conn,
                job_id,
                status="failed",
                error_message=str(exc),
                mark_finished=True,
            )
        except Exception:
            pass



def _jobs_worker_handle_proposal_export_docx(conn, job_id: int, payload: dict) -> None:
    """Worker handler for full proposal DOCX export."""
    import os as _os
    try:
        path = payload.get("out_path") or payload.get("out_name")
    except Exception:
        path = None
    if not path:
        rfp_id = payload.get("rfp_id")
        if rfp_id:
            path = _os.path.join(DATA_DIR, f"Proposal_RFP_{int(rfp_id)}.docx")
        else:
            path = _os.path.join(DATA_DIR, f"Proposal_{job_id}.docx")

    doc_title = payload.get("doc_title") or "Proposal"
    sections = payload.get("sections") or []
    clins = payload.get("clins")
    checklist = payload.get("checklist")
    metadata = payload.get("metadata") or {}
    font_name = payload.get("font_name") or "Calibri"
    try:
        font_size_pt = int(payload.get("font_size_pt") or 11)
    except Exception:
        font_size_pt = 11
    spacing = payload.get("spacing") or "1.15"

    try:
        jobs_update_status(conn, job_id, status="running", progress=0.05, mark_started=True)
    except Exception:
        pass

    try:
        exported = _export_docx(
            path,
            doc_title=doc_title,
            sections=sections,
            clins=clins,
            checklist=checklist,
            metadata=metadata,
            font_name=font_name,
            font_size_pt=font_size_pt,
            spacing=spacing,
        )
        if not exported:
            try:
                jobs_update_status(
                    conn,
                    job_id,
                    status="failed",
                    error_message="DOCX export returned no path.",
                    mark_finished=True,
                )
            except Exception:
                pass
            return
        try:
            jobs_update_status(
                conn,
                job_id,
                status="done",
                progress=1.0,
                result={"path": str(exported)},
                mark_finished=True,
            )
        except Exception:
            pass
    except Exception as exc:
        try:
            jobs_update_status(
                conn,
                job_id,
                status="failed",
                error_message=str(exc),
                mark_finished=True,
            )
        except Exception:
            pass


def _jobs_worker_handle_fast_rfq_export_docx(conn, job_id: int, payload: dict) -> None:
    """Worker handler for Fast RFQ one-page DOCX export."""
    _jobs_worker_handle_proposal_export_docx(conn, job_id, payload)


def _jobs_worker_handle_rfq_pack_build(conn, job_id: int, payload: dict) -> None:
    """Worker handler for RFQ pack ZIP build."""
    try:
        pack_id = int(payload.get("pack_id") or 0)
    except Exception:
        pack_id = 0

    if not pack_id:
        try:
            jobs_update_status(
                conn,
                job_id,
                status="failed",
                error_message="Missing pack_id in RFQ pack build payload.",
                mark_finished=True,
            )
        except Exception:
            pass
        return

    try:
        jobs_update_status(conn, job_id, status="running", progress=0.05, mark_started=True)
    except Exception:
        pass

    try:
        z = _rfq_build_zip(conn, pack_id)
        if not z:
            try:
                jobs_update_status(
                    conn,
                    job_id,
                    status="failed",
                    error_message="RFQ ZIP build returned no path.",
                    mark_finished=True,
                )
            except Exception:
                pass
            return
        try:
            jobs_update_status(
                conn,
                job_id,
                status="done",
                progress=1.0,
                result={"zip_path": str(z)},
                mark_finished=True,
            )
        except Exception:
            pass
    except Exception as exc:
        try:
            jobs_update_status(
                conn,
                job_id,
                status="failed",
                error_message=str(exc),
                mark_finished=True,
            )
        except Exception:
            pass



def _jobs_worker_handle_outreach_batch_send(conn, job_id: int, payload: dict):
    """Worker handler for job_type='outreach_batch_send'.

    Sends outreach mail-merge batches using the generic SMTP helper so large
    blasts run in the jobs worker instead of the Streamlit UI.
    """
    import pandas as _pd
    from contextlib import closing as _closing
    import datetime as _dt

    rows_data = payload.get("rows") or []
    subject_tpl = payload.get("subject_tpl") or ""
    html_tpl = payload.get("html_tpl") or ""
    attachments = payload.get("attachments") or []
    try:
        max_send = int(payload.get("max_send") or 0)
    except Exception:
        max_send = 0

    if not rows_data:
        # Nothing to do; mark job done with zero work.
        jobs_update_status(
            conn,
            job_id,
            status="done",
            progress=1.0,
            mark_finished=True,
            result={"sent": 0, "row_count": 0},
        )
        return

    try:
        df = _pd.DataFrame(rows_data)
    except Exception:
        # Fallback if payload isn't tabular
        df = _pd.DataFrame(list(rows_data))

    row_count = int(len(df))

    # Ensure Outreach schema exists
    try:
        _o3_ensure_schema(conn)
    except Exception:
        pass

    # Derive sender email from payload
    sender = payload.get("sender") or {}
    sender_email = (sender.get("email") or sender.get("username") or "").strip()

    # Determine owner_user from jobs.created_by when available
    owner_user = "system"
    try:
        with _closing(conn.cursor()) as cur:
            cur.execute("SELECT created_by FROM jobs WHERE id = ?", (int(job_id),))
            row = cur.fetchone()
        if row and row[0]:
            owner_user = str(row[0])
    except Exception:
        owner_user = "system"

    # Create a new blast record
    blast_title = payload.get("blast_title") or f"Outreach job #{job_id} ({_dt.datetime.utcnow().strftime('%Y-%m-%d %H:%M')})"
    template_name = payload.get("template_name") or ""
    with _closing(conn.cursor()) as cur:
        cur.execute(
            "INSERT INTO outreach_blasts(title, template_name, sender_email, owner_user) VALUES(?,?,?,?);",
            (blast_title, template_name, sender_email, owner_user),
        )
        conn.commit()
        blast_id = cur.lastrowid

    sent_count = 0

    for _, r in df.iterrows():
        if max_send and sent_count >= max_send:
            break

        # Basic email + name extraction
        to_email = str(r.get("email") or r.get("Email") or r.get("EMAIL") or "").strip()
        to_name = str(r.get("name") or r.get("Name") or r.get("contact_name") or "").strip()

        if not to_email or "@" not in to_email:
            status = "Skipped: invalid email"
            error = "Missing or malformed address"
        else:
            data = {k: ("" if v is None else v) for k, v in dict(r).items()}
            if "name" not in data:
                data["name"] = to_name

            # Simple merge rendering
            subject = _o3_merge(subject_tpl or "", data)
            body_html = _o3_merge(html_tpl or "", data)

            ok, err = send_email_smtp(to_email, subject, body_html, attachments or [])
            status = "Sent" if ok else "Error"
            error = "" if ok else (err or "Unknown error")
            if ok:
                sent_count += 1

        with _closing(conn.cursor()) as cur:
            cur.execute(
                "INSERT INTO outreach_log(blast_id,to_email,to_name,subject,status,error) VALUES(?,?,?,?,?,?);",
                (blast_id, to_email, to_name, subject_tpl or "", status, error),
            )
            conn.commit()

    jobs_update_status(
        conn,
        job_id,
        status="done",
        progress=1.0,
        mark_finished=True,
        result={"sent": int(sent_count), "row_count": row_count},
    )

def _jobs_worker_run_single(conn, job_id: int, job_type: str, payload: dict):
    """Dispatch a single job by type and handle errors uniformly."""
    import traceback as _traceback

    try:
        if job_type == "backup_full":
            _jobs_worker_handle_backup_full(conn, job_id, payload)
        elif job_type == "restore_from_backup":
            _jobs_worker_handle_restore_from_backup(conn, job_id, payload)
        elif job_type == "sam_live_search":
            _jobs_worker_handle_sam_live_search(conn, job_id, payload)
        elif job_type == "sam_live_search":
            _jobs_worker_handle_sam_live_search(conn, job_id, payload)
        elif job_type == "rfp_ingest_analyze":
            _jobs_worker_handle_rfp_ingest_analyze(conn, job_id, payload)
        elif job_type == "pb_draft_all":
            _jobs_worker_handle_pb_draft_all(conn, job_id, payload)
        elif job_type == "proposal_export_docx":
            _jobs_worker_handle_proposal_export_docx(conn, job_id, payload)
        elif job_type == "fast_rfq_export_docx":
            _jobs_worker_handle_fast_rfq_export_docx(conn, job_id, payload)
        elif job_type == "rfq_pack_build":
            _jobs_worker_handle_rfq_pack_build(conn, job_id, payload)
        elif job_type == "outreach_batch_send":
            _jobs_worker_handle_outreach_batch_send(conn, job_id, payload)
        elif job_type == "fts_index_rebuild":
            _jobs_worker_handle_fts_index_rebuild(conn, job_id, payload)
        else:
            _jobs_worker_handle_unknown(conn, job_id, job_type, payload)
    except Exception as _exc:
        err_msg = f"{_exc.__class__.__name__}: {_exc}"
        try:
            jobs_update_status(
                conn,
                job_id,
                status="failed",
                error_message=err_msg[:500],
                mark_finished=True,
            )
        except Exception:
            _traceback.print_exc()


def jobs_worker_loop(sleep_seconds: float = 3.0) -> None:
    """Main background worker loop.

    Usage (separate process):

        python app.py --worker

    It will:
    - Connect to the same DB as the app
    - Repeatedly fetch the next queued job
    - Mark it as running
    - Execute the handler
    - Sleep briefly when there is no work
    """
    import time as _time
    import traceback as _traceback

    while True:
        conn = None
        try:
            conn = _jobs_worker_connect_db()
            job_info = _jobs_worker_fetch_next_queued_job(conn)
            if not job_info:
                _time.sleep(sleep_seconds)
                continue

            job_id, job_type, payload = job_info
            print(f"[worker] Starting job #{job_id} type={job_type!r}")
            _jobs_worker_run_single(conn, job_id, job_type, payload)
            print(f"[worker] Finished job #{job_id} type={job_type!r}")
        except Exception:
            _traceback.print_exc()
            _time.sleep(sleep_seconds)
        finally:
            if conn is not None:
                try:
                    conn.close()
                except Exception:
                    pass


def jobs_list_for_user(conn: "sqlite3.Connection", user_name: str, limit: int = 50):
    import pandas as pd
    """Return a DataFrame of recent jobs for the given user name."""
    import pandas as _pd

    ensure_jobs_schema(conn)
    try:
        df = _pd.read_sql_query(
            "SELECT id, type, status, progress, created_at, started_at, finished_at, error_message "
            "FROM jobs WHERE created_by_user = ? ORDER BY created_at DESC LIMIT ?;",
            conn,
            params=(str(user_name), int(limit)),
        )
        return df
    except Exception:
        return _pd.DataFrame(columns=["id", "type", "status", "progress", "created_at", "started_at", "finished_at", "error_message"])


def run_my_jobs(conn: "sqlite3.Connection") -> None:
    """Small dashboard showing background jobs created by the current user."""
    import streamlit as st
    import pandas as pd

    page_header("My Jobs", "Background work created from this login in the current workspace.")
    ensure_jobs_schema(conn)

    try:
        user_name = get_current_user_name()
    except Exception:
        user_name = ""
    if not user_name:
        st.info("Select a user in the sidebar to see your jobs.")
        return

    st.caption(f"Showing jobs where created_by_user = '{user_name}'.")
    col_status, col_limit = st.columns([2, 1])
    with col_status:
        status_filter = st.multiselect(
            "Status filter",
            options=["queued", "running", "done", "failed"],
            default=["queued", "running", "done", "failed"],
            key="jobs_status_filter",
        )
    with col_limit:
        limit = st.number_input("Max rows", min_value=10, max_value=500, value=50, step=10, key="jobs_limit")

    df = jobs_list_for_user(conn, user_name=user_name, limit=int(limit))
    if df is None or df.empty:
        st.info("No jobs recorded yet for this user.")
        return

    if status_filter:
        df = df[df["status"].isin(status_filter)]

    if "progress" in df.columns:
        try:
            df["progress_pct"] = (df["progress"].fillna(0.0).astype(float) * 100.0).round(1)
        except Exception:
            df["progress_pct"] = df.get("progress", 0.0)

    cols = [c for c in ["id", "type", "status", "progress_pct", "created_at", "started_at", "finished_at", "error_message"] if c in df.columns]
    if cols:
        df = df[cols]

    st.dataframe(
        df,
        use_container_width=True,
        hide_index=True,
    )

    with st.expander("Details", expanded=False):
        st.caption("Raw jobs table for debugging or export.")
        st.dataframe(df, use_container_width=True, hide_index=True)





# --- Proposal Template Library UI (lifted earlier to avoid NameError) ---
def x7_template_library_ui(conn: "sqlite3.Connection") -> None:
    """Basic Proposal Template Library management UI.

    - Shows existing templates.
    - Lets you clone or archive an existing template.
    - Provides forms to create a new template and edit the selected template,
      including a token grid.
    """
    from contextlib import closing as _closing

    st.markdown("**Template library**")

    try:
        df = pd.read_sql_query(
            "SELECT id, name, template_type, default_section, is_active, created_at FROM proposal_templates ORDER BY created_at DESC;",
            conn,
            params=(),
        )
    except Exception:
        df = pd.DataFrame(columns=["id", "name", "template_type", "default_section", "is_active", "created_at"])

    if df.empty:
        st.info("No templates yet. Use the forms below to create your first template.")
    else:
        st.dataframe(df)

    selected_id: int | None = None
    if not df.empty:
        selected_id = st.selectbox(
            "Template to edit",
            options=[None] + df["id"].tolist(),
            format_func=lambda tid: "None" if tid is None else (
                f"#{tid} — {df.loc[df['id']==tid,'name'].values[0]} ({df.loc[df['id']==tid,'template_type'].values[0] or 'section'})"
            ),
            key="x7_tpl_edit_select",
        )

        c1, c2, c3 = st.columns(3)
        with c1:
            if st.button("Clone selected", key="x7_tpl_clone", disabled=selected_id is None):
                if selected_id is not None:
                    with _closing(conn.cursor()) as cur:
                        cur.execute(
                            """
                            INSERT INTO proposal_templates(name, description, template_type, default_section, body, owner, is_active)
                            SELECT name || ' (Copy)', description, template_type, default_section, body, owner, is_active
                            FROM proposal_templates WHERE id = ?;
                            """,
                            (int(selected_id),),
                        )
                        new_id = cur.lastrowid
                        cur.execute(
                            """
                            INSERT INTO proposal_template_tokens(template_id, token_name, source_type, source_expr, default_value, description)
                            SELECT ?, token_name, source_type, source_expr, default_value, description
                            FROM proposal_template_tokens
                            WHERE template_id = ?;
                            """,
                            (int(new_id), int(selected_id)),
                        )
                        conn.commit()
                    st.success("Template cloned.")
                    st.rerun()
        with c2:
            if st.button("Archive selected", key="x7_tpl_archive", disabled=selected_id is None):
                if selected_id is not None:
                    with _closing(conn.cursor()) as cur:
                        cur.execute(
                            "UPDATE proposal_templates SET is_active = 0 WHERE id = ?;",
                            (int(selected_id),),
                        )
                        conn.commit()
                    st.success("Template archived.")
                    st.rerun()

        with c3:
            if st.button("Delete selected", key="x7_tpl_delete", disabled=selected_id is None):
                if selected_id is not None:
                    with _closing(conn.cursor()) as cur:
                        cur.execute(
                            "DELETE FROM proposal_template_tokens WHERE template_id = ?;",
                            (int(selected_id),),
                        )
                        cur.execute(
                            "DELETE FROM proposal_templates WHERE id = ?;",
                            (int(selected_id),),
                        )
                        conn.commit()
                    st.success("Template deleted.")
                    st.rerun()

    st.markdown("---")
    st.markdown("**New template**")

    empty_tokens_df = pd.DataFrame(
        columns=["token_name", "source_type", "source_expr", "default_value", "description"]
    )

    with st.form("x7_tpl_new_form"):
        new_name = st.text_input("Template name", "")
        new_type = st.selectbox(
            "Template type",
            options=["section", "proposal", "rfq_pack", "compliance"],
            index=0,
            key="x7_tpl_new_type",
        )
        new_default_section = st.text_input("Default section label (optional)", "")
        new_description = st.text_area("Description (optional)", "")
        new_body = st.text_area(
            "Template body (use {{TOKEN_NAME}} placeholders)",
            height=200,
            key="x7_tpl_new_body",
        )
        st.caption("Examples: {{ORG_NAME}}, {{RFP_TITLE}}, {{DEAL_AMOUNT}}.")
        new_tokens_df = st.data_editor(
            empty_tokens_df,
            num_rows="dynamic",
            hide_index=True,
            key="x7_tpl_new_tokens",
        )
        submitted_new = st.form_submit_button("Create template")

        if submitted_new:
            if not new_name.strip():
                st.warning("Template name is required.")
            elif not new_body.strip():
                st.warning("Template body is required.")
            else:
                with _closing(conn.cursor()) as cur:
                    cur.execute(
                        """
                        INSERT INTO proposal_templates(name, description, template_type, default_section, body, owner, is_active)
                        VALUES(?,?,?,?,?,?,1);
                        """,
                        (
                            new_name.strip(),
                            new_description.strip() or None,
                            new_type,
                            new_default_section.strip() or None,
                            new_body,
                            None,
                        ),
                    )
                    new_id = cur.lastrowid
                    try:
                        records = new_tokens_df.to_dict("records")
                    except Exception:
                        records = []
                    for row in records:
                        tname = str(row.get("token_name") or "").strip().upper()
                        if not tname:
                            continue
                        source_type = str(row.get("source_type") or "manual").strip()
                        source_expr = str(row.get("source_expr") or "").strip() or None
                        default_value = row.get("default_value")
                        description = row.get("description")
                        cur.execute(
                            """
                            INSERT INTO proposal_template_tokens(template_id, token_name, source_type, source_expr, default_value, description)
                            VALUES(?,?,?,?,?,?);
                            """,
                            (new_id, tname, source_type, source_expr, default_value, description),
                        )
                    conn.commit()
                st.success("Template created.")
                st.rerun()

    st.markdown("---")
    st.markdown("**Edit selected template**")

    if selected_id is None:
        st.info("Select a template above to edit it.")
        return

    with _closing(conn.cursor()) as cur:
        cur.execute(
            """
            SELECT id, name, description, template_type, default_section, body, is_active
            FROM proposal_templates
            WHERE id = ?;
            """,
            (int(selected_id),),
        )
        row = cur.fetchone()
        if not row:
            st.warning("Selected template not found.")
            return
        tpl_id, tpl_name, tpl_desc, tpl_type, tpl_default_section, tpl_body, tpl_is_active = row
        cur.execute(
            """
            SELECT token_name, source_type, source_expr, default_value, description
            FROM proposal_template_tokens
            WHERE template_id = ?;
            """,
            (int(selected_id),),
        )
        token_rows = cur.fetchall()

    edit_tokens_df = pd.DataFrame(
        token_rows,
        columns=["token_name", "source_type", "source_expr", "default_value", "description"],
    )

    with st.form("x7_tpl_edit_form"):
        edit_name = st.text_input("Template name", tpl_name or "", key="x7_tpl_edit_name")
        edit_type = st.selectbox(
            "Template type",
            options=["section", "proposal", "rfq_pack", "compliance"],
            index=["section", "proposal", "rfq_pack", "compliance"].index(tpl_type or "section")
            if (tpl_type or "section") in ["section", "proposal", "rfq_pack", "compliance"]
            else 0,
            key="x7_tpl_edit_type",
        )
        edit_default_section = st.text_input(
            "Default section label (optional)",
            tpl_default_section or "",
            key="x7_tpl_edit_default_section",
        )
        edit_description = st.text_area(
            "Description (optional)",
            tpl_desc or "",
            key="x7_tpl_edit_description",
        )
        edit_body = st.text_area(
            "Template body (use {{TOKEN_NAME}} placeholders)",
            tpl_body or "",
            height=200,
            key="x7_tpl_edit_body",
        )
        edit_tokens_df = st.data_editor(
            edit_tokens_df,
            num_rows="dynamic",
            hide_index=True,
            key=f"x7_tpl_edit_tokens_{selected_id}",
        )
        submitted_edit = st.form_submit_button("Save changes")

        if submitted_edit:
            if not edit_name.strip():
                st.warning("Template name is required.")
            elif not edit_body.strip():
                st.warning("Template body is required.")
            else:
                with _closing(conn.cursor()) as cur:
                    cur.execute(
                        """
                        UPDATE proposal_templates
                        SET name = ?, description = ?, template_type = ?, default_section = ?, body = ?, is_active = ?
                        WHERE id = ?;
                        """,
                        (
                            edit_name.strip(),
                            edit_description.strip() or None,
                            edit_type,
                            edit_default_section.strip() or None,
                            edit_body,
                            int(bool(tpl_is_active)),
                            int(selected_id),
                        ),
                    )
                    # Replace token definitions.
                    cur.execute(
                        "DELETE FROM proposal_template_tokens WHERE template_id = ?;",
                        (int(selected_id),),
                    )
                    try:
                        records = edit_tokens_df.to_dict("records")
                    except Exception:
                        records = []
                    for row in records:
                        tname = str(row.get("token_name") or "").strip().upper()
                        if not tname:
                            continue
                        source_type = str(row.get("source_type") or "manual").strip()
                        source_expr = str(row.get("source_expr") or "").strip() or None
                        default_value = row.get("default_value")
                        description = row.get("description")
                        cur.execute(
                            """
                            INSERT INTO proposal_template_tokens(template_id, token_name, source_type, source_expr, default_value, description)
                            VALUES(?,?,?,?,?,?);
                            """,
                            (int(selected_id), tname, source_type, source_expr, default_value, description),
                        )
                    conn.commit()
                st.success("Template updated.")
                st.rerun()





def fetch_sam_attachments_for_notice(notice):
    """
    Best-effort helper to enumerate attachments for a SAM.gov notice.

    `notice` may be a row dict from SAM Watch or a plain notice_id string.

    This uses sam_list_attachments when available (preferred), and falls back
    to sam_try_fetch_attachments to at least recover filenames. It returns a
    list of dicts with keys: file_name, file_url, file_type, last_updated.
    """
    # Normalize inputs
    notice_row = {}
    notice_id = ""
    if isinstance(notice, dict):
        notice_row = notice
        notice_id = str(
            notice_row.get("notice_id")
            or notice_row.get("Notice ID")
            or notice_row.get("NoticeID")
            or notice_row.get("id")
            or ""
        ).strip()
    else:
        notice_id = str(notice or "").strip()

    items = []

    # Preferred path: SamSearch / SAM client helper that understands row context
    if "sam_list_attachments" in globals():
        try:
            # Pass the row dict when we have it; otherwise a minimal stub
            ctx = notice_row or {"notice_id": notice_id, "Notice ID": notice_id}
            items = list(sam_list_attachments(ctx) or [])
        except Exception:
            items = []

    # Fallback: use sam_try_fetch_attachments to get at least names via notice_id
    if not items and "sam_try_fetch_attachments" in globals() and notice_id:
        try:
            tmp = list(sam_try_fetch_attachments(notice_id) or [])
        except Exception:
            tmp = []
        items = []
        for it in tmp:
            if isinstance(it, dict):
                name = it.get("name") or it.get("filename") or it.get("File Name") or "attachment"
                url = it.get("url") or it.get("URL") or it.get("link")
                mime = it.get("mime") or it.get("MimeType") or it.get("content_type")
                updated = (
                    it.get("last_updated")
                    or it.get("LastUpdated")
                    or it.get("lastModified")
                    or it.get("last_modified")
                )
                items.append(
                    {
                        "file_name": str(name).strip() or "attachment",
                        "file_url": str(url).strip() if url else None,
                        "file_type": str(mime).strip() if mime else None,
                        "last_updated": str(updated) if updated else None,
                    }
                )
            else:
                # Could be (name, bytes) tuples
                try:
                    name, _blob = it
                    items.append(
                        {
                            "file_name": str(name).strip() or "attachment",
                            "file_url": None,
                            "file_type": None,
                            "last_updated": None,
                        }
                    )
                except Exception:
                    continue
        return items

    # Normalize any dict-style items into the standard shape
    results = []
    for it in items:
        if not isinstance(it, dict):
            continue
        name = (
            it.get("name")
            or it.get("filename")
            or it.get("File Name")
            or "attachment"
        )
        url = it.get("url") or it.get("URL") or it.get("link")
        mime = it.get("mime") or it.get("MimeType") or it.get("content_type")
        updated = (
            it.get("last_updated")
            or it.get("LastUpdated")
            or it.get("lastModified")
            or it.get("last_modified")
        )
        results.append(
            {
                "file_name": str(name).strip() or "attachment",
                "file_url": str(url).strip() if url else None,
                "file_type": str(mime).strip() if mime else None,
                "last_updated": str(updated) if updated else None,
            }
        )
    return results


def sync_sam_attachments_metadata(conn, tenant_id, notice_id, attachments):
    """Insert or update attachment rows for a notice.
    attachments is a list of dicts with keys file_name, file_url, file_type, last_updated.
    """
    cur = conn.cursor()
    for att in attachments:
        cur.execute(
            """
            INSERT OR IGNORE INTO sam_attachments
            (tenant_id, notice_id, file_name, file_url, file_type, last_updated)
            VALUES (?, ?, ?, ?, ?, ?)
            """,
            (
                tenant_id,
                notice_id,
                att.get('file_name'),
                att.get('file_url'),
                att.get('file_type'),
                att.get('last_updated'),
            ),
        )
    conn.commit()

def download_sam_attachment(conn, att_id, base_dir="data/sam_attachments"):
    cur = conn.cursor()
    cur.execute(
        "SELECT tenant_id, notice_id, file_name, file_url FROM sam_attachments WHERE id = ?",
        (att_id,),
    )
    row = cur.fetchone()
    if not row:
        return None
    tenant_id, notice_id, file_name, file_url = row

    os.makedirs(base_dir, exist_ok=True)
    safe_name = f"{tenant_id}_{notice_id}_{file_name}".replace("/", "_")
    local_path = os.path.join(base_dir, safe_name)

    resp = requests.get(file_url, timeout=60)
    resp.raise_for_status()
    with open(local_path, "wb") as f:
        f.write(resp.content)

    cur.execute(
        "UPDATE sam_attachments SET local_path = ?, status = 'downloaded' WHERE id = ?",
        (local_path, att_id,),
    )
    conn.commit()
    return local_path

def link_attachment_to_rfp(conn, tenant_id, rfp_id, att_row):
    cur = conn.cursor()
    cur.execute(
        """
        INSERT INTO rfp_documents (
            tenant_id, rfp_id, notice_id, file_name, local_path, file_type, source, status
        ) VALUES (?, ?, ?, ?, ?, ?, 'sam_watch', 'pending')
        """,
        (
            tenant_id,
            rfp_id,
            att_row['notice_id'],
            att_row['file_name'],
            att_row['local_path'],
            att_row['file_type'],
        ),
    )
    conn.commit()




if __name__ == '__main__':
    import argparse as _argparse
    import traceback as _traceback

    _parser = _argparse.ArgumentParser(add_help=True)
    _parser.add_argument(
        "--worker",
        action="store_true",
        help="Run the background jobs worker loop instead of the Streamlit app.",
    )
    _args, _unknown = _parser.parse_known_args()

    if getattr(_args, "worker", False):
        try:
            jobs_worker_loop()
        except Exception:
            _traceback.print_exc()
    else:
        try:
            main()
        except NameError:
            # fallback: run default entry if main() not defined in this build
            pass


def router(page: str, conn: "sqlite3.Connection") -> None:
    """Dynamic router. Resolves run_<snake_case(page)> and executes safely."""
    import re as _re
    name = "run_" + _re.sub(r"[^a-z0-9]+", "_", (page or "").lower()).strip("_")
    fn = globals().get(name)
    # explicit fallbacks for known variant names
    if not callable(fn):
        alt = {
            "L and M Checklist": ["run_l_and_m_checklist", "run_lm_checklist"],
            "Backup & Data": ["run_backup_data", "run_backup_and_data"],
        }.get((page or "").strip(), [])
        for a in alt:
            fn = globals().get(a)
            if callable(fn):
                break
    if not callable(fn):
        import streamlit as _st
        _st.warning(f"No handler for page '{page}' resolved as {name}.")
        return
    _safe_route_call(fn, conn)
    # Hooks
    if (page or "").strip() == "Proposal Builder":
        _safe_route_call(globals().get("pb_phase_v_section_library", lambda _c: None), conn)

# =========================
# APPEND-ONLY PATCH • E1 (Google Places enrichment: phone + website hyperlinks)
# =========================
import streamlit as _st
import pandas as _pd
import re as _re
import time as _time

# ---- Streamlit write guard: suppress rendering of None ----
try:
    import streamlit as st

    if not hasattr(st, "_write_wrapped"):
        _orig_write = st.write
        def _write_no_none(*args, **kwargs):
            if len(args) == 1 and args[0] is None:
                return
            return _orig_write(*args, **kwargs)
        st.write = _write_no_none
        st._write_wrapped = True  # marker
except Exception:
    pass

# ---- Phase 1 bootstrap (guarded) ----
try:
    _title_safe = globals().get("APP_TITLE", "ELA GovCon Suite")
except Exception:
    _title_safe = "ELA GovCon Suite"
try:
    import streamlit as st

    st.set_page_config(page_title=_title_safe, page_icon="🧭", layout="wide")
except Exception:
    pass
for _fn in ("apply_theme_phase1", "_init_phase1_ui", "_sidebar_brand"):
    try:
        globals().get(_fn) and globals()[_fn]()
    except Exception:
        pass

# ---- Phase 0 neutralizer ----
# If any Phase 0 functions exist, convert to no-ops so they cannot override Phase 1.
for _fn in ("apply_theme_phase0", "_init_phase0_ui", "_sidebar_brand_phase0", "_apply_theme_old"):
    try:
        if _fn in globals() and callable(globals()[_fn]):
            globals()[_fn] = (lambda *a, **k: "")
    except Exception:
        pass

# ==== X.6 Compliance Matrix v1 ====
def x6_requirements_df(conn: "sqlite3.Connection", rfp_id: int):
    import pandas as pd
    try:
        df = pd.read_sql_query(
            "SELECT id, must_flag, file, page, text FROM compliance_requirements WHERE rfp_id=? ORDER BY must_flag DESC, id ASC;",
            conn, params=(int(rfp_id),)
        )
    except Exception:
        import pandas as pd
        df = pd.DataFrame(columns=["id","must_flag","file","page","text"])
    return df

def x6_save_links(conn: "sqlite3.Connection", rfp_id: int, mapping: list[tuple[int, str]]) -> int:
    from contextlib import closing as _closing
    saved = 0
    with _closing(conn.cursor()) as cur:
        for rid, sec in mapping:
            if not sec:
                continue
            cur.execute(
                "INSERT INTO compliance_links(rfp_id, requirement_id, section) VALUES(?,?,?)",
                (int(rfp_id), int(rid), str(sec)[:200])
            )
            saved += 1
    conn.commit()
    return int(saved)

# ==== X.7 Proposal Builder v1 ====

def x7_snippet_library_ui(conn: "sqlite3.Connection") -> None:
    """Manage reusable proposal snippets that can be inserted into Proposal Builder sections.

    Snippets are short blocks of text categorized by type and tagged by NAICS and work type.
    """
    from contextlib import closing as _closing

    st.markdown("**Snippet library**")

    try:
        df = pd.read_sql_query(
            """
            SELECT id, name, category, naics, work_type, is_active, created_at
            FROM proposal_snippets
            ORDER BY created_at DESC;
            """,
            conn,
            params=(),
        )
    except Exception:
        df = pd.DataFrame(columns=["id", "name", "category", "naics", "work_type", "is_active", "created_at"])

    if df.empty:
        st.caption("No snippets saved yet.")
    else:
        st.dataframe(df)

    st.markdown("### Add or edit snippet")

    categories = ["technical", "management", "past_performance", "risks"]

    with st.form("pb_snippet_new"):
        name = st.text_input("Snippet name", key="pb_snip_name")
        category = st.selectbox("Category", categories, key="pb_snip_category")
        naics = st.text_input("NAICS code tag", key="pb_snip_naics")
        work_type = st.text_input("Work type tag", key="pb_snip_work_type", help="Short label such as janitorial, grounds, training")
        body = st.text_area("Snippet body", height=160, key="pb_snip_body")
        submitted = st.form_submit_button("Save snippet")

        if submitted:
            if not name.strip():
                st.warning("Name is required.")
            elif not body.strip():
                st.warning("Body is required.")
            else:
                try:
                    current_user = None
                    try:
                        current_user = get_current_user_name()
                    except Exception:
                        current_user = None
                    with _closing(conn.cursor()) as cur:
                        cur.execute(
                            """
                            INSERT INTO proposal_snippets(name, category, naics, work_type, body, owner, is_active)
                            VALUES(?,?,?,?,?,?,1);
                            """,
                            (name.strip(), category, naics.strip() or None, work_type.strip() or None, body.strip(), current_user),
                        )
                        conn.commit()
                    st.success("Snippet saved.")
                    st.rerun()
                except Exception as _e:
                    st.error(f"Could not save snippet: {_e}")

    if not df.empty:
        with st.expander("Archive snippet", expanded=False):
            to_archive = st.selectbox(
                "Snippet to archive",
                options=[None] + df["id"].tolist(),
                format_func=lambda sid: "Choose..." if sid is None else f"#{sid} — {df.loc[df['id']==sid,'name'].values[0]}",
                key="pb_snip_archive_id",
            )
            if st.button("Archive selected snippet", disabled=to_archive is None, key="pb_snip_archive_btn"):
                if to_archive is not None:
                    try:
                        with _closing(conn.cursor()) as cur:
                            cur.execute(
                                "UPDATE proposal_snippets SET is_active = 0, updated_at = CURRENT_TIMESTAMP WHERE id = ?;",
                                (int(to_archive),),
                            )
                            conn.commit()
                        st.success("Snippet archived.")
                        st.rerun()
                    except Exception as _e:
                        st.error(f"Could not archive snippet: {_e}")

def _ensure_x7_schema(conn: "sqlite3.Connection") -> None:
    from contextlib import closing as _closing
    with _closing(conn.cursor()) as cur:
        cur.execute(
            """
            CREATE TABLE IF NOT EXISTS proposals(
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                rfp_id INTEGER NOT NULL,
                title TEXT,
                status TEXT DEFAULT 'draft',
                created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                updated_at TEXT DEFAULT CURRENT_TIMESTAMP
            );
            """
        )
        try:
            cur.execute("PRAGMA table_info(proposals);")
            _cols = [r[1] for r in cur.fetchall()]
            if "owner" not in _cols:
                cur.execute("ALTER TABLE proposals ADD COLUMN owner TEXT;")
        except Exception:
            # Best-effort migration; ignore if PRAGMA or ALTER are not supported.
            pass

        cur.execute(
            """
            CREATE TABLE IF NOT EXISTS proposal_sections(
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                proposal_id INTEGER NOT NULL,
                ord INTEGER NOT NULL,
                title TEXT,
                content TEXT,
                settings_json TEXT,
                updated_at TEXT DEFAULT CURRENT_TIMESTAMP
            );
            """
        )

        cur.execute(
            """
            CREATE TABLE IF NOT EXISTS proposal_templates(
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                name TEXT NOT NULL,
                description TEXT,
                template_type TEXT DEFAULT 'section',
                default_section TEXT,
                body TEXT NOT NULL,
                owner TEXT,
                is_active INTEGER NOT NULL DEFAULT 1,
                created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                updated_at TEXT DEFAULT CURRENT_TIMESTAMP
            );
            """
        )

        cur.execute(
            """
            CREATE TABLE IF NOT EXISTS proposal_template_tokens(
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                template_id INTEGER NOT NULL,
                token_name TEXT NOT NULL,
                source_type TEXT NOT NULL,
                source_expr TEXT,
                default_value TEXT,
                description TEXT,
                UNIQUE(template_id, token_name),
                FOREIGN KEY(template_id) REFERENCES proposal_templates(id) ON DELETE CASCADE
            );
            """
        )
    
        cur.execute(
            """
            CREATE TABLE IF NOT EXISTS proposal_snippets(
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                name TEXT NOT NULL,
                category TEXT NOT NULL,
                naics TEXT,
                work_type TEXT,
                body TEXT NOT NULL,
                owner TEXT,
                is_active INTEGER NOT NULL DEFAULT 1,
                created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                updated_at TEXT DEFAULT CURRENT_TIMESTAMP
            );
            """
        )

        conn.commit()



def x7_create_proposal_from_outline(conn: "sqlite3.Connection", rfp_id: int, title: str | None = None) -> int:
    from contextlib import closing as _closing
    import json, datetime as _dt
    outline = st.session_state.get(f"proposal_outline_{int(rfp_id)}", "") or ""
    if not outline.strip():
        outline = "\n".join([
            "# Proposal Outline",
            "1. Cover Letter",
            "2. Executive Summary",
            "3. Technical Approach",
            "4. Management Approach",
            "5. Past Performance",
            "6. Pricing",
            "7. Compliance Matrix",
        ])
    lines = [ln.strip() for ln in outline.splitlines() if ln.strip() and not ln.startswith("#")]
    if not title:
        title = f"Proposal for RFP #{int(rfp_id)}"
    with _closing(conn.cursor()) as cur:
        _owner = _x7_current_user()
        cur.execute("INSERT INTO proposals(rfp_id, title, owner) VALUES(?,?,?)", (int(rfp_id), title, _owner))
        pid = cur.lastrowid
        for i, ln in enumerate(lines, start=1):
            cur.execute(
                "INSERT INTO proposal_sections(proposal_id,ord,title,content,settings_json) VALUES (?, ?, ?, ?, ?,?)",
                (int(pid), i, ln, "", json.dumps({"font":"Times New Roman","size":11}))
            )
    conn.commit()
    try:
        _fts_index_proposal(conn, int(pid))
    except Exception:
        # FTS indexing is best-effort; ignore failures.
        pass
    return int(pid)


def _x7_current_user() -> str:
    """Return the current logical user name for scoping proposals."""
    try:
        # Reuse the global helper so all parts of the app agree
        return get_current_user_name()
    except Exception:
        return KNOWN_USERS[0]

def x7_list_proposals(conn: "sqlite3.Connection", rfp_id: int):
    import pandas as pd
    try:
        return pd.read_sql_query("SELECT id, title, status, created_at FROM proposals WHERE rfp_id=? ORDER BY id DESC;", conn, params=(int(rfp_id),))
    except Exception:
        import pandas as pd
        return pd.DataFrame(columns=["id","title","status","created_at"])

def x7_get_sections(conn: "sqlite3.Connection", proposal_id: int):
    import pandas as pd
    try:
        return pd.read_sql_query("SELECT id, ord, title, content, settings_json FROM proposal_sections WHERE proposal_id=? ORDER BY ord ASC;", conn, params=(int(proposal_id),))
    except Exception:
        import pandas as pd
        return pd.DataFrame(columns=["id","ord","title","content","settings_json"])


def x7_save_section(conn: "sqlite3.Connection", section_id: int, content: str | None, settings_json: str | None = None) -> None:
    """
    Save a single proposal section and update the FTS index for the parent proposal.
    """
    from contextlib import closing as _closing

    proposal_id: int | None = None
    try:
        with _closing(conn.cursor()) as cur:
            cur.execute("SELECT proposal_id FROM proposal_sections WHERE id=?", (int(section_id),))
            row = cur.fetchone()
            if row:
                proposal_id = int(row[0])
    except Exception:
        proposal_id = None

    with _closing(conn.cursor()) as cur:
        cur.execute(
            "UPDATE proposal_sections SET content=?, settings_json=COALESCE(?, settings_json), updated_at=CURRENT_TIMESTAMP WHERE id=?",
            (content or "", settings_json, int(section_id)),
        )
    conn.commit()

    if proposal_id is not None:
        try:
            _fts_index_proposal(conn, int(proposal_id))
        except Exception:
            # FTS is best-effort; ignore failures so the main save path still works.
            pass

def x7_generate_section_ai(conn, rfp_id: int, section_title: str, notes: str = "", temperature: float = 0.1) -> str:
    import pandas as _pd, json
    try:
        df_lm = _pd.read_sql_query("SELECT section, item_text AS item FROM lm_items WHERE rfp_id=? ORDER BY id;", conn, params=(int(rfp_id),))
    except Exception:
        df_lm = _pd.DataFrame(columns=["section","item"])
    try:
        df_dates = _pd.read_sql_query("SELECT label, date_text FROM key_dates WHERE rfp_id=? ORDER BY id;", conn, params=(int(rfp_id),))
    except Exception:
        df_dates = _pd.DataFrame(columns=["label","date_text"])
    try:
        df_pocs = _pd.read_sql_query("SELECT name, role, email, phone FROM pocs WHERE rfp_id=? ORDER BY id;", conn, params=(int(rfp_id),))
    except Exception:
        df_pocs = _pd.DataFrame(columns=["name","role","email","phone"])
    try:
        df_clin = _pd.read_sql_query("SELECT clin, description, qty, unit FROM clin_lines WHERE rfp_id=? ORDER BY id;", conn, params=(int(rfp_id),))
    except Exception:
        df_clin = _pd.DataFrame(columns=["clin","description","qty","unit"])
    try:
        df_meta = _pd.read_sql_query("SELECT key, value FROM rfp_meta WHERE rfp_id=?;", conn, params=(int(rfp_id),))
        meta = {str(r["key"]): str(r["value"]) for _, r in df_meta.iterrows()} if not df_meta.empty else {}
    except Exception:
        meta = {}
    try:
        _df_all = _pd.read_sql_query(
            "SELECT COALESCE(text, '') AS t FROM rfp_files_t WHERE rfp_id=? ORDER BY id;",
            conn, params=(int(rfp_id),),
        )
        if _df_all is not None and not _df_all.empty:
            full_text = "\n\n".join([str(x or "") for x in _df_all['t'].tolist()])
        else:
            raise ValueError("rfp_files_t empty")
    except Exception:
        try:
            full_text = y5_extract_from_rfp(conn, int(rfp_id)) or ""
        except Exception:
            full_text = ""
    full_text = "\n".join([ln.strip() for ln in str(full_text or "").splitlines() if ln.strip()])[:20000]
    try:
        dfp = _pd.read_sql_query("SELECT * FROM org_profile WHERE id=1;", conn)
        profile = dfp.iloc[0].to_dict() if not dfp.empty else {}
    except Exception:
        profile = {}
    company = (profile.get("company_name") or "ELA Management LLC").strip()
    cage = profile.get("cage") or "14ZP6"
    uei  = profile.get("uei") or "U32LBVK3DDF7"
    duns = profile.get("duns") or "14-483-4790"

    lm_str = "\n".join([f"- {r['item']}" for _, r in df_lm.head(150).iterrows()]) if not df_lm.empty else ""
    dates_str = "\n".join([f"- {r['label']}: {r['date_text']}" for _, r in df_dates.iterrows()]) if not df_dates.empty else ""
    pocs_str = "\n".join([f"- {r['name']} ({r['role']}) {r['email']} {r['phone']}"] for _, r in df_pocs.iterrows()) if not df_pocs.empty else ""
    clin_str = "\n".join([f"- {r['clin']}: {r['description']} (Qty {r['qty']} {r['unit']})"] for _, r in df_clin.iterrows()) if not df_clin.empty else ""

    system = ("You are a veteran federal proposal writer with $70M+ in awards. "
              "Draft in precise federal style. Tailor to THIS RFP only. "
              "NO citations. Provide procedures and concrete steps. Apply the style guide.")
    user = f"""
SECTION TO DRAFT: {section_title}

{_style_guide()}

USE THIS SCAFFOLD:
- Section lead mirroring solicitation.
- Client need.
- Deliverables.
- Technical approach (step-by-step).
- Management approach (roles and RACI).
- Staffing and coverage.
- Equipment and materials.
- Timeline milestones.
- Quality control and metrics.
- Subcontractors and oversight.
- Risks and mitigations.
- Compliance crosswalk (bullets to L&M).
- Past performance tie-in.
- Price approach note.

RFP META:
{json.dumps(meta, ensure_ascii=False)}

KEY DATES:
{dates_str or '(none)'}

POCs:
{pocs_str or '(none)'}

CLINs:
{clin_str or '(none)'}

L&M ITEMS:
{lm_str or '(none)'}

FULL TEXT (truncated):
{full_text}

COMPANY:
- {company} | CAGE {cage} | UEI {uei} | DUNS {duns}

NOTES:
{notes or '(none)'}

REQUIREMENTS:
- Write one paragraph per idea. Start a new paragraph when the topic shifts.
- Use solicitation terms verbatim when relevant.
- No citations or references.
- Short sentences (<=10 words). Paragraphs max 10 sentences.
- Organize for easy scoring.
- If something is unknown, state the dependency or required input. Do not assume.
- Output only the requested section. No other sections.
- Do not include TOC or extra headings.
- Apply this blueprint:
{_section_blueprint(section_title)}
"""

    try:
        from openai import OpenAI as _OpenAI
        client = _OpenAI()
        model = _resolve_model()
        resp = client.chat.completions.create(
            model=model,
            temperature=float(temperature or 0.1),
            messages=[{"role":"system","content":system},{"role":"user","content":user}]
        )
        text = (resp.choices[0].message.content or "").strip()
    except Exception as e:
        text = f"[AI unavailable] {e}"

    return _finalize_section(section_title, text)
def _safe_int(v, default: int = 0) -> int:
    try:
        if v is None:
            return default
        if isinstance(v, int):
            return v
        s = str(v).strip()
        import re as _re
        m = _re.search(r"\d+", s)
        return int(m.group()) if m else default
    except Exception:
        return default

def __e1_google_api_key():
    try: return _st.secrets["google"]["api_key"]
    except Exception:
        try: return _st.secrets["GOOGLE_API_KEY"]
        except Exception: return ""

def __e1_norm_phone(p):
    digits = "".join(_re.findall(r"\d+", str(p or "")))
    return digits[1:] if len(digits)==11 and digits.startswith("1") else digits

def __e1_existing_vendor_keys(conn):
    return _s1d_select_existing_pairs(conn)

def __e1_enrich_and_render(conn, lat=None, lng=None, radius_m=80467, query=""):

    by_np, by_pid = _s1d_select_existing_pairs(conn)
    key = __e1_google_api_key()
    if not key:
        _st.error("E1 requires a Google API key in secrets: [google].api_key or GOOGLE_API_KEY")
        return
    if not query:
        _st.info("Enter a query in your existing finder, then use this enrichment to view details.")
        return
    import requests as _rq
    params={"query":query, "key":key, "region":"us"}
    if lat is not None and lng is not None:
        params.update({"location": f"{lat},{lng}", "radius": int(radius_m)})
    js = _rq.get("https://maps.googleapis.com/maps/api/place/textsearch/json", params=params, timeout=12).json()
    results = js.get("results", [])
    by_np, by_pid = __e1_existing_vendor_keys(conn)
    rows = []
    for r in results:
        name = r.get("name",""); pid = r.get("place_id",""); addr = r.get("formatted_address","")
        phone = ""; website = ""; gurl = ""
        try:
            det = _rq.get("https://maps.googleapis.com/maps/api/place/details/json",
                          params={"place_id": pid, "fields": "formatted_phone_number,website,url", "key": key}, timeout=10).json().get("result", {}) or {}
            digits = "".join(_re.findall(r"\\d+", det.get("formatted_phone_number","") or ""))
            if len(digits)==11 and digits.startswith("1"): digits=digits[1:]
            phone = digits; website = det.get("website","") or ""; gurl = det.get("url","") or ""
        except Exception:
            pass
        dup = ((name.strip().lower(), phone) in by_np) or (pid in by_pid)
        rows.append(dict(name=name, phone=phone, website=website, address=addr, place_id=pid, google_url=gurl, _dup=dup))
        _time.sleep(0.05)
    if not rows:
        _st.info("No results to enrich.")
        return
    df = _pd.DataFrame(rows)
    st.session_state["s1d_df"] = df.to_dict("records")
    def _link(u,t): return f"<a href='{u}' target='_blank'>{t}</a>" if u else t
    df["name"] = df.apply(lambda r: _link(r["google_url"], r["name"]), axis=1)
    df["website"] = df.apply(lambda r: _link(r["website"], "site") if r["website"] else "", axis=1)
    _st.write(df[["name","phone","website","address","place_id","_dup"]].to_html(escape=False, index=False), unsafe_allow_html=True)

# =========================

# --- Tab name alias ---
def run_l_and_m_checklist(conn):
    return run_lm_checklist(conn)

# --- Tab name alias ---
def run_backup_data(conn):
    return run_backup_and_data(conn)

# === Outreach: fallback sender picker and guard wrapper =====================
# Robust fallback picker that reads smtp_settings, only if not provided earlier.
if "_o3_render_sender_picker" not in globals():
    def _o3_render_sender_picker():
        import streamlit as st

        from contextlib import closing
        host, port, username, password, use_tls = "smtp.gmail.com", 465, "", "", False
        try:
            if "get_db" in globals():
                conn2 = get_db()
                with closing(conn2.cursor()) as cur:
                    cur.execute("CREATE TABLE IF NOT EXISTS smtp_settings (id INTEGER PRIMARY KEY, label TEXT, host TEXT, port INTEGER, username TEXT, password TEXT, use_tls INTEGER)")
                    row = cur.execute("SELECT host, port, username, password, use_tls FROM smtp_settings WHERE id=1").fetchone()
                if row:
                    host = row[0] or host
                    port = int(row[1] or port)
                    username = row[2] or username
                    password = row[3] or password
                    use_tls = bool(row[4] or use_tls)
        except Exception:
            pass
        if not username or not password:
            st.error("Sender not configured. Go to Outreach → Sender and Save sender.")
            return {}
        st.caption(f"Using {username} via {host}:{int(port)} TLS={'on' if use_tls else 'off'}")
        return {"host": host, "port": int(port), "email": username, "app_password": password, "use_tls": bool(use_tls)}

# Guard wrapper: ensure sender is configured before original Mail Merge UI runs
if '_wrapped_o4_mailmerge' not in globals():
    _wrapped_o4_mailmerge = True
    _orig__render_outreach_mailmerge = render_outreach_mailmerge
    def render_outreach_mailmerge(conn):
        import streamlit as st

        sender = _o3_render_sender_picker() if "_o3_render_sender_picker" in globals() else {}
        if sender and "username" in sender and "email" not in sender:
            sender["email"] = sender.get("username","")
        if sender and "password" in sender and "app_password" not in sender:
            sender["app_password"] = sender.get("password","")
        if not sender.get("email") or not sender.get("app_password"):
            st.warning("Configure a sender first in Outreach → Sender.")
            return
        return _orig__render_outreach_mailmerge(conn)
# === End Outreach guard =====================================================

def o1_sender_accounts_ui(conn):
    globals()['_O4_CONN'] = conn
    import streamlit as st

    import pandas as _pd
    ensure_outreach_o1_schema(conn)
    st.subheader("Sender accounts")
    email = st.text_input("Email")
    display = st.text_input("Display name")
    app_pw = st.text_input("Gmail App password", type="password")
    c1,c2,c3 = st.columns(3)
    with c1: host = st.text_input("SMTP host", value="smtp.gmail.com")
    with c2: port = st.number_input("SMTP port", 1, 65535, value=465)
    with c3: ssl = st.checkbox("Use SSL", value=True)
    if st.button("Save account", key="o4_ac_save__2"):
        if not email:
            st.error("Email required")
        else:
            with conn:
                conn.execute("""
                INSERT INTO email_accounts(user_email, display_name, app_password, smtp_host, smtp_port, use_ssl)
                VALUES (?, ?, ?, ?, ?,?,?)
                ON CONFLICT(user_email) DO UPDATE SET
                    display_name=excluded.display_name,
                    app_password=excluded.app_password,
                    smtp_host=excluded.smtp_host,
                    smtp_port=excluded.smtp_port,
                    use_ssl=excluded.use_ssl
                """, (email.strip(), display or "", app_pw or "", host or "smtp.gmail.com", int(port or 465), 1 if ssl else 0))
            st.success("Saved")
    try:
        import streamlit as st

        st.session_state["o4_sender_sel"] = email.strip()
    except Exception:
        pass

    try:
        df = _pd.__p_read_sql_query("SELECT user_email, display_name, smtp_host, smtp_port, use_ssl FROM email_accounts ORDER BY user_email", conn)
        _styled_dataframe(df, use_container_width=True)
    except Exception:
        pass

# ---- helper: stable pagination (Phase 0) ----
def _pager_init(key: str):
    if key not in st.session_state:
        st.session_state[key] = 0

def _pager_update(key: str, delta: int, max_pages: int | None = None):
    _pager_init(key)
    st.session_state[key] = max(st.session_state[key] + delta, 0)
    if max_pages is not None:
        st.session_state[key] = min(st.session_state[key], max_pages - 1)
    return st.session_state[key]

# ---- helper: write guard (Phase 0) ----
def _write_guard(conn, fn, *args, **kwargs):
    # call DB write functions inside a transaction
    with conn:
        return fn(*args, **kwargs)

# ---- Phase 1: Global UI setup (theme, CSS, helpers) ----
def _init_phase1_ui():
    if st.session_state.get('_phase1_ui_ready'):
        return
    st.session_state['_phase1_ui_ready'] = True
    st.markdown("<div class='ela-banner'>Phase 1 theme active · polished layout & tables</div>", unsafe_allow_html=True)
    st.markdown('''
    <style>
    .block-container {padding-top: 1.2rem; padding-bottom: 1.2rem; max-width: 1400px;}
    h1, h2, h3 {margin-bottom: .4rem;}
    .ela-subtitle {color: rgba(49,51,63,0.65); font-size: .95rem; margin-bottom: 1rem;}
    div[data-testid="stDataFrame"] thead th {position: sticky; top: 0; background: #fff; z-index: 2;}
    div[data-testid="stDataFrame"] tbody tr:hover {background: rgba(64, 120, 242, 0.06);}
    .ela-card {border: 1px solid rgba(49,51,63,0.16); border-radius: 12px; padding: 12px; margin-bottom: 12px;}
    .ela-chip {display:inline-block; padding: 2px 8px; border-radius: 999px; font-size: 12px; margin-right:6px; background: rgba(49,51,63,.06);}
    .ela-ok {background: rgba(0,200,83,.12);}
    .ela-warn {background: rgba(251,140,0,.12);}
    .ela-bad {background: rgba(229,57,53,.12);}
    
    /* Top ribbon banner */
    .ela-banner {position: sticky; top: 0; z-index: 999; background: linear-gradient(90deg, #4068f2, #7a9cff); color: #fff; padding: 6px 12px; border-radius: 8px; margin-bottom: 10px;}
    /* Sidebar branding spacing */
    section[data-testid="stSidebar"] .block-container {padding-top: 0.8rem;}
    /* Expander cards */
    [data-testid="stExpander"] {border: 1px solid rgba(49,51,63,0.16); border-radius: 12px; margin-bottom: 10px;}
    [data-testid="stExpander"] summary {font-weight: 600;}
    /* Buttons subtle shadow */
    button[kind="primary"] {box-shadow: 0 1px 4px rgba(0,0,0,.08);}
    /* Text inputs rounding */
    .stTextInput>div>div>input, .stNumberInput input, .stTextArea textarea {border-radius: 10px !important;}
    
    </style>
    ''', unsafe_allow_html=True)

def _sidebar_brand():
    with st.sidebar:
        st.markdown("### 🧭 ELA GovCon Suite")
        st.caption("Phase 1 UI loaded")
        st.caption("Faster sourcing, compliant bids, higher win rates.")
        st.markdown(
            """
            <div class='ela-card'>
              <div style="font-weight:700; margin-bottom:4px;">Company</div>
              <div style="font-size:13px; line-height:1.35">
                <strong>ELA Management LLC</strong><br>
                999 Fortino Blvd Lot 246<br>
                Pueblo, CO 81008, US
                <hr style="border:0;border-top:1px solid rgba(49,51,63,0.16);margin:6px 0;">
                <div><span style="opacity:.7">CAGE</span> 14ZP6</div>
                <div><span style="opacity:.7">UEI</span> U32LBVK3DDF7</div>
                <div><span style="opacity:.7">DUNS</span> 14-483-4790</div>
              </div>
            </div>
            """,
            unsafe_allow_html=True,
        )

def _styled_dataframe(df, use_container_width=True, height=None, hide_index=True, column_config=None):
    try:
        return _styled_dataframe(df, use_container_width=use_container_width, height=height, hide_index=hide_index, column_config=column_config)
    except TypeError:
        return _styled_dataframe(df, use_container_width=use_container_width, height=height)

def _chip(text: str, kind: str = 'neutral'):
    cls = 'ela-chip'
    if kind == 'ok': cls += ' ela-ok'
    elif kind == 'warn': cls += ' ela-warn'
    elif kind == 'bad': cls += ' ela-bad'
    st.markdown(f"<span class='{cls}'>{text}</span>", unsafe_allow_html=True)

# === Phase 2: NL Query Parsing & Relevance Score (SAM-only) ===
import math, datetime, re, json
from datetime import datetime as _dt, timedelta as _td

def parse_nl_query(q: str) -> dict:
    if not q: return {}
    s = q.lower()
    out = {"keywords": [], "naics": [], "state": None, "set_aside": None, "due_lte_days": None}
    for m in re.findall(r"\b(\d{5,6})\b", s):
        out["naics"].append(m)
    for key, aliases in {
        "8a": ["8a", "8(a)"],
        "sdvosb": ["sdvosb", "service disabled", "service-disabled"],
        "wosb": ["wosb", "women owned"],
        "hubzone": ["hubzone"],
        "sdb": ["sdb", "small disadvantaged"],
        "veteran": ["veteran", "vosb"]
    }.items():
        if any(a in s for a in aliases):
            out["set_aside"] = key.upper(); break
    for m in re.findall(r"\bin\s+([a-z]{2})\b", s):
        out["state"] = m.upper()
    m = re.search(r"due\s*<\s*(\d+)\s*days", s) or re.search(r"due\s*<=\s*(\d+)\s*days", s)
    if m: out["due_lte_days"] = int(m.group(1))
    tokens = re.findall(r"[a-zA-Z][a-zA-Z0-9\-/]+", s)
    banned = set(["in","due","days","sdvosb","wosb","hubzone","8a","8","veteran","sdb"] + out["naics"])
    out["keywords"] = [t for t in tokens if t not in banned]
    return out

def relevance_score(row: dict, profile: dict | None = None) -> float:
    score = 0.0
    try:
        pref_naics = set((profile or {}).get("preferred_naics", []))
        pref_set = (profile or {}).get("preferred_set_aside", "").upper().strip()
        r_naics = set(str(row.get("naics") or "").replace(";",",").split(","))
        if pref_naics and r_naics & pref_naics: score += 2.5
        if pref_set and str(row.get("set_aside") or "").upper().startswith(pref_set): score += 2.0
        due = str(row.get("due_date") or "")
        if due:
            try:
                dd = _dt.fromisoformat(due.replace("Z","").replace("T"," ")); days = (dd - _dt.utcnow()).days
                if days <= 14: score += 1.5
                elif days <= 30: score += 0.5
            except Exception: pass
        kw = (profile or {}).get("keywords", [])
        hay = (str(row.get("title") or "") + " " + str(row.get("description") or "")).lower()
        hits = sum(1 for k in kw if k.lower() in hay); score += min(2.0, 0.3 * hits)
    except Exception: pass
    return round(score, 3)

# === Phase 2: Alerts Center (SAM-only) ===

def _render_ask_rfp_autorender():
    """If a previous click requested the Ask RFP modal, render it open on rerun."""
    import streamlit as st

    if st.session_state.get("ask_rfp_open"):
        _ask_rfp_analyzer_modal(st.session_state.get("ask_rfp_notice") or {})

def _safe_write(path: str, data: bytes):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "wb") as f:
        f.write(data)

def _extract_text_from_pdf_bytes(b: bytes) -> str:
    try:
        import PyPDF2
        reader = PyPDF2.PdfReader(io.BytesIO(b))
        return "\n".join([page.extract_text() or "" for page in reader.pages])
    except Exception as e:
        return f"[PDF extract unavailable: {e}]"

def _extract_text_from_docx_bytes(b: bytes) -> str:
    try:
        import docx
        d = docx.Document(io.BytesIO(b))
        return "\n".join([p.text for p in d.paragraphs])
    except Exception as e:
        return f"[DOCX extract unavailable: {e}]"

def _extract_text_from_xlsx_bytes(b: bytes) -> str:
    try:
        import pandas as pd
        with tempfile.NamedTemporaryFile(suffix=".xlsx", delete=False) as tmp:
            tmp.write(b); tmp.flush()
            x = pd.ExcelFile(tmp.name)
            parts = []
            for name in x.sheet_names:
                df = x.parse(name)
                parts.append(f"--- SHEET: {name} ---\n" + df.to_csv(index=False))
            return "\n\n".join(parts)
    except Exception as e:
        return f"[XLSX extract unavailable: {e}]"

def _extract_text_from_zip_bytes(b: bytes) -> str:
    out = []
    try:
        with zipfile.ZipFile(io.BytesIO(b)) as z:
            for name in z.namelist():
                if name.endswith("/"):
                    continue
                try:
                    data = z.read(name)
                    if name.lower().endswith(".pdf"):
                        out.append(f"--- {name} ---\n" + _extract_text_from_pdf_bytes(data))
                    elif name.lower().endswith(".docx"):
                        out.append(f"--- {name} ---\n" + _extract_text_from_docx_bytes(data))
                    elif name.lower().endswith((".xlsx",".xls")):
                        out.append(f"--- {name} ---\n" + _extract_text_from_xlsx_bytes(data))
                    elif name.lower().endswith((".txt",".csv",".md")):
                        try: out.append(f"--- {name} ---\n" + data.decode("utf-8", "ignore"))
                        except Exception: out.append(f"--- {name} --- [text decode failed]")
                    else:
                        out.append(f"--- {name} --- [skipped: unsupported]")
                except Exception as e:
                    out.append(f"--- {name} --- [error: {e}]")
        return "\n\n".join(out)
    except Exception as e:
        return f"[ZIP extract unavailable: {e}]"

def _extract_text_guess(name: str, b: bytes) -> str:
    n = (name or "").lower()
    if n.endswith(".pdf"):
        return _extract_text_from_pdf_bytes(b)
    if n.endswith(".docx"):
        return _extract_text_from_docx_bytes(b)
    if n.endswith((".xlsx",".xls")):
        return _extract_text_from_xlsx_bytes(b)
    if n.endswith(".zip"):
        return _extract_text_from_zip_bytes(b)
    if n.endswith((".txt",".csv",".md",".rtf")):
        try: return b.decode("utf-8", "ignore")
        except Exception as e: return f"[text decode failed: {e}]"
    return "[unsupported file type]"

_TZMAP = {
    "ET":"-0500","EST":"-0500","EDT":"-0400",
    "CT":"-0600","CST":"-0600","CDT":"-0500",
    "MT":"-0700","MST":"-0700","MDT":"-0600",
    "PT":"-0800","PST":"-0800","PDT":"-0700",
    "UTC":"+0000","Z":"+0000","GMT":"+0000"
}
_date_time_pat = re.compile(
    r"(?:(?:proposal|proposals?|responses?)\\s*(?:are\\s*)?(?:due|closing|close|response\\s*due|submission\\s*deadline)\\s*[:\\-–]?\\s*)?"
    r"("
    r"(?:[A-Za-z]{3,9}\\s+\\d{1,2},\\s*\\d{4}(?:\\s+\\d{1,2}:\\d{2}(?:\\s*[AP]M)?)?)"
    r"|(?:\\d{1,2}/\\d{1,2}/\\d{2,4}(?:\\s+\\d{1,2}:\\d{2}(?:\\s*[AP]M)?)?)"
    r"|(?:\\d{4}-\\d{2}-\\d{2}(?:[ T]\\d{1,2}:\\d{2})?)"
    r")"
    r"(?:\\s*(?:hrs|hours|h|at|by)?\\s*(\\d{3,4})(?:\\s*(?:hrs|hours|h))?)?"
    r"(?:\\s*(ET|EST|EDT|CT|CST|CDT|MT|MST|MDT|PT|PST|PDT|UTC|GMT|Z))?",
    re.IGNORECASE
)

def _parse_dt_guess(s: str):
    try:
        from dateutil import parser as _dp
        return _dp.parse(s, fuzzy=True)
    except Exception:
        for fmt in ("%B %d, %Y %I:%M %p", "%B %d, %Y", "%b %d, %Y %I:%M %p", "%b %d, %Y",
                    "%m/%d/%Y %I:%M %p", "%m/%d/%Y", "%Y-%m-%d %H:%M", "%Y-%m-%d"):
            try:
                return datetime.strptime(s, fmt)
            except Exception:
                continue
    return None

def _detect_due_date(text: str, notice: dict=None):
    candidates = []
    if isinstance(notice, dict):
        for key in ["Response Date", "Close Date", "Due Date", "Archive Date", "Date Response Due", "Offers Due Date", "Offers Due", "Closing Date", "Response Deadline"]:
            val = notice.get(key)
            if val and isinstance(val, str):
                dt = _parse_dt_guess(val)
                if dt: candidates.append(("notice:"+key, val, dt))
    for m in _date_time_pat.finditer(text or ""):
        chunk = m.group(1); hhmm = m.group(2); tz = (m.group(3) or "").upper()
        s = chunk
        if hhmm and (":" not in s):
            try: s = s.strip() + f" {hhmm[:-2]}:{hhmm[-2:]}"
            except Exception: pass
        if tz in _TZMAP: s = s + " " + _TZMAP[tz]
        dt = _parse_dt_guess(s)
        if dt: candidates.append(("text", s, dt))
    from datetime import timedelta
    now = datetime.now()
    future = [(src, raw, dt) for (src, raw, dt) in candidates if dt and dt >= now - timedelta(days=3)]
    pool = future or candidates
    if pool:
        pool.sort(key=lambda x: x[2])
        chosen = pool[0]
        return chosen[1], chosen[2]
    return None, None

def _make_ics(title: str, dt):
    try:
        if not dt: return None
        if dt.hour == 0 and dt.minute == 0:
            ics = "BEGIN:VCALENDAR\\nVERSION:2.0\\nPRODID:-//RFP//Analyzer//EN\\nBEGIN:VEVENT\\n"
            ics += f"SUMMARY:{title or 'Proposal Due'}\\n"
            ics += f"DTSTART;VALUE=DATE:{dt.strftime('%Y%m%d')}\\n"
            ics += "END:VEVENT\\nEND:VCALENDAR\\n"
            return ics.encode("utf-8")
        ics = "BEGIN:VCALENDAR\\nVERSION:2.0\\nPRODID:-//RFP//Analyzer//EN\\nBEGIN:VEVENT\\n"
        ics += f"SUMMARY:{title or 'Proposal Due'}\\n"
        ics += f"DTSTART:{dt.strftime('%Y%m%dT%H%M00Z')}\\n"
        ics += "DURATION:PT1H\\nEND:VEVENT\\nEND:VCALENDAR\\n"
        return ics.encode("utf-8")
    except Exception:
        return None

def run_router(conn):
    import streamlit as st
    pages = ["SAM Watch", "RFP Analyzer"]
    choice = st.sidebar.radio("Go to", pages, index=0, key="phase3_nav")
    if choice == "RFP Analyzer":
        return run_rfp_analyzer(conn)
    # If SAM Watch exists, delegate; otherwise show a note
    if "run_sam_watch" in globals():
        return run_sam_watch(conn)
    st.info("SAM Watch page is not available in this build. Use RFP Analyzer above.")

import sqlite3

# --- Style guide accessor to avoid NameError ---
def _style_guide() -> str:
    try:
        return PROPOSAL_STYLE_GUIDE  # type: ignore[name-defined]
    except Exception:
        return (
            "Follow these rules strictly:\n"
            "1) Understand client need. Mirror solicitation terms exactly.\n"
            "2) State deliverables explicitly.\n"
            "3) Address evaluation factors: technical, management, past performance, price.\n"
            "4) Answer each L&M requirement directly.\n"
            "5) Obey page and format rules.\n"
            "6) Provide HOW procedures, not claims.\n"
            "7) Short sentences (<=10 words). One idea per paragraph.\n"
            "8) Use bullets. Clean headings.\n"
            "9) Include roles, equipment, timeline, QC checks, metrics.\n"
            "10) Identify subcontractors and responsibilities.\n"
            "11) Organize clearly for easy scoring.\n"
            "12) Include a Risk table with mitigations.\n"
            "13) Add a brief L&M compliance crosswalk.\n"
            "14) Keep tone federal and precise."
            "15) Refer to the contractor as 'ELA Management' instead of 'we' or 'us'. "
            "Avoid first-person plural."
        )



# === Proposal Style + Finalizers ===
PROPOSAL_STYLE_GUIDE = (
    "Follow these rules strictly:\n"
    "1) Understand client need. Mirror solicitation terms exactly.\n"
    "2) State deliverables explicitly.\n"
    "3) Address evaluation factors: technical, management, past performance, price.\n"
    "4) Answer each L&M requirement directly.\n"
    "5) Obey page and format rules.\n"
    "6) Provide HOW procedures, not claims.\n"
    "7) Short sentences (<=10 words). One idea per paragraph.\n"
    "8) Use bullets. Clean headings.\n"
    "9) Include roles, equipment, timeline, QC checks, metrics.\n"
    "10) Identify subcontractors and responsibilities.\n"
    "11) Organize clearly for easy scoring.\n"
    "12) Include a Risk table with mitigations.\n"
    "13) Add a brief L&M compliance crosswalk.\n"
    "14) Keep tone federal and precise."
)

def _strip_citations(text: str) -> str:
    import re
    if not text: return text
    t = str(text)
    t = re.sub(r"\s*\[\d+\]", "", t)
    t = re.sub(r"\s*\(\d+\)", "", t)
    t = re.sub(r"\n+references?:\n.*$", "", t, flags=re.I|re.S)
    t = re.sub(r"^source:.*$", "", t, flags=re.I|re.M)
    t = re.sub(r"\n{3,}", "\n\n", t)
    return t.strip()

def _enforce_style_guide(text: str, max_words=10, max_sents_per_para=10) -> str:
    import re
    if not text: return text
    t = re.sub(r"[ \t]+", " ", str(text)).strip()
    paras = re.split(r"\n\s*\n", t)
    out = []
    for p in paras:
        sents = re.split(r"(?<=[.!?])\s+", p.strip())
        fixed = []
        for s in sents:
            if not s: continue
            words = s.split()
            if len(words) <= max_words:
                fixed.append(" ".join(words))
                continue
            s2 = re.sub(r"\s*,\s*", ". ", s)
            s2 = re.sub(r"\s+and\s+", ". ", s2)
            parts = re.split(r"(?<=[.!?])\s+", s2)
            for part in parts:
                w = part.strip().split()
                if not w: continue
                while len(w) > max_words:
                    fixed.append(" ".join(w[:max_words]) + ".")
                    w = w[max_words:]
                if w: fixed.append(" ".join(w))
        if len(fixed) > max_sents_per_para:
            for i in range(0, len(fixed), max_sents_per_para):
                out.append(" ".join(fixed[i:i+max_sents_per_para]))
        else:
            out.append(" ".join(fixed))
    return "\n\n".join(out).strip()



def _pb_psychology_framework() -> str:
    """
    Psychology rules used to shape tone and trust. Not printed verbatim.
    """
    return (
        "Psychology layer:\n"
        "- Lead with empathy to the mission and constraints.\n"
        "- Tell a compact story: context, challenge, method, outcome.\n"
        "- Reciprocity: mutual wins for agency and vendor.\n"
        "- Certainty language: clear commitments, no hedging, show controls.\n"
        "- Trust markers: past performance, QA, safety, data security, subcontractor vetting.\n"
        "- Disarm risk early: list top risks, mitigations, verification steps.\n"
        "- Smooth cadence: short sentences, one idea per paragraph, clean transitions.\n"
        "- No citations or snippet IDs. Self-contained answer.\n"
    ).strip()



def _finalize_draft(text: str) -> str:
    try:
        t = _strip_citations(text)
    except Exception:
        t = str(text or "")
    try:
        t = _enforce_style_guide(t)
    except Exception:
        pass
    try:
        import re as _re_fix
        # Headings and terms: Assumptions -> Dependencies
        t = _re_fix.sub(r'(?im)^\s*assumptions\s*:?', 'Dependencies', t)
        def _swap_dep(mo):
            word = mo.group(0)
            # preserve case
            if word.isupper():
                return 'DEPENDENCIES' if word.endswith('S') else 'DEPENDENCY'
            if word[0].isupper():
                return 'Dependencies' if word.endswith('s') else 'Dependency'
            return 'dependencies' if word.endswith('s') else 'dependency'
        t = _re_fix.sub(r'(?i)\bassumptions\b', _swap_dep, t)
        t = _re_fix.sub(r'(?i)\bassumption\b', _swap_dep, t)
    except Exception:
        pass
    # Enforce ELA Management instead of first-person plural
    try:
        import re as _re_pron
        t = _re_pron.sub(r'\b[Ww]e\b', 'ELA Management', t)
        t = _re_pron.sub(r'\b[Uu]s\b', 'ELA Management', t)
    except Exception:
        pass
    return _one_idea_per_paragraph(t)

def _get_conn(db_path: str = "samwatch.db"):
    """Legacy SamWatch-style connector.

    In the unified app we prefer the central `_db_connect` helpers so we still
    get WAL mode, PRAGMAs, and indices even if older code paths call this.
    """
    import os
    import sqlite3  # local import as a safe fallback

    # Make sure the directory exists
    os.makedirs(os.path.dirname(db_path) or ".", exist_ok=True)

    try:
        # Prefer the main app database and connector if available
        if "DB_PATH" in globals():
            return _db_connect(DB_PATH)
    except Exception:
        # Fall back to the passed-in path if anything goes wrong
        pass

    return sqlite3.connect(db_path, check_same_thread=False)

# (Phase 3 router entry removed to avoid duplicate UI in host app)

    st.set_page_config(page_title="GovCon — SAM Watch & Analyzer", layout="wide")
    conn = _get_conn()
    try:
        _ensure_phase2_schema(conn)
    except Exception:
        pass
    run_router(conn)


# SIG+LOGO patch removed

# O4 wrapper disabled


# === Proposal Builder: structured output mix (paragraphs + bullets + tables) ===
def _pb_decide_structure(section: str) -> dict:
    s = (section or "").strip().lower()
    use = {"paragraphs": True, "bullets": True, "tables": []}
    if any(k in s for k in ["technical", "approach", "methodology"]):
        use["tables"] = ["traceability"]
    elif any(k in s for k in ["management", "transition"]):
        use["tables"] = ["traceability", "schedule"]
    elif "quality" in s or "qc" in s or "qa" in s:
        use["tables"] = ["traceability", "risks"]
    elif "risk" in s:
        use["tables"] = ["risks"]
    elif "staff" in s or "key personnel" in s:
        use["tables"] = ["staffing"]
    elif any(k in s for k in ["price", "pricing", "clin", "cost"]):
        use["tables"] = ["price"]
        use["bullets"] = True
    elif any(k in s for k in ["compliance", "crosswalk", "requirements"]):
        use["tables"] = ["traceability"]
    else:
        use["tables"] = []
    return use

def _pb_extract_requirements_from_context(context: str, limit: int = 8):
    reqs = []
    try:
        import re as _re
        for ln in (context or "").splitlines():
            l = ln.strip()
            if not l or len(l) < 10:
                continue
            if _re.search(r"\b(shall|must|required|will provide|deliver|install|maintain|perform)\b", l, _re.I):
                l = _re.sub(r"^\s*[\(\[\d\.\)-]+\s*", "", l)
                reqs.append(l)
            if len(reqs) >= limit:
                break
    except Exception:
        pass
    return reqs

def _pb_paragraph_index_map(text_body: str):
    paras = [p.strip() for p in (text_body or "").split("\n\n") if p.strip()]
    return {i+1: p for i, p in enumerate(paras)}

def _pb_build_markdown_table(headers, rows):
    try:
        import pandas as _pd
        if not rows:
            return ""
        df = _pd.DataFrame(rows, columns=headers)
        cols = list(df.columns)
        header = "| " + " | ".join(cols) + " |\n"
        sep = "| " + " | ".join(["---"] * len(cols)) + " |\n"
        lines = []
        for _, r in df.iterrows():
            lines.append("| " + " | ".join(str(r[c]) if r[c] is not None else "" for c in cols) + " |")
        return header + sep + "\n".join(lines)
    except Exception:
        return ""

def _pb_bullets_from_text(section: str, draft: str, context: str) -> dict:
    import re as _re
    bullets = {"features": [], "compliance": [], "evidence": [], "benefits": []}
    t = (draft or "")

    for m in _re.finditer(r"\b(\d{1,3}(?:[,]\d{3})*|\d+)(\s?(?:%|days?|months?|hours?|FTEs?|SLA|ISO|CAGE|UEI))\b", t):
        frag = m.group(0)
        if frag not in bullets["evidence"]:
            bullets["evidence"].append(frag)

    for line in (context or "").splitlines():
        l = line.strip()
        if _re.search(r"\b(shall|must|required)\b", l, _re.I):
            l = _re.sub(r"^\s*[\(\[\d\.\)-]+\s*", "", l)
            bullets["compliance"].append(l[:180] + ('…' if len(l) > 180 else ""))
            if len(bullets["compliance"]) >= 6:
                break

    for s in _re.split(r"[.\n]", t):
        s2 = s.strip()
        if not s2:
            continue
        if _re.search(r"\b(automate|validate|inspect|monitor|dispatch|staff|notify|report|train|transition|migrate)\b", s2, _re.I):
            bullets["features"].append(s2[:120] + ('…' if len(s2) > 120 else ""))
        if len(bullets["features"]) >= 6:
            break

    benefits_pool = [
        "Reduces schedule risk through defined checkpoints",
        "Improves accuracy with double-check QC steps",
        "Cuts cycle time by removing rework loops",
        "Keeps you compliant with L&M and clauses",
        "Gives rapid status visibility for oversight"
    ]
    for b in benefits_pool:
        if len(bullets["benefits"]) < 5:
            bullets["benefits"].append(b)

    return {k: v for k, v in bullets.items() if v}

def _pb_tables_for_section(section: str, draft: str, context: str) -> list[str]:
    use = _pb_decide_structure(section)
    tables = []

    if "traceability" in use.get("tables", []):
        reqs = _pb_extract_requirements_from_context(context)
        if reqs:
            pmap = _pb_paragraph_index_map(draft)
            rows = []
            for i, r in enumerate(reqs, 1):
                where = f"Paragraph {min(i, len(pmap))}" if pmap else ""
                rows.append([r, where, "Yes"])
            tables.append("**Requirements Traceability**\n" + _pb_build_markdown_table(["Requirement", "Where Addressed", "Meets?"], rows))

    if "schedule" in use.get("tables", []):
        import re as _re
        days = None
        m = _re.search(r"\b(\d{1,3})\s*(calendar\s*)?days?\b", context or "", flags=_re.I)
        if m:
            days = int(m.group(1))
        rows = [["Kickoff", "Day 0", "Project start"],
                ["Transition complete", f"Day {min(30, days or 30)}", "Core services live"],
                ["Final acceptance", f"Day {days or 90}", "All deliverables accepted"]]
        tables.append("**Schedule Milestones**\n" + _pb_build_markdown_table(["Milestone", "Target", "Notes"], rows))

    if "risks" in use.get("tables", []):
        rows = [["Late access to sites", "Medium", "Pre-coordination and backup crew", "PM"],
                ["Spec ambiguity", "Medium", "RFI early, confirmation log", "PM"],
                ["Sub availability", "Low", "Bench with surge SVs", "Vendor Lead"]]
        tables.append("**Key Risks and Controls**\n" + _pb_build_markdown_table(["Risk", "Impact", "Control", "Owner"], rows))

    if "staffing" in use.get("tables", []):
        rows = [["Project Manager", "1", "PMP or equivalent", "Oversight, reporting"],
                ["Lead Tech", "1", "5+ yrs domain exp", "Site coordination"],
                ["Technicians", "2–6", "Certs per spec", "Execution"]]
        tables.append("**Staffing Plan**\n" + _pb_build_markdown_table(["Role", "Qty", "Qualifications", "Responsibility"], rows))

    if "price" in use.get("tables", []):
        import re as _re
        clin_lines = [ln.strip() for ln in (context or "").splitlines() if _re.search(r"\bCLIN\b|\b000\d\b", ln, _re.I)]
        rows = []
        for ln in clin_lines[:6]:
            # Extract a CLIN token if possible
            m = _re.search(r"(CLIN\s*\d+|000\d)", ln, _re.I)
            token = m.group(1) if m else "CLIN"
            rows.append([token, "See RFP", "", ""])
        if rows:
            tables.append("**CLIN Summary**\n" + _pb_build_markdown_table(["CLIN", "Description", "Qty", "Unit Price"], rows))

    return [t for t in tables if t]

def _assemble_section_output(section: str, draft: str, context: str) -> str:
    use = _pb_decide_structure(section)
    out = []
    if use.get("paragraphs"):
        out.append(draft.strip())
    if use.get("bullets"):
        b = _pb_bullets_from_text(section, draft, context)
        order = [("Features", "features"), ("Compliance", "compliance"), ("Evidence", "evidence"), ("Benefits", "benefits")]
        lines = []
        for label, key in order:
            items = b.get(key, [])
            if not items:
                continue
            lines.append(f"**{label}**")
            for x in items[:6]:
                lines.append(f"- {x}")
            lines.append("")
        if lines:
            out.append("\n".join(lines).strip())
    for tbl in _pb_tables_for_section(section, draft, context):
        out.append(tbl.strip())
    return "\n\n".join([b for b in out if b]).strip()
# === End structured output mix ===
def _pb_word_count_section(text: str) -> int:
    try:
        import re as _re
        return len(_re.findall(r"[A-Za-z0-9']+", str(text or "")))
    except Exception:
        
        try:
            return len((text or "").split())
        except Exception:
            return 0



# === Append-only patch: Chat+ attachment manifest and robust composer ===
def _chat_plus_attachment_manifest(files: list[dict]) -> str:
    try:
        lines = [f"ATTACHMENT MANIFEST: {len(files)} file(s) attached)."]
        for i, r in enumerate(files, start=1):
            try:
                name = str(r.get('name') or f'file_{i}')
                mime = str(r.get('mime') or '')
                size = int(r.get('size') or 0)
            except Exception:
                name, mime, size = (str(r.get('name') or f'file_{i}'), '', 0)
            kb = (size // 1024) if isinstance(size, int) and size >= 0 else 0
            lines.append(f"{i}. {name} ({mime}, {kb} KB)")
        return "\\n".join(lines)
    except Exception as e:
        return f"ATTACHMENT MANIFEST: error {e}"

def _chat_plus_compose_messages(context_text: str, history: list[dict], q: str, mode: str = 'Auto') -> list[dict]:
    import streamlit as st  # local import safe in Streamlit env
    files = st.session_state.get('chat_plus_files') or []
    manifest = _chat_plus_attachment_manifest(files)
    system = (
        "You are the ELA Chat Assistant. Answer using ONLY the provided context or the attachment manifest. "
        "If the user asks about attachments, count and list them from the manifest exactly. "
        "Do not invent attachments. No citations. Be concise. "
        "If information is missing, state the missing input."
    )
    tool_ctx = f"{manifest}\\n\\nCONTEXT SNIPPETS:\\n{context_text or '(no snippets)'}"
    messages = [{"role": "system", "content": system},
                {"role": "system", "content": tool_ctx}]
    for m in history or []:
        role = str(m.get('role') or 'user'); content = str(m.get('content') or '')
        messages.append({"role": role, "content": content})
    messages.append({"role": "user", "content": q or ""})
    return messages

def _chat_plus_call_openai(messages: list[dict], temperature: float | int = 0.15) -> str:
    try:
        from openai import OpenAI as _OpenAI
        client = _OpenAI()
        model = _resolve_model()
        resp = client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=float(temperature or 0.15),
        )
        return (resp.choices[0].message.content or "").strip()
    except Exception as e:
        return f"[AI unavailable] {e}"

# --- patched Phase 3 schema override ---

def _ensure_phase3_schema(conn):
    """Ensure rfp_records and rfp_files tables exist with all required columns."""
    import sqlite3 as _sqlite3  # noqa: F401
    try:
        with conn:
            conn.execute(
                """
                CREATE TABLE IF NOT EXISTS rfp_records (
                    id INTEGER PRIMARY KEY,
                    notice_id TEXT,
                    title TEXT,
                    sam_url TEXT,
                    created_at TEXT DEFAULT (datetime('now'))
                );
                """
            )
            conn.execute(
                """
                CREATE TABLE IF NOT EXISTS rfp_files (
                    id INTEGER PRIMARY KEY,
                    rfp_id INTEGER,
                    filename TEXT,
                    mime TEXT,
                    sha256 TEXT,
                    pages INTEGER,
                    bytes BLOB,
                    path TEXT,
                    kind TEXT,
                    extracted_path TEXT,
                    created_at TEXT DEFAULT (datetime('now')),
                    FOREIGN KEY(rfp_id) REFERENCES rfp_records(id)
                );
                """
            )
            try:
                cols = [r[1] for r in conn.execute("PRAGMA table_info(rfp_files);").fetchall()]
            except Exception:
                cols = []
            needed = {
                "mime": "TEXT",
                "sha256": "TEXT",
                "pages": "INTEGER",
                "path": "TEXT",
                "kind": "TEXT",
                "extracted_path": "TEXT",
                "created_at": "TEXT",
            }
            for col, decl in needed.items():
                if col not in cols:
                    try:
                        conn.execute(f"ALTER TABLE rfp_files ADD COLUMN {col} {decl};")
                    except Exception:
                        pass
    except Exception:
        return


# --- User-scoped override for proposal listing --------------------------------
def x7_list_proposals(conn: "sqlite3.Connection", rfp_id: int):
    """
    Return proposals for a given RFP, scoped to the current user when set.

    Existing rows without an owner remain visible to all users; new rows
    created after this build will set proposals.owner from _x7_current_user().
    """
    import pandas as pd
    sql = "SELECT id, title, status, created_at FROM proposals WHERE rfp_id=?"
    params = [int(rfp_id)]
    _owner = _x7_current_user()
    if _owner:
        sql += " AND (owner = ? OR owner IS NULL)"
        params.append(_owner)
    sql += " ORDER BY id DESC;"
    try:
        return pd.read_sql_query(sql, conn, params=tuple(params))
    except Exception:
        return pd.DataFrame(columns=["id", "title", "status", "created_at"])